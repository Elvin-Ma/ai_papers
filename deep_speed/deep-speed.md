# DeepSpeed：面向所有人的大规模模型训练
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在二月份，我们宣布推出了[DeepSpeed](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)，这是一个开源的深度学习训练优化库，以及库中的一项创新的内存优化技术ZeRO（Zero Redundancy Optimizer）。DeepSpeed通过提升规模、速度、成本和易用性，极大地改进了大型模型训练。DeepSpeed使研究人员能够创建图灵自然语言生成（Turing-NLG）模型，这是当时参数数量达到170亿个并且具有最先进准确性的最大语言模型。在五月份，我们发布了支持最多2000亿参数的ZeRO-2技术，相比于现有技术，训练速度提高了10倍。同时，我们还发布了一系列计算、输入/输出和收敛优化技术，为最快的BERT训练提供支持。从那时起，我们一直以快速的速度进行创新，推动着深度学习训练速度和规模的界限。我们不断努力创新，不断突破深度学习训练的速度和规模限制。<br>


