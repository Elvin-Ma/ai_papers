# SDXL: Improving Latent Diffusion Models (LDM) for High-Resolution Image Synthesis(图像合成)

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们介绍了SDXL，一种用于文本到图像合成的潜空间扩散模型(LDM)。与先前版本的稳定扩散相比，SDXL利用了一个**三倍大的UNet骨干**：模型参数增加主要是由于**更多的注意力块**和**更大的交叉注意力上下文**，因为SDXL使用了第二个文本编码器(总共使用了两个文本编辑器)。我们设计了多种新颖的条件方案，并在**多个宽高比**上对SDXL进行训练。我们还引入了一个改进模型(refinement model)，该模型使用事后图像到图像技术**改善**SDXL生成的样本的视觉保真度。我们证明了与先前版本的稳定扩散相比，SDXL显示出了显著提高的性能，并且在与最先进的黑盒图像生成器相比的结果上取得了竞争力。秉承促进开放研究和提高大型模型训练和评估透明度的精神，我们提供代码和模型权重的访问权限。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;过去一年，在深度生成建模领域取得了巨大的突破，涵盖了各种数据领域，如自然语言[50]、音频[17]和视觉媒体[38, 37, 40, 44, 15, 3, 7]。在本报告中，我们专注于后者(视觉媒体)，并介绍了SDXL，这是稳定扩散的一个极大改进版本。稳定扩散是一种潜空间文本到图像扩散模型（DM），为近期的一系列进展提供了基础，例如3D分类[43]、可控图像编辑[54]、图像个性化[10]、合成数据增强[48]、图形用户界面原型设计[51]等。值得注意的是，应用范围非常广泛，涵盖了音乐生成[9]和从fMRI脑部扫描重建图像[49]等领域。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户研究表明，SDXL在性能上始终明显超过(surpasses)了所有先前版本的稳定扩散（见图1）。在本报告中，我们介绍了导致性能提升的设计选择，包括：<br>
1. 相比于先前的稳定扩散模型，使用了**3倍大的UNet骨干**（第2.1节）；
2. 使用了两种简单而有效的**附加条件技术**（第2.2节），不需要任何形式的额外监督；
3. 引入了一个独立的基于扩散的改进模型(refinement model)，该模型对SDXL生成的潜空间进行**噪声去噪处理**[28]，以提高其样本的视觉质量（第2.5节）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在视觉媒体创作领域，一个主要关注点是，虽然黑盒模型(black-box-models)通常被认为是最先进的，但其架构的不透明性阻碍了对其性能的准确评估和验证。这种缺乏透明度影响了可重现性，阻碍了创新，并阻止了社区在这些模型的基础上进行进一步的科学和艺术进展。此外，这些闭源策略使得以公正客观的方式评估这些模型的偏见(bias)和局限性变得具有挑战性，这对于它们的负责任和道德部署至关重要。通过SDXL，我们发布了一个开放模型，其性能与黑盒图像生成模型(eg: midjourney)具有竞争力（见图10和图11）。<br>

# 2 提升稳定扩散
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在本节中，我们介绍了我们对稳定扩散架构的改进。这些改进是**模块化的**，可以单独或一起使用来扩展任何模型。尽管以下策略是作为潜空间扩散模型（LDMs）[38]的扩展实现的，但它们中的大多数**也适用于像素空间**的对应模型。<br>

![figure1](images/SDXL-figure1.jpg)

## 2.1 架构与规模
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从Ho等人的开创性工作[14][DDPM](https://arxiv.org/pdf/2006.11239.pdf)和Song等人的工作[47]开始，它们证明了DM对于图像合成是强大的生成模型，Convolution UNet[39](https://arxiv.org/pdf/1505.04597.pdf)架构一直是基于扩散的图像合成的主导架构。然而，随着基础DM的发展[40, 37, 38]，底层架构不断演化：从添加自注意力和改进的上采样层[5](https://arxiv.org/pdf/2105.05233.pdf)，到用于文本到图像合成的交叉注意力[38](https://arxiv.org/pdf/2112.10752.pdf)，再到纯Transformer-based架构[33](https://arxiv.org/pdf/2212.09748.pdf)。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们遵循这一趋势，并且在Hoogeboom等人的研究[16]的基础上，将Transformer计算的大部分转移到UNet中的**较低级特征上**。与原始的稳定扩散架构不同，我们在UNet中**使用了不同类型的Transformer块分布**：出于效率考虑，我们省略了最高级特征层上的Transformer块，在**较低级别**使用2个和10个块，并完全删除了UNet中最低级别（8倍下采样）的层级-请参见表1，对比了稳定扩散1.x和2.x以及SDXL的架构。我们选择了一个更强大的预训练文本编码器，用于文本条件。具体来说，我们使用OpenCLIP ViT-bigG [19]结合CLIP ViT-L [34]，在通道轴上沿着倒数第二个文本编码器输出进行连接[1]。除了使用交叉注意力层来将模型与文本输入进行条件化外，我们还遵循[30]的方法，额外将模型与OpenCLIP模型的池化文本嵌入进行条件化。这些改变导致UNet中的模型参数数量为26亿(2.6B)个，详见表1。文本编码器总共有8.17亿个参数。<br>

![table1](images/SDXL-table1.jpg)

## 2.2 微调条件化
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在图像大小上对模型进行条件化LDM范例的一个**臭名昭著的缺点是**，由于其两阶段架构，训练模型需要一个最小的图像尺寸。解决这个问题的两种主要方法要么是**丢弃所有低于某个最小分辨率的训练图像**（例如，Stable Diffusion 1.4/1.5会丢弃所有尺寸低于512像素的图像），要么是对太小的图像进行放大。然而，根据所需的图像分辨率，前一种方法可能导致训练数据的大部分被丢弃，这可能会导致性能下降并影响泛化能力。我们在图2中对SDXL预训练的数据集可视化了这种效果。对于这个特定的数据选择，丢弃所有分辨率低于我们预训练分辨率 $256^{2}$ 像素的样本将导致39%的数据被丢弃。另一方面，第二种方法通常会引入放大伪影，这可能会泄漏到最终模型输出中，导致例如模糊的样本。<br>

![figure2](images/SDXL-figure2.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相反，我们提议在UNet模型中**对原始图像分辨率进行条件化**，这在训练过程中是轻而易举的。具体来说，我们将图像的原始高度和宽度（即，在任何缩放之前）作为额外的条件提供给模型 $c_{size} = (h_{original},w_{original})$ 。每个组件都使用傅里叶特征编码进行独立嵌入，并将这些编码连接成一个单一的向量，然后通过将其添加到时间步骤嵌入[5]中输入给模型。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在推理过程中，用户可以通过这个尺寸条件设置图像的期望视觉分辨率。显然（见图3），模型已经学会将条件化的尺寸 $c_{size}$ 与分辨率相关的图像特征关联起来，这可以用来修改与给定提示(prompt)相对应的输出的外观。请注意，在图3中展示的可视化中，我们展示了由512×512模型生成的样本（详见第2.5节的细节），因为在我们最终的SDXL模型中，尺寸条件化的效果在之后的多方面（宽高比）微调中不太明显可见。<br>

![figure3](images/SDXL-figure3.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们通过在大小为 $512^{2}$ 的类条件ImageNet [4]上训练和评估三个LDM（Latent Diffusion Models）来定量评估这种简单但有效的条件化技术的效果：对于第一个模型（CIN-512-only），我们**丢弃**了所有至少**有一个边缘小于512像素的训练样本**，导致训练数据集**只有**7万个图像。对于CIN-nocond，我们使用所有的训练样本，但**不进行尺寸条件化**。尺寸条件化只用于CIN-size-cond。训练后，我们使用50个DDIM步骤[46]和(无分类器的)引导尺度为5 [13]为每个模型生成5k个样本，并计算IS [42]和FID [12]（与完整的验证集进行比较）。对于CIN-size-cond，我们始终生成以 $c_{size} = (512, 512)$ 为条件的样本。表2总结了结果，并验证了**CIN-size-cond在两个度量指标上改进了基准模型**。我们将CIN-512-only的性能下降归因于由于在**小训练数据集上过拟合而导致的泛化能力差**，而CIN-nocond中模糊样本分布的一种模式效应导致了降低的FID分数。请注意，尽管我们发现这些经典的定量评分不适合评估基础（文本到图像）DM（Data Models）的性能[40、37、38]（见附录F），但它们仍然是ImageNet上合理的度量指标，因为FID和IS的神经骨干已经在ImageNet上进行了训练。<br>

![table2](images/SDXL-table2.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将模型条件化为裁剪参数 图4的前两行展示了以前SD模型的典型故障模式：合成的对象可能会被裁剪，例如左侧示例中的猫的截断头部对于SD 1-5和SD 2-1。对于这种行为的直观解释是**在模型训练过程中使用了随机裁剪**：由于在DL框架（如PyTorch [32]）中整理批次需要相同大小的张量，典型的处理流程是（i）调整图像大小，使最短边与目标尺寸匹配，然后（ii）沿着较长的轴随机裁剪图像。虽然随机裁剪是一种自然的数据增强形式，但它可能泄露到生成的样本中，导致上述恶意效果。<br>

![figure4](images/SDXL-figure4.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了解决这个问题，我们提出了另一种简单但有效的条件化方法：在数据加载过程中，我们均匀采样裁剪坐标 $c_{top}$ 和 $c_{left}$（分别指定从左上角沿高度和宽度轴裁剪的像素数量的整数），并通过傅里叶特征嵌入将它们作为条件化参数输入模型中，类似于上面描述的尺寸条件化。然后，将连接的嵌入 $c_{crop}$ 用作附加的条件化参数。我们强调，这种技术不仅限于LDM，可以用于任何DM。注意，裁剪和尺寸条件化可以直接组合在一起。在这种情况下，我们将特征嵌入沿通道维度进行连接，然后将其添加到UNet中的时间步嵌入之前。算法1说明了如果应用了这种组合，我们在训练过程中如何采样 $c_{crop}$ 和 $c_{size}$ 。<br>

![algorithm1](images/SDXL-algorithm1.jpg)







