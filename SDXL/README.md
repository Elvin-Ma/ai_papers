# SDXL: Improving Latent Diffusion Models (LDM) for High-Resolution Image Synthesis(图像合成)

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们介绍了SDXL，一种用于文本到图像合成的潜空间扩散模型(LDM)。与先前版本的稳定扩散相比，SDXL利用了一个**三倍大的UNet骨干**：模型参数增加主要是由于**更多的注意力块**和**更大的交叉注意力上下文**，因为SDXL使用了第二个文本编码器(总共使用了两个文本编辑器)。我们设计了多种新颖的条件方案，并在**多个宽高比**上对SDXL进行训练。我们还引入了一个改进模型(refinement model)，该模型使用事后图像到图像技术**改善**SDXL生成的样本的视觉保真度。我们证明了与先前版本的稳定扩散相比，SDXL显示出了显著提高的性能，并且在与最先进的黑盒图像生成器相比的结果上取得了竞争力。秉承促进开放研究和提高大型模型训练和评估透明度的精神，我们提供代码和模型权重的访问权限。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;过去一年，在深度生成建模领域取得了巨大的突破，涵盖了各种数据领域，如自然语言[50]、音频[17]和视觉媒体[38, 37, 40, 44, 15, 3, 7]。在本报告中，我们专注于后者(视觉媒体)，并介绍了SDXL，这是稳定扩散的一个极大改进版本。稳定扩散是一种潜空间文本到图像扩散模型（DM），为近期的一系列进展提供了基础，例如3D分类[43]、可控图像编辑[54]、图像个性化[10]、合成数据增强[48]、图形用户界面原型设计[51]等。值得注意的是，应用范围非常广泛，涵盖了音乐生成[9]和从fMRI脑部扫描重建图像[49]等领域。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户研究表明，SDXL在性能上始终明显超过(surpasses)了所有先前版本的稳定扩散（见图1）。在本报告中，我们介绍了导致性能提升的设计选择，包括：<br>
1. 相比于先前的稳定扩散模型，使用了**3倍大的UNet骨干**（第2.1节）；
2. 使用了两种简单而有效的**附加条件技术**（第2.2节），不需要任何形式的额外监督；
3. 引入了一个独立的基于扩散的改进模型(refinement model)，该模型对SDXL生成的潜空间进行**噪声去噪处理**[28]，以提高其样本的视觉质量（第2.5节）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在视觉媒体创作领域，一个主要关注点是，虽然黑盒模型(black-box-models)通常被认为是最先进的，但其架构的不透明性阻碍了对其性能的准确评估和验证。这种缺乏透明度影响了可重现性，阻碍了创新，并阻止了社区在这些模型的基础上进行进一步的科学和艺术进展。此外，这些闭源策略使得以公正客观的方式评估这些模型的偏见(bias)和局限性变得具有挑战性，这对于它们的负责任和道德部署至关重要。通过SDXL，我们发布了一个开放模型，其性能与黑盒图像生成模型(eg: midjourney)具有竞争力（见图10和图11）。<br>

# 2 提升稳定扩散
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在本节中，我们介绍了我们对稳定扩散架构的改进。这些改进是**模块化的**，可以单独或一起使用来扩展任何模型。尽管以下策略是作为潜空间扩散模型（LDMs）[38]的扩展实现的，但它们中的大多数**也适用于像素空间**的对应模型。<br>

![figure1](images/SDXL-figure1.jpg)

## 2.1 架构与规模
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从Ho等人的开创性工作[14][DDPM](https://arxiv.org/pdf/2006.11239.pdf)和Song等人的工作[47]开始，它们证明了DM对于图像合成是强大的生成模型，Convolution UNet[39](https://arxiv.org/pdf/1505.04597.pdf)架构一直是基于扩散的图像合成的主导架构。然而，随着基础DM的发展[40, 37, 38]，底层架构不断演化：从添加自注意力和改进的上采样层[5](https://arxiv.org/pdf/2105.05233.pdf)，到用于文本到图像合成的交叉注意力[38](https://arxiv.org/pdf/2112.10752.pdf)，再到纯Transformer-based架构[33](https://arxiv.org/pdf/2212.09748.pdf)。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们遵循这一趋势，并且在Hoogeboom等人的研究[16]的基础上，将Transformer计算的大部分转移到UNet中的**较低级特征上**。与原始的稳定扩散架构不同，我们在UNet中**使用了不同类型的Transformer块分布**：出于效率考虑，我们省略了最高级特征层上的Transformer块，在**较低级别**使用2个和10个块，并完全删除了UNet中最低级别（8倍下采样）的层级-请参见表1，对比了稳定扩散1.x和2.x以及SDXL的架构。我们选择了一个更强大的预训练文本编码器，用于文本条件。具体来说，我们使用OpenCLIP ViT-bigG [19]结合CLIP ViT-L [34]，在通道轴上沿着倒数第二个文本编码器输出进行连接[1]。除了使用交叉注意力层来将模型与文本输入进行条件化外，我们还遵循[30]的方法，额外将模型与OpenCLIP模型的池化文本嵌入进行条件化。这些改变导致UNet中的模型参数数量为26亿(2.6B)个，详见表1。文本编码器总共有8.17亿个参数。<br>

![table1](images/SDXL-table1.jpg)

## 2.2 微调条件化
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在图像大小上对模型进行条件化LDM范例的一个**臭名昭著的缺点是**，由于其两阶段架构，训练模型需要一个最小的图像尺寸。解决这个问题的两种主要方法要么是**丢弃所有低于某个最小分辨率的训练图像**（例如，Stable Diffusion 1.4/1.5会丢弃所有尺寸低于512像素的图像），要么是对太小的图像进行放大。然而，根据所需的图像分辨率，前一种方法可能导致训练数据的大部分被丢弃，这可能会导致性能下降并影响泛化能力。我们在图2中对SDXL预训练的数据集可视化了这种效果。对于这个特定的数据选择，丢弃所有分辨率低于我们预训练分辨率 $256^{2}$ 像素的样本将导致39%的数据被丢弃。另一方面，第二种方法通常会引入放大伪影，这可能会泄漏到最终模型输出中，导致例如模糊的样本。<br>

![figure2](images/SDXL-figure2.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相反，我们提议在UNet模型中**对原始图像分辨率进行条件化**，这在训练过程中是轻而易举的。具体来说，我们将图像的原始高度和宽度（即，在任何缩放之前）作为额外的条件提供给模型 $c_{size} = (h_{original},w_{original})$ 。每个组件都使用傅里叶特征编码进行独立嵌入，并将这些编码连接成一个单一的向量，然后通过将其添加到时间步骤嵌入[5]中输入给模型。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在推理过程中，用户可以通过这个尺寸条件设置图像的期望视觉分辨率。显然（见图3），模型已经学会将条件化的尺寸 $c_{size}$ 与分辨率相关的图像特征关联起来，这可以用来修改与给定提示(prompt)相对应的输出的外观。请注意，在图3中展示的可视化中，我们展示了由512×512模型生成的样本（详见第2.5节的细节），因为在我们最终的SDXL模型中，尺寸条件化的效果在之后的多方面（宽高比）微调中不太明显可见。<br>

![figure3](images/SDXL-figure3.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们通过在大小为 $512^{2}$ 的类条件ImageNet [4]上训练和评估三个LDM（Latent Diffusion Models）来定量评估这种简单但有效的条件化技术的效果：对于第一个模型（CIN-512-only），我们**丢弃**了所有至少**有一个边缘小于512像素的训练样本**，导致训练数据集**只有**7万个图像。对于CIN-nocond，我们使用所有的训练样本，但**不进行尺寸条件化**。尺寸条件化只用于CIN-size-cond。训练后，我们使用50个DDIM步骤[46]和(无分类器的)引导尺度为5 [13]为每个模型生成5k个样本，并计算IS [42]和FID [12]（与完整的验证集进行比较）。对于CIN-size-cond，我们始终生成以 $c_{size} = (512, 512)$ 为条件的样本。表2总结了结果，并验证了**CIN-size-cond在两个度量指标上改进了基准模型**。我们将CIN-512-only的性能下降归因于由于在**小训练数据集上过拟合而导致的泛化能力差**，而CIN-nocond中模糊样本分布的一种模式效应导致了降低的FID分数。请注意，尽管我们发现这些经典的定量评分不适合评估基础（文本到图像）DM（Data Models）的性能[40、37、38]（见附录F），但它们仍然是ImageNet上合理的度量指标，因为FID和IS的神经骨干已经在ImageNet上进行了训练。<br>

![table2](images/SDXL-table2.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将模型条件化为裁剪参数 图4的前两行展示了以前SD模型的典型故障模式：合成的对象可能会被裁剪，例如左侧示例中的猫的截断头部对于SD 1-5和SD 2-1。对于这种行为的直观解释是**在模型训练过程中使用了随机裁剪**：由于在DL框架（如PyTorch [32]）中整理批次需要相同大小的张量，典型的处理流程是（i）调整图像大小，使最短边与目标尺寸匹配，然后（ii）沿着较长的轴随机裁剪图像。虽然随机裁剪是一种自然的数据增强形式，但它可能泄露到生成的样本中，导致上述恶意效果。<br>

![figure4](images/SDXL-figure4.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了解决这个问题，我们提出了另一种简单但有效的条件化方法：在数据加载过程中，我们均匀采样裁剪坐标 $c_{top}$ 和 $c_{left}$（分别指定从左上角沿高度和宽度轴裁剪的像素数量的整数），并通过傅里叶特征嵌入将它们作为条件化参数输入模型中，类似于上面描述的尺寸条件化。然后，将连接的嵌入 $c_{crop}$ 用作附加的条件化参数。我们强调，这种技术不仅限于LDM，可以用于任何DM。注意，裁剪和尺寸条件化可以直接组合在一起。在这种情况下，我们将特征嵌入沿通道维度进行连接，然后将其添加到UNet中的时间步嵌入之前。算法1说明了如果应用了这种组合，我们在训练过程中如何采样 $c_{crop}$ 和 $c_{size}$ 。<br>

![algorithm1](images/SDXL-algorithm1.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据我们的经验，大规模数据集平均上是以对象为中心的，因此在推断过程中，我们将 $(c_{top}, c_{left})$ 设置为(0, 0)，从而从训练好的模型中获得以对象为中心的样本。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;请参见图5进行说明：通过调整 $(c_{top} , c_{left})$ ，我们可以成功地在推断过程中模拟裁剪的数量。这是一种条件化增强的形式，已经在各种形式的自回归模型[20]和最近的扩散模型[21]中使用过。<br>

![figure5](images/SDXL-figure5.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然像数据分桶[31]这样的其他方法成功地解决了同样的任务，但我们仍然从裁剪引起的数据增强中受益，同时确保它不会泄漏到生成过程中——我们实际上将其用于我们的优势，以更好地控制图像合成过程。此外，它易于实现，并且可以在训练过程中以在线方式应用，无需额外的数据预处理。<br>

## 2.3 多方位训练
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;真实世界的数据集包含尺寸和长宽比差异很大的图像（参见图2）。虽然文本到图像模型的常见输出分辨率是512×512或1024×1024像素的正方形图像，但我们认为这是一个相当不自然的选择，考虑到广泛分布和使用的横向（例如16:9）或纵向格式屏幕。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;受此启发，我们微调我们的模型以**同时处理多个长宽比**：我们遵循常见做法[31]，将数据分成不同长宽比的桶，其中像素数量尽可能接近 $1024^{2}$ 像素，相应地**按64的倍数变化高度和宽度**。在附录I中提供了用于训练的所有长宽比的完整列表。在优化过程中，训练批次(batch)由同一桶中的图像组成，并且我们在每个训练步骤中在不同的桶大小之间**交替选择**。此外，模型接收桶大小（或目标大小）作为条件，表示为一个整数元组 $car = (h_{tgt}, w_{tgt})$ , 类似于上面描述的尺寸和裁剪条件，将其嵌入到傅里叶空间中。<br>

![Multi-Aspect](images/SDXL-multi-aspect.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在实践中，我们将多方位训练(multi-aspect training)作为预训练阶段后的微调阶段，**预训练阶段模型采用固定的长宽比和分辨率**，并通过沿通道轴连接方式与第2.2节中介绍的条件化技术相结合。附录J中的图16提供了此操作的Python代码。请注意，裁剪条件化和多方位训练是**互补的操作**，而裁剪条件化仅适用于桶边界内（通常为64像素）。然而，为了实现的便利，我们选择保留这个控制参数用于多方位模型。<br>

## 2.4 改进的自编码器
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Stable Diffusion 是一个基于预训练、学习得到的（固定的）自编码器潜空间(vae)的LDM。虽然**大部分语义组合是由LDM[38]完成的**，但我们可以通过改进自编码器(VAE)来改善生成图像中的局部高频细节。为此，我们使用相同的自编码器架构对原始Stable Difussion 进行训练，但批量大小更大（256 vs 9），并额外使用**指数移动平均跟踪权重**。得到的自编码器在所有评估的重构指标中优于原始模型，请参见表3。我们在所有实验中使用这个自编码器。<br>

![table3](images/SDXL-table3.jpg)

## 2.5 Putting Everything Together
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们通过一个多阶段的过程来训练最终模型SDXL。SDXL使用来自第2.4节的自编码器，以及具有1000个 step 的离散时间扩散计划(discrete-time diffusion schedule)[14, 45]。<br>
- 首先，我们在内部数据集上对基本模型进行预训练（参见表1），该数据集的高度和宽度分布在图2中可视化，分辨率为256×256像素，batch size为2048，使用第2.2节中描述的尺寸和裁剪条件化进行了600,000次优化步骤。<br>
- 然后，我们继续在512×512像素图像上进行另外200,000次优化步骤的训练;
- 最后利用多方位训练(multi-spec)（第2.3节）结合偏移噪声[11, 25]水平为0.05，对不同长宽比（第2.3节，附录I）的约1024×1024像素区域训练模型。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**优化阶段** 根据经验，我们发现生成的模型有时会产生**局部质量较低的样本**，参见图6。为了提高样本质量，我们在相同的潜空间中训练了一个单独的LDM，该LDM专门用于处理高质量、高分辨率的数据，并在基础模型生成的样本上应用了噪声去噪过程，该过程由SDEdit [28](https://arxiv.org/pdf/2108.01073.pdf)引入。我们遵循[1](https://arxiv.org/pdf/2211.01324.pdf)的方法，将这个优化模型专门用于前200个（离散的）噪声尺度。在推理过程中，我们从基础SDXL模型中提取潜变量，直接在潜空间中使用优化模型进行扩散和去噪（参见图1），使用相同的文本输入。我们指出，这一步骤是可选的，但可以提高详细背景和人脸的样本质量，如图6和图13所示。<br>

![figure6](images/SDXL-figure6.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了评估我们的模型（包括有和没有优化阶段的版本），我们进行了用户研究，并让用户从以下四个模型中选择他们最喜欢的生成结果：SDXL、带有优化阶段的SDXL、稳定扩散1.5和稳定扩散2.1。结果表明，**带有优化阶段的SDXL是评分最高的选择**，并且在很大程度上优于稳定扩散1.5和2.1（胜率：带有优化阶段的SDXL：48.44％，SDXL基础版：36.93％，稳定扩散1.5：7.91％，稳定扩散2.1：6.71％）。请参见图1，该图还提供了完整流程的概述。然而，**当使用FID和CLIP等经典性能指标时，SDXL相对于先前方法的改进并未得到体现**，如图12所示，并在附录F中进行了讨论。这与Kirstain等人的研究结果[23]相一致，并进一步支持了他们的发现。<br>

# 3 未来的工作
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本报告介绍了对文本到图像合成基础模型稳定扩散的改进的初步分析。虽然我们在合成图像质量、提示符一致性和组合方面取得了显著的改进，但在接下来的内容中，我们讨论了一些可能进一步改进模型的方面：<br>
- 单阶段：目前，我们使用两阶段的方法(多阶段训练)和额外的优化模型从SDXL生成最佳样本。这导致需要将**两个庞大的模型加载到内存中**，影响了可访问性和采样速度。未来的工作应该探索提供等效或更好质量的单阶段方法的方式。<br>
- 文本合成：尽管规模和更大的文本编码器（OpenCLIP ViT-bigG [19]）有助于改善文本渲染能力，但引入字节级分词器[52, 27]或简单地将模型扩展到更大的尺寸[53, 40]可能进一步改善文本合成。<br>
- 架构：在这项工作的探索阶段，我们简要尝试了基于Transformer的架构，如UViT [16]和DiT [33]，但**没有看到立即的好处**。然而，我们仍然对经过精心的超参数研究最终能够实现更大的Transformer主导架构的扩展持乐观态度。<br>
- 蒸馏：虽然我们相对于原始的稳定扩散模型取得了显著的改进，但代价是推理成本的增加（包括VRAM和采样速度）。因此，未来的工作将专注于减少推理所需的计算量，并增加采样速度，例如通过引导蒸馏[29]、知识蒸馏[6, 22, 24]和渐进蒸馏[41, 2, 29]等方法。<br>
- 我们的模型是在[14]的离散时间公式(DDPM)中进行训练的，并且需要偏移噪声[11, 25]以获得美观的结果。Karras等人的EDM框架[21]是未来模型训练的一个有希望的候选方案，因为它在连续时间中的表述允许更大的采样灵活性，而且不需要噪声计划修正。<br>

