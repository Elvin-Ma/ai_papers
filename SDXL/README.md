# SDXL: Improving Latent Diffusion Models (LDM) for High-Resolution Image Synthesis(图像合成)

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们介绍了SDXL，一种用于文本到图像合成的潜空间扩散模型(LDM)。与先前版本的稳定扩散相比，SDXL利用了一个**三倍大的UNet骨干**：模型参数增加主要是由于**更多的注意力块**和**更大的交叉注意力上下文**，因为SDXL使用了第二个文本编码器(总共使用了两个文本编辑器)。我们设计了多种新颖的条件方案，并在**多个宽高比**上对SDXL进行训练。我们还引入了一个改进模型(refinement model)，该模型使用事后图像到图像技术**改善**SDXL生成的样本的视觉保真度。我们证明了与先前版本的稳定扩散相比，SDXL显示出了显著提高的性能，并且在与最先进的黑盒图像生成器相比的结果上取得了竞争力。秉承促进开放研究和提高大型模型训练和评估透明度的精神，我们提供代码和模型权重的访问权限。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;过去一年，在深度生成建模领域取得了巨大的突破，涵盖了各种数据领域，如自然语言[50]、音频[17]和视觉媒体[38, 37, 40, 44, 15, 3, 7]。在本报告中，我们专注于后者(视觉媒体)，并介绍了SDXL，这是稳定扩散的一个极大改进版本。稳定扩散是一种潜空间文本到图像扩散模型（DM），为近期的一系列进展提供了基础，例如3D分类[43]、可控图像编辑[54]、图像个性化[10]、合成数据增强[48]、图形用户界面原型设计[51]等。值得注意的是，应用范围非常广泛，涵盖了音乐生成[9]和从fMRI脑部扫描重建图像[49]等领域。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户研究表明，SDXL在性能上始终明显超过(surpasses)了所有先前版本的稳定扩散（见图1）。在本报告中，我们介绍了导致性能提升的设计选择，包括：<br>
1. 相比于先前的稳定扩散模型，使用了**3倍大的UNet骨干**（第2.1节）；
2. 使用了两种简单而有效的**附加条件技术**（第2.2节），不需要任何形式的额外监督；
3. 引入了一个独立的基于扩散的改进模型(refinement model)，该模型对SDXL生成的潜空间进行**噪声去噪处理**[28]，以提高其样本的视觉质量（第2.5节）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在视觉媒体创作领域，一个主要关注点是，虽然黑盒模型(black-box-models)通常被认为是最先进的，但其架构的不透明性阻碍了对其性能的准确评估和验证。这种缺乏透明度影响了可重现性，阻碍了创新，并阻止了社区在这些模型的基础上进行进一步的科学和艺术进展。此外，这些闭源策略使得以公正客观的方式评估这些模型的偏见(bias)和局限性变得具有挑战性，这对于它们的负责任和道德部署至关重要。通过SDXL，我们发布了一个开放模型，其性能与黑盒图像生成模型(eg: midjourney)具有竞争力（见图10和图11）。<br>

# 2 提升稳定扩散
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在本节中，我们介绍了我们对稳定扩散架构的改进。这些改进是**模块化的**，可以单独或一起使用来扩展任何模型。尽管以下策略是作为潜空间扩散模型（LDMs）[38]的扩展实现的，但它们中的大多数**也适用于像素空间**的对应模型。<br>

![figure1](images/SDXL-figure1.jpg)

## 2.1 架构与规模
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从Ho等人的开创性工作[14][DDPM](https://arxiv.org/pdf/2006.11239.pdf)和Song等人的工作[47]开始，它们证明了DM对于图像合成是强大的生成模型，Convolution UNet[39](https://arxiv.org/pdf/1505.04597.pdf)架构一直是基于扩散的图像合成的主导架构。然而，随着基础DM的发展[40, 37, 38]，底层架构不断演化：从添加自注意力和改进的上采样层[5](https://arxiv.org/pdf/2105.05233.pdf)，到用于文本到图像合成的交叉注意力[38](https://arxiv.org/pdf/2112.10752.pdf)，再到纯Transformer-based架构[33](https://arxiv.org/pdf/2212.09748.pdf)。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们遵循这一趋势，并且在Hoogeboom等人的研究[16]的基础上，将Transformer计算的大部分转移到UNet中的**较低级特征上**。与原始的稳定扩散架构不同，我们在UNet中**使用了不同类型的Transformer块分布**：出于效率考虑，我们省略了最高级特征层上的Transformer块，在**较低级别**使用2个和10个块，并完全删除了UNet中最低级别（8倍下采样）的层级-请参见表1，对比了稳定扩散1.x和2.x以及SDXL的架构。我们选择了一个更强大的预训练文本编码器，用于文本条件。具体来说，我们使用OpenCLIP ViT-bigG [19]结合CLIP ViT-L [34]，在通道轴上沿着倒数第二个文本编码器输出进行连接[1]。除了使用交叉注意力层来将模型与文本输入进行条件化外，我们还遵循[30]的方法，额外将模型与OpenCLIP模型的池化文本嵌入进行条件化。这些改变导致UNet中的模型参数数量为26亿(2.6B)个，详见表1。文本编码器总共有8.17亿个参数。<br>

![table1](images/SDXL-table1.jpg)

## 2.2 微调条件化
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在图像大小上对模型进行条件化LDM范例的一个**臭名昭著的缺点是**，由于其两阶段架构，训练模型需要一个最小的图像尺寸。解决这个问题的两种主要方法要么是**丢弃所有低于某个最小分辨率的训练图像**（例如，Stable Diffusion 1.4/1.5会丢弃所有尺寸低于512像素的图像），要么是对太小的图像进行放大。然而，根据所需的图像分辨率，前一种方法可能导致训练数据的大部分被丢弃，这可能会导致性能下降并影响泛化能力。我们在图2中对SDXL预训练的数据集可视化了这种效果。对于这个特定的数据选择，丢弃所有分辨率低于我们预训练分辨率 $256^{2}$ 像素的样本将导致39%的数据被丢弃。另一方面，第二种方法通常会引入放大伪影，这可能会泄漏到最终模型输出中，导致例如模糊的样本。<br>

![figure2](images/SDXL-figure2.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相反，我们提议在UNet模型中**对原始图像分辨率进行条件化**，这在训练过程中是轻而易举的。具体来说，我们将图像的原始高度和宽度（即，在任何缩放之前）作为额外的条件提供给模型 $c_{size} = (h_{original},w_{original})$ 。每个组件都使用傅里叶特征编码进行独立嵌入，并将这些编码连接成一个单一的向量，然后通过将其添加到时间步骤嵌入[5]中输入给模型。<br>



