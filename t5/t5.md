# T5: Text-to-Text Transfer Transformer

- [论文链接](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1910.10683.pdf)

# 0. Abstract
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;迁移学习是一种在进行下游任务的微调之前，首先对模型进行数据丰富的预训练的技术，在自然语言处理（NLP）中已经成为一种强大的技术。迁移学习的有效性催生了多样化的方法、方法论和实践。在本文中，我们通过引入一个统一的框架，将所有基于文本的语言问题转化为文本到文本的格式，来探索NLP中的迁移学习技术的现状。我们的系统研究比较了预训练目标、架构、无标签数据集、迁移方法和其他因素在几十个语言理解任务上的表现。通过结合我们探索的见解、规模以及我们的新的“巨大干净爬取语料库”，我们在许多涵盖摘要、问答、文本分类等基准测试中取得了最先进的结果。为了促进NLP迁移学习的未来工作，我们发布了我们的数据集、预训练模型和代码。<br>

关键词：迁移学习、自然语言处理、多任务学习、基于注意力的模型、深度学习。

# 1. 简介
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在训练自然语言处理（NLP）任务的机器学习模型时，通常需要使模型能够以适合下游学习的方式处理文本。这可以宽泛地看作是开发通用知识，使模型能够“理解”文本。这种知识可以从低层次（例如单词的拼写或含义）到高层次（例如大多数背包无法容纳大号音乐喇叭）的范围内。在现代机器学习实践中，提供这种知识很少是显式地完成的；相反，它通常作为辅助任务的一部分进行学习。例如，历史上常见的方法是使用单词向量（Mikolov等人，2013b，a；Pennington等人，2014）将单词映射到连续表示，其中理想情况下，相似的单词映射到相似的向量。这些向量通常是通过鼓励共现的单词在连续空间中靠近位置的目标进行学习的（Mikolov等人，2013b）。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最近，越来越普遍的做法是在数据丰富的任务上对整个模型进行预训练。理想情况下，这种预训练使模型能够发展出通用的能力和知识，然后可以将其转移到下游任务中。在将迁移学习应用于计算机视觉中（Oquab等人，2014；Jia等人，2014；Huh等人，2016；Yosinski等人，2014），预训练通常通过在大型标记数据集（例如ImageNet）上进行监督学习来完成（Russakovsky等人，2015；Deng等人，2009）。相比之下，现代NLP中的迁移学习技术通常使用无监督学习在无标签数据上进行预训练。最近，这种方法已经在许多常见的NLP基准测试中取得了最先进的结果（Devlin等人，2018；Yang等人，2019；Dong等人，2019；Liu等人，2019c；Lan等人，2019）。除了其实证的强大性外，对于NLP的无监督预训练尤其具有吸引力，因为通过互联网可以轻松获取大量的无标签文本数据，例如，每个月Common Crawl项目2会产生约20TB的从网页中提取的文本数据。这与神经网络非常契合，因为已经证明神经网络具有出色的可扩展性，即仅通过在更大的数据集上训练更大的模型，通常可以获得更好的性能（Hestness等人，2017；Shazeer等人，2017；Jozefowicz等人，2016；Mahajan等人，2018；Radford等人，2019；Shazeer等人，2018；Huang等人，2018b；Keskar等人，2019a）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种协同作用在最近的工作中产生了大量的迁移学习方法论，为NLP领域开发了广泛的预训练目标（Howard和Ruder，2018；Devlin等人，2018；Yang等人，2019；Dong等人，2019）、无标签数据集（Yang等人，2019；Liu等人，2019c；Zellers等人，2019）、基准测试（Wang等人，2019b，2018；Conneau和Kiela，2018）、微调方法（Howard和Ruder，2018；Houlsby等人，2019；Peters等人，2019）等等。这个蓬勃发展的领域中进展迅速、**技术多样的速度**可能会使不同算法之间的比较、新贡献的影响以及现有迁移学习方法的空间**难以理解**。出于对更严格理解的需求，我们利用一种统一的迁移学习方法，可以系统地研究不同的方法，并推动该领域的当前限制。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们工作的基本思想是**将每个文本处理问题都视为“文本到文本”问题**，即将文本作为输入，并产生新的文本作为输出。这种方法受到了先前用于NLP任务的统一框架的启发，包括将所有文本问题都视为问答问题（McCann等人，2018）、语言建模（Radford等人，2019）或跨度提取（Keskar等人，2019b）任务。重要的是，文本到文本的框架允许我们直接将相同的模型、目标、训练过程和解码过程应用于我们考虑的每个任务。通过在各种基于英语的NLP问题上评估性能，包括问答、文档摘要和情感分类等，我们利用这种灵活性。通过这种统一的方法，我们可以比较不同迁移学习目标、无标签数据集和其他因素的有效性，并通过扩展模型和数据集的规模超越以前考虑过的限制，探索NLP迁移学习的极限。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们强调，我们的目标不是提出新的方法，而是提供一个全面的视角来了解该领域的现状。因此，我们的工作主要包括对现有技术的调查、探索和实证比较。我们还通过扩展我们系统研究的见解（训练模型达到110亿个参数）来探索当前方法的限制，并在我们考虑的许多任务中获得最先进的结果。为了在这个规模上进行实验，我们引入了“Colossal Clean Crawled Corpus”（C4），这是一个由数百吉字节的干净英语文本组成的数据集，从网络上抓取而来。鉴于迁移学习的主要用途是在数据稀缺的环境中利用预训练模型，我们发布了我们的代码、数据集和预训练模型。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文的其余部分结构如下：在下一节中，我们讨论了我们的基础模型及其实现，我们将每个文本处理问题制定为文本到文本任务的过程，以及我们考虑的任务套件。在第3节中，我们提出了一系列大规模实验，探索NLP迁移学习领域。在该部分的最后一节（第3.7节），我们结合我们系统研究的见解，获得了各种基准测试的最先进结果。最后，我们在第4节中总结了我们的结果，并展望未来。<br>

# 2 setup
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在展示我们大规模实证研究的结果之前，我们回顾了理解我们结果所需的必要背景知识，包括Transformer模型架构和我们评估的下游任务。我们还介绍了我们将每个问题视为文本到文本任务的方法，并描述了我们创建的基于Common Crawl的无标签文本数据源——“Colossal Clean Crawled Corpus”（C4）。我们将我们的模型和框架称为“Text-to-Text Transfer Transformer”（T5）。<br>

## 2.1 模型
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;早期关于NLP迁移学习的结果利用了循环神经网络（Peters等人，2018；Howard和Ruder，2018），但最近更常见的是使用基于“Transformer”架构的模型（Vaswani等人，2017）。Transformer最初被证明对机器翻译非常有效，但随后在各种NLP环境中被广泛应用（Radford等人，2018；Devlin等人，2018；McCann等人，2018；Yu等人，2018）。由于它的广泛应用，我们研究的所有模型都基于Transformer架构。除了下面提到的细节和我们在第3.2节中探索的变体之外，我们与最初提出的架构**没有明显偏离**。我们不提供对该模型的全面定义，而是建议有兴趣的读者参考原始论文（Vaswani等人，2017）或后续教程3,4，以获得更详细的介绍。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Transformer的主要构建模块是自注意力（Cheng等人，2016）。自注意力是注意力的一种变体（Graves，2013；Bahdanau等人，2015），通过将每个元素替换为序列中其余元素的加权平均值来处理序列。原始的Transformer由编码器-解码器架构组成，旨在用于序列到序列（Sutskever等人，2014；Kalchbrenner等人，2014）任务。最近，常见的做法也是使用由单个Transformer层堆叠而成的模型，采用不同形式的自注意力来生成适用于语言建模（Radford等人，2018；Al-Rfou等人，2019）或分类和跨度预测任务（Devlin等人，2018；Yang等人，2019）的架构。我们在第3.2节中对这些架构变体进行了实证研究。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总体而言，我们的编码器-解码器Transformer实现与最初提出的形式（Vaswani等人，2017）非常接近。首先，将输入的token序列映射为嵌入序列，然后传递到编码器中。编码器由一堆“块”组成，每个块包含两个子组件：自注意力层和一个小的前馈网络。对每个子组件的输入应用层归一化（Ba等人，2016）。我们使用简化版本的层归一化，只对激活进行重新缩放，不应用加性偏置。在层归一化之后，将每个子组件的输入添加到其输出中，使用残差跳过连接（He等人，2016）。在前馈网络、跳过连接、注意力权重以及整个堆栈的输入和输出上应用了Dropout（Srivastava等人，2014）。解码器的结构与编码器类似，不同之处在于它在每个自注意力层之后包含一个标准的注意力机制，用于关注编码器的输出。解码器中的自注意机制还使用了一种自回归或因果自注意机制，它只允许模型关注过去的输出。最终解码器块的输出输入到一个具有softmax输出的稠密层中，其权重与输入嵌入矩阵共享。Transformer中的所有注意机制都被分解为独立的“头”，它们的输出在进一步处理之前进行连接。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于自注意力是无序的（即它是集合上的操作），因此通常需要为Transformer提供显式的位置信号。虽然原始的Transformer使用了正弦形式的位置信号或学习的位置嵌入，但最近更常见的做法是使用相对位置嵌入（Shaw等人，2018；Huang等人，2018a）。相对位置嵌入不是为每个位置使用固定的嵌入，而是根据自注意机制中“键”和“查询”之间的偏移量生成不同的学习嵌入。我们使用一种简化的位置嵌入形式，其中每个“嵌入”只是一个标量，添加到用于计算注意力权重的相应逻辑回归中。为了提高效率，我们还在模型的所有层之间共享位置嵌入参数，尽管在给定层内，每个注意头使用不同的学习位置嵌入。通常情况下，学习了一定数量的嵌入，每个嵌入对应一系列可能的键-查询偏移范围。在这项工作中，我们为所有模型学习了32个嵌入，其范围按对数增加到偏移量为128，超过这个范围的所有相对位置都分配到相同的嵌入中。请注意，给定的层对于超过128个标记的相对位置不敏感，但后续层可以通过组合前面层的局部信息来对更大的偏移产生敏感。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总之，我们的模型与Vaswani等人提出的原始Transformer基本等效（2017），只是在去除了层归一化的偏置、将层归一化放置在残差路径之外，并使用了不同的位置嵌入方案。由于这些架构变化与我们在迁移学习的实证调查中考虑的实验因素无关，我们将留待将来进行对其影响的消融研究。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;作为我们研究的一部分，我们对这些模型的可扩展性进行了实验，即当这些模型具有更多的参数或层时，它们的性能如何变化。训练大型模型可能并不简单，因为它们可能无法适应单台计算机，并且需要大量的计算资源。因此，我们采用了模型并行和数据并行的组合，并在Cloud TPU Pods的“切片”上训练模型。TPU Pod是多机架ML超级计算机，包含1,024个TPU v3芯片，通过高速2D网格互连与支持的CPU主机机器相连。我们利用Mesh TensorFlow库（Shazeer等人，2018）来方便实现模型并行和数据并行（Krizhevsky，2014）。<br>

## 2.2 庞大的爬取语料库清理
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;先前关于自然语言处理的迁移学习的许多研究都利用了大规模的无标签数据集进行无监督学习。本文旨在衡量无标签数据的质量、特征和规模对模型的影响。为了生成符合我们需求的数据集，我们利用Common Crawl作为从网络上爬取的文本的数据源。Common Crawl之前已被用作自然语言处理的文本数据源，例如用于训练n-gram语言模型（Buck等，2014），作为常识推理的训练数据（Trinh和Le，2018），用于挖掘机器翻译的平行文本（Smith等，2013），以及作为预训练数据集（Grave等，2018；Zellers等，2019；Liu等，2019c），甚至仅作为测试优化器的巨大文本语料库（Anil等，2019）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Common Crawl是一个公开可用的网络存档，提供通过从爬取的HTML文件中删除标记和其他非文本内容而生成的“网络提取文本”。这个过程每个月会产生约20TB的爬取文本数据。不幸的是，其中大部分文本并不是自然语言。相反，它主要由无意义的文字或样板文字组成，例如菜单、错误消息或重复的文本。此外，大量爬取的文本包含的内容对于我们考虑的任何任务来说都不太可能有帮助（包括冒犯性语言、占位符文本、源代码等）。为了解决这些问题，我们使用了以下启发式方法来清理Common Crawl的网络提取文本：<br>

- 我们只保留以结束标点符号结尾的行（即句号、感叹号、问号或结束引号）。
- 我们丢弃了少于3个句子的页面，并只保留包含至少5个单词的行。
- 我们删除了任何包含“脏话、淫秽话、下流话或其他不良词语列表”中的任何单词的页面。
- 许多爬取的页面包含要求启用Javascript的警告，因此我们删除了包含“Javascript”一词的任何行。
- 一些页面包含占位符的“lorem ipsum”文本；我们删除了任何包含“lorem ipsum”短语的页面。
- 一些页面无意中包含代码。由于花括号“{”在许多编程语言中出现（例如广泛用于Web的Javascript），但在自然文本中不常见，因此我们删除了包含花括号的页面。
- 由于一些爬取的页面来自维基百科并带有引用标记（例如[1]、[citation needed]等），我们删除了这类标记。
- 许多页面包含样板政策通知，因此我们删除了包含“使用条款”、“隐私政策”、“Cookie政策”、“使用Cookies”、“使用Cookie”或“使用Cookies”等字符串的行。
- 为了去重数据集，我们丢弃了数据集中出现超过一次的任何三句连续的部分，只保留其中一次。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此外，由于我们的大部分下游任务侧重于英文文本，我们使用langdetect7来过滤出未被分类为英文的页面，概率至少为0.99。我们的启发式方法受到以往关于使用Common Crawl作为NLP数据源的工作的启发：例如，Grave等人（2018）也使用自动语言检测器过滤文本并丢弃短行，Smith等人（2013）和Grave等人（2018）都进行了行级去重。然而，我们选择创建一个新的数据集，因为先前的数据集使用了更有限的过滤启发式方法，不公开可用，或者在范围上有所不同（例如仅限于新闻数据（Zellers等，2019；Liu等，2019c），仅包含创作共用内容（Habernal等，2016），或者侧重于机器翻译的平行训练数据（Smith等，2013））。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了组装我们的基础数据集，我们下载了2019年4月的网络提取文本，并应用了上述过滤方法。这产生了一个文本集合，不仅比大多数用于预训练的数据集大数个数量级（约750GB），而且包含相对清洁和自然的英文文本。我们将此数据集命名为“庞大的清理爬取语料库”（简称C4），并作为TensorFlow Datasets的一部分发布。我们在第3.4节中考虑了使用此数据集的各种替代版本的影响。<br>

## 2.3 下游任务
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在本文中，我们的目标是**衡量通用语言学习能力**。因此，我们研究了多个不同的基准任务的下游性能，包括机器翻译、问答、抽象摘要和文本分类。具体而言，我们衡量了GLUE和SuperGLUE文本分类元基准、CNN/Daily Mail抽象摘要、SQuAD问答以及WMT英德、英法和英罗马尼亚翻译任务的性能。所有数据均来自于TensorFlow Datasets。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GLUE（Wang等，2018）和SuperGLUE（Wang等，2019b）分别包含一系列文本分类任务，旨在测试通用语言理解能力：<br>

- 句子可接受性判断（CoLA（Warstadt等，2018））
- 情感分析（SST-2（Socher等，2013））
- 释义/句子相似度（MRPC（Dolan和Brockett，2005）、STS-B（Cer等，2017）、QQP（Iyer等，2017））
- 自然语言推理（MNLI（Williams等，2017）、QNLI（Rajpurkar等，2016）、RTE（Dagan等，2005）、CB（De Marneff等，2019））
- 共指消解（WNLI和WSC（Levesque等，2012））
- 句子补全（COPA（Roemmele等，2011））
- 词义消歧（WIC（Pilehvar和Camacho-Collados，2018））
- 问答（MultiRC（Khashabi等，2018）、ReCoRD（Zhang等，2018）、BoolQ（Clark等，2019））

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们使用GLUE和SuperGLUE基准所提供的数据集。为了简化起见，在微调时，我们将GLUE基准中的所有任务（类似地，对于SuperGLUE也是如此）视为单个任务，通过连接所有组成数据集进行处理。正如Kocijan等人（2019）建议的那样，我们还将Definite Pronoun Resolution（DPR）数据集（Rahman和Ng，2012）包含在组合的SuperGLUE任务中。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CNN/Daily Mail（Hermann等人，2015）数据集最初被引入作为一个问答任务，但被Nallapati等人（2016）改编为文本摘要任务；我们使用See等人（2017）的非匿名版本作为抽象摘要任务。SQuAD（Rajpurkar等人，2016）是一个常见的问答基准。在我们的实验中，模型会接收问题及其上下文，并逐个生成答案的标记。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于WMT英德翻译任务，我们使用与（Vaswani等人，2017）相同的训练数据（即News Commentary v13、Common Crawl、Europarl v7），并使用newstest2013作为验证集（Bojar等人，2014）。对于英法翻译，我们使用2015年的标准训练数据和newstest2014作为验证集（Bojar等人，2015）。对于英罗马尼亚翻译，这是一个标准的资源较少的机器翻译基准，我们使用了来自WMT 2016的训练和验证集（Bojar等人，2016）。请注意，我们只在英文数据上进行预训练，因此为了学会翻译，模型需要学会生成新语言的文本。<br>

## 2.4 Input and output
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了**在上述多样化的任务集上训练单个模型**，我们将所有考虑的任务转化为“文本到文本”（text-to-text）的格式，即模型接收一些文本作为上下文或条件，并被要求生成一些输出文本。这个框架为预训练和微调提供了一致的训练目标。具体而言，无论任务是什么，模型都使用最大似然目标（使用“teacher forcing”（Williams和Zipser，1989））进行训练。为了指定模型应执行的任务，我们在将原始输入序列输入模型之前，添加了特定于任务的（文本）前缀。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;举个例子，如果要求模型将句子“That is good.”从英语翻译成德语，模型将接收到序列“translate English to German: That is good.”并被训练输出“Das ist gut.”。对于文本分类任务，模型只需预测与目标标签对应的单个词。例如，在MNLI基准测试中（Williams等人，2017），目标是预测前提是否蕴含（“entailment”），与之矛盾（“contradiction”）或两者都不是（“neutral”）。通过我们的预处理，输入序列变为“mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.”，相应的目标词为“entailment”。请注意，如果我们的模型在文本分类任务上输出的文本不对应任何可能的标签（例如，如果模型在任务的唯一可能标签是“entailment”、“neutral”或“contradiction”时输出了“hamburger”），那么我们总是将模型的输出视为错误，尽管我们从未观察到我们训练的任何模型出现这种行为。注意，用于给定任务的文本前缀的选择实际上是一个超参数；我们发现更改前缀的确切措辞对结果影响有限，因此没有对不同前缀选择进行大量实验。我们在图1中展示了我们的文本到文本框架，并提供了一些输入/输出示例。在附录D中，我们提供了我们研究的每个任务的预处理输入的完整示例。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们的文本到文本框架遵循之前的工作，将多个自然语言处理任务转化为一个通用的格式：McCann等人（2018）提出了“**自然语言十项全能**”（Natural Language Decathlon），这是一个使用一致的问答格式进行十个自然语言处理任务的基准。自然语言十项全能还规定所有模型必须是多任务的，即能够同时处理所有任务。相反，我们允许对每个单独的任务进行单独微调，并使用**简短的任务前缀**而不是明确的问答格式。Radford等人（2019）通过将一些输入作为前缀输入模型，然后自回归地对输出进行采样，评估了语言模型的零样本学习能力。例如，自动摘要是通过输入一个文档，然后输入文本“TL;DR:”（太长，没时间读，常见的缩写）来完成的，然后通过自回归解码来预测摘要。我们主要考虑显式地使用编码器处理输入，然后使用单独的解码器生成输出的模型，并且我们专注于迁移学习而不是零样本学习。最后，Keskar等人（2019b）将许多自然语言处理任务统一为“跨度提取”，其中与可能的输出选择对应的文本附加到输入中，并且模型被训练以提取与正确选择对应的输入跨度。与此相反，我们的框架还允许生成型任务，如机器翻译和抽象摘要，其中不可能列举出所有可能的输出选择。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们能够直接将我们考虑的所有任务都转换成文本到文本的格式，只有STS-B是个例外，它是一个回归任务，目标是预测1到5之间的相似度分数。我们发现大多数分数的注释以0.2的增量进行标注，因此我们只需将任何分数四舍五入到最接近的0.2的增量，并将结果转换为数字的字符串表示形式（例如，浮点数值2.57将映射为字符串“2.6”）。在测试时，如果模型输出的字符串对应于1到5之间的数字，我们将其转换为浮点数值；否则，我们将模型的预测视为错误。这有效地将STS-B回归问题转化为一个21类分类问题。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另外，我们还将Winograd任务（GLUE中的WNLI，SuperGLUE中的WSC以及我们添加到SuperGLUE的DPR数据集）转换为更简单、更适合文本到文本框架的格式。Winograd任务的示例包含一个包含模糊代词的文本段落，该代词可能指的是段落中的多个名词短语之一。例如，段落可能是“市议会拒绝向示威者发放许可证，因为他们担心会发生暴力事件。”，其中包含模糊代词“他们”，可能指的是“市议会”或“示威者”。我们通过在文本段落中突出显示模糊代词，并要求模型预测它所指的名词，将WNLI、WSC和DPR任务转换为文本到文本问题。上面提到的示例将转换为输入“市议会拒绝向示威者发放许可证，因为*他们*担心会发生暴力事件。”，并且模型将被训练以预测目标文本“市议会”。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于WSC，示例包含段落、模糊代词、候选名词和一个True/False标签，反映候选名词是否与代词匹配（忽略任何冠词）。我们只训练具有“True”标签的示例，因为对于具有“False”标签的示例，我们不知道正确的名词目标。对于评估，如果模型输出的单词是候选名词短语中单词的子集（或反之亦然），我们分配一个“True”标签，否则分配一个“False”标签。这样大约会删除WSC训练集的一半，但是DPR数据集会增加约1000个代词消解示例。DPR的示例带有正确的指代名词的注释，因此很容易使用该数据集的上述格式。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;WNLI的训练集和验证集与WSC的训练集有很大的重叠。为了避免将验证示例泄漏到训练数据中（在第3.5.2节的多任务实验中是一个特别的问题），因此我们从不在WNLI上进行训练，也不在WNLI验证集上报告结果。由于WNLI验证集相对于训练集是“对抗性”的，即验证示例都是训练示例相反标签的轻微扰动版本，因此省略在WNLI验证集上的结果是标准做法（Devlin等人，2018）。因此，每当我们在验证集上报告结果时（除了在第3.7节中在测试集上呈现结果的情况下），我们不将WNLI包括在平均GLUE得分中。将WNLI示例转换为上述的“指代名词预测”变体稍微复杂一些；我们在附录B中描述了这个过程。<br>

# 3 实验













