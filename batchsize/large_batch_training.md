# 一种大批量训练的经验模型(An Empirical Model of Large-Batch Training)

- [paper 链接](https://arxiv.org/pdf/1812.06162)

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在越来越多的领域中，已经证明深度学习模型**可以使用相对较大的batch size进行训练，而不会牺牲(sacrificing)数据效率**。然而，这种大规模数据并行处理的限制似乎在**不同的领域之间有所不同**，从ImageNet中的数万 batches 到玩Dota 2游戏的RL代理的数百万批次不等。据我们所知，对于为什么批处理大小的限制不同以及如何在新的领域中选择正确的批处理大小，存在有限的概念理解。在本文中，我们证明了一种简单易行且易于测量的统计量，称为**梯度噪声尺度，可以预测许多领域和应用中最大的有用batch size**，包括一些监督学习数据集（MNIST、SVHN、CIFAR10、ImageNet、Billion Word）、强化学习领域（Atari和Dota）甚至生成模型训练（在SVHN上的自编码器）。我们发现，噪声尺度在训练过程中随着损失的减少而增加，并且主要依赖于模型大小来改进模型性能。我们基于经验的理论还**描述了计算效率和时间效率之间的权衡**，并提供了**自适应批处理大小**训练的好处的大致模型。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;过去几年中，用于训练深度学习模型的计算量呈现出快速增长[AH18]。这种增长的一个重要推动因素，同时也是限制因素，是并行性(即训练过程可以有效地分布在多个设备上的程度)。无论总的计算量有多少，如果模型训练无法充分并行化，那么可能需要太长的串行时间，从而在实践上是不可行的。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在深度学习中，非常常见的并行性来源是**数据并行性**，它涉及将数据批次分割到多个设备上，然后对grad 进行allreduce并应用。数据并行性要求设备之间进行快速通信，同时也要求**大batch在算法上对加速学习起到有效作用**。最近的一些论文经验证明，**在特定的数据集或任务上，大批次大小可以在训练中实现几乎线性的加速，而不会严重损害样本效率或泛化能力**。例如，已经成功地使用了8千个[GDG+17]、16千个[SKYL17]、32千个[YGG17, YZH+17, ASF17]，甚至64千个[JSH+18]样本来训练ImageNet，而数千个样本的批次大小在语言模型和生成模型中也是有效的[OEGA18, PKYC18, BDS18]。这种现象并不局限于监督学习：在强化学习中，使用超过一百万个time step（并行运行数万个环境）的批次大小已经在Dota玩家代理中被使用[BCD+18]，甚至在简单的Atari环境中，数千个time step的批次大小也被证明是有效的[AAG+18, HQB+18, SA18]。这些发现使得大量的数据和计算能够在合理的时间内有效地注入到模型中，从而在监督学习、强化学习和其他领域中实现更强大的模型。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然而，对于给定的数据集和模型，我们很难预测我们可以合理使用多大的批处理大小，为什么该数字取特定值，以及如果使用不同的数据集或模型，我们预期它会有何不同。例如，为什么在训练Dota代理时似乎可以使用超过一百万的批处理大小，而在训练图像识别模型时只能使用数千或数万的批处理大小？在实践中，研究人员倾向于仅仅尝试不同的批处理大小，并观察哪种效果最好，但这种方法的缺点是，**大batch size通常需要仔细调整才能发挥有效作用（例如，它们可能需要一个预热期或非常规的学习率调度）**，因此可以使用大batch size的事实可能在很长一段时间内被忽视。例如，Atari和ImageNet任务在几年的时间里通常使用的批处理大小比现在被认为是可能的要小得多。提前知道我们预期的有效批处理大小将对训练新模型具有重大的实际优势。<br>

![figure1](images/large-batch-training-figure1.png)

*(图1：在将模型训练到一定性能水平的过程中，时间和计算资源的权衡关系形成了帕累托前沿（左图）。训练时间和计算成本分别主要由优化步骤的数量和处理的训练样本数量确定。我们可以以更多计算资源的代价更快地训练模型。右图显示了通过将模型训练到不同性能水平来获得的Atari Breakout游戏的帕累托前沿的具体示例。成本和训练时间取决于计算架构，并且仅显示近似值。)*

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在本论文中，我们试图回答其中一些问题。我们测量了一个简单的经验统计量，即**梯度噪声尺度（实质上是衡量训练样本间梯度信噪比的指标）**，并展示了它能够大致预测各种任务的**最大有效batch size**。我们的模型还预测了计算/时间权衡曲线的具体形状，如图1所示。我们的贡献是一个基本理论和对该理论的广泛经验测试的结合。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在概念层面上，我们建立了一个框架，在一些基本假设下，预测训练应该在批处理大小接近噪声尺度时几乎线性地进行并行化，之后应该平滑但相对快速地转变为进一步的并行化提供的收益较小的状态。此外，我们预期随着模型变得更准确，噪声尺度应该在训练过程中增加，并且对于更复杂的任务来说，噪声尺度应该更大，但不应强烈依赖于模型大小本身。我们还对根据噪声尺度在训练过程中动态调整批处理大小所能期望的效率收益进行了分析。最后，我们预测，在其他条件相同的情况下，由于环境的随机性和信用分配问题引入的额外方差，噪声尺度在复杂的强化学习任务中倾向于更大。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在实证方面，我们在监督学习、强化学习和生成模型的8个任务中验证了这些预测，包括ImageNet、CIFAR-10、SVHN、MNIST、BillionWord、Atari、OpenAI的Dota代理[BCD+18]以及用于图像的变分自编码器。对于每个任务，我们展示了噪声尺度准确预测最大可用批处理大小（在数量级上），并且并行性的收益下降符合理论预测的方式。我们还展示了噪声尺度在训练过程中的增加，并展示了通过动态批处理大小调整获得的效率提升。噪声尺度最终会随着模型的性能提高而增大，但这似乎是因为更高性能的模型仅仅实现了更好的损失。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本论文的其余部分组织如下。在第2节中，我们推导了关于噪声尺度、数据并行性和批处理大小的简单概念图，并解释了它对于最佳批处理大小以及它们在训练过程中和不同任务之间如何变化的预测。在此基础上，我们在第2.3节中进一步研究了训练效率。然后在第3节中，我们对第2节中的预测进行了实证测试，并探讨了噪声尺度如何随数据集、模型大小和学习范式（监督学习与强化学习与生成模型）的变化。第4节介绍了相关工作，第5节讨论了这些结果的影响以及可能的未来实验。<br>

# 2 理论和梯度噪声尺度的预测
## 2.1 直观图像
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在详细讨论梯度噪声尺度和批处理大小之前，先给出一个直观的图像是有用的。假设我们通过随机梯度下降（SGD）来优化一个函数。存在一个真实的优化景观，对应于整个数据集（或更抽象地说，从中抽取的分布）上的损失。当我们使用有限的batch size进行SGD更新时，我们在**逼近这个真实损失的梯度**。我们应该如何决定使用什么batch size呢？<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当批处理大小非常小的时候，近似值的方差会非常高，导致梯度更新主要是噪声。**连续应用这些SGD更新将平均掉方差并使我们总体上朝着正确的方向前进，但是参数的个别更新并不会很有用**，而且我们几乎可以通过并行聚合(aggregating)这些更新（update）并一次性应用它们（换句话说，使用更大的批处理大小）来达到几乎相同的效果。有关大批量训练和小批量训练的对比，请参见图2。<br>

![figure2](images/large-batch-training-figure2.png)

*(图2：较少噪声的梯度估计允许SGD类型的优化器采取更大的步长(learning rate)，从而在较少的迭代次数(iterations)内收敛(convergence)。为了说明这一点，我们展示了使用动量在二次损失函数上的两条优化轨迹，其中步长和梯度中添加的人工噪声量不同。）*

**注释：较少噪声的梯度估计允许SGD类型的优化器采取更大的步长，这一点是采用较少step 就可以收敛的根本原因** <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另一方面，当批处理大小非常大时，batch gradient将几乎完全匹配真实梯度，并且相应地，两个随机抽样的批次将具有**几乎相同的梯度**。因此，将批处理大小增加一倍**几乎不会改善更新** ,我们将使用两倍的计算量, 但收益很小。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;直观地说，从第一个阶段（其中增加批处理大小几乎可以实现**完全线性加速**）到第二个阶段（其中增加批处理大小主要浪费计算资源）的**转变**大致应发生在**梯度的噪声和信号之间达到平衡的地方**——即**梯度的方差与梯度本身的尺度相当的地方**。将这种启发式观察形式化，可以得到梯度噪声尺度。<br>

![figure1](images/large-batch-training-figure1.png)

*(图1：在将模型训练到一定性能水平的过程中，时间和计算资源的权衡关系形成了帕累托前沿（左图）。训练时间和计算成本分别主要由优化步骤的数量和处理的训练样本数量确定。我们可以以更多计算资源的代价更快地训练模型。右图显示了通过将模型训练到不同性能水平来获得的Atari Breakout游戏的帕累托前沿的具体示例。成本和训练时间取决于计算架构，并且仅显示近似值。)*

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种情况在图1中以图示方式展示。对于给定的模型，我们希望在**尽可能短的墙时间（wall time）（x轴）内进行训练**，同时**尽可能少地使用总计算量（y轴）——这是并行化的通常目标**。改变批量大小将使我们在这两者之间沿着一条权衡曲线移动。**最初，我们可以增加批量大小而不会显著增加总计算量，然后在某个“转折点”处，两者之间存在实质性的权衡，最后，当批量大小很大时，我们无法在训练时间上再获得进一步的收益**。在下面的概念和实验结果中，我们对这些概念进行了形式化(formalize)，并展示了曲线的弯曲（因此近似的最大有效批量大小）实际上大致由**噪声尺度**确定。<br>

## 2.2 gradients、batches 和梯度噪声尺度(Gradient Noise Scale)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们现在将对第2.1节中描述的直觉进行形式化。考虑一个由变量 $θ ∈ R^{D}$ 参数化的模型，其性能由损失函数L(θ)评估。损失函数是通过对数据点x上的分布ρ(x)进行平均得到的。每个数据点x都有一个相关的损失函数 $L_{x}(θ)$ ，而完整的损失由 $L(θ) = E_{x∼ρ}[Lx(θ)]$ 给出。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们希望使用类似SGD的优化器来最小化L(θ)，因此相关的量是梯度G(θ) = ∇L(θ)。然而，直接优化L(θ)可能是低效的，甚至不可能，因为它需要在每一次优化步骤中处理**整个数据分布**。相反，我们通过对来自ρ的**一组样本进行平均来获得梯度的估计(estimate)**，这组样本称为批次(batch)：<br>

![formula1](images/large-batch-training-formula1.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个近似(estimate)形成了随机(stochastic)优化方法（如小批量(mini-batch)随机梯度下降（SGD）和Adam [KB14]）的基础。梯度现在是一个随机变量，其期望值（在随机批次上取平均）由真实梯度给出。其方差与批次大小B的倒数成比例：<br>

![formula2](images/large-batch-training-formula2.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中每个示例的协方差矩阵定义为：<br>

![formula3](images/large-batch-training-formula3.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关键点在于小批量梯度(mini-batch gradients)提供了真实梯度的噪声估计(estimate)，而**较大的batch提供了更高质量的估计**。我们对梯度在优化目的(purpose)中的有用性随着B的变化感兴趣，以及如何通过这一点来指导我们选择一个好的B。我们可以通过将梯度中的噪声与我们可以从单个梯度更新中期望的**真实损失的最大改善**联系起来来实现这一点。首先，设**G表示真实梯度，H表示参数值θ处的真实Hessian矩阵**。如果我们通过某个**向量V扰动参数θ**，使其变为 $θ - \epsilon V$ ，其中 $\epsilon$ 是步长，我们可以将该新点处的真实损失在 $\epsilon$ 的二次阶展开：<br>

![formula4](images/large-batch-training-formula4.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果我们可以访问无噪声的真实梯度G并用它来扰动参数，那么当V = G时，方程2.4将通过设定 $\epsilon = \epsilon_{max} = \frac{|G|^{2}}{G^{T}HG} $ 来最小化. 然而，在现实中，我们只能访问由大小为B的批次**估计**得到的**带噪声的梯度G_{est}**，因此我们能做的最好的是将期望 $E[L(θ − εG_{est})]$ 相对于ε最小化。可以使用方程2.2来评估这个期望值。<br>

**使得近似泰勒级数下降最多的最优步长为 $\epsilon = \frac{g^Tg}{g^THg}$ ,最坏情况下 g 与 H 最大特征值 $λ_{max}$ 对应的特征向量对齐，则最优步长是 $\frac{1}{λ_{max}}$ , 当我们要最小化的函数能用二次函数很好地近似的情况下，Hessian矩阵的特征值决定了学习率的量级** <br>

![formula5](images/large-batch-training-formula5.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相对于ε最小化这个方程得到：<br>

![formula6](images/large-batch-training-formula6.png)

作为最优步长，这将产生从带噪声的梯度中获得的最佳损失改善：<br>

![formula7](images/large-batch-training-formula7.png)

如上所述，我们将噪声尺度定义为：

![formula8](images/large-batch-training-formula8.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;请注意，我们对噪声尺度的定义**与完整训练集的大小无关**。如果我们使用的步长大于两倍的 $ε_{opt}$ ，损失可能会增加，导致发散，如图3所示。<br>

![figure3](images/large-batch-training-figure3.png)

*(图3：较大的批次大小平均产生更接近真实梯度的估计梯度。当估计梯度接近真实梯度时，可以使用较大的步长，以便每步可以取得更多进展。左图：在小批次大小下使用较大步长可能导致不稳定，如二次损失所示。右图：方程2.6预测，在噪声尺度B之后，较大的批次大小变得不那么有帮助，训练速度降至最大可能速度的50%。）* <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;尽管上述推导中存在许多无根据的假设，但我们将发现方程2.7和2.8对大批量训练的行为提供了有益的指导，即使使用其他优化器（包括动量法、Adam和RMSProp）也是如此。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于噪声尺度与学习率的依赖关系的讨论，请参阅附录C中关于训练的“temperature”的部分。<br>

**推论和简化**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;方程2.7暗示着当Batch远小于噪声尺度时，即 $B << B_{noise}$ 时，分母中的第二项占主导(denominator)地位，因此增加批次大小B线性增加了损失的进展。这是**小批次的阶段**，其中Batch size的增加**线性加速了训练**。相比之下，当 $B >= B_{noise}$ 时，第一项占主导地位，因此增加B几乎对损失的进展没有影响。这是大batch的阶段，增加批次大小不会加速训练，**只会浪费计算资源**；两者之间的切换发生在 $B ≈ B_{noise}$ 处（参见图3）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于存在Hessian矩阵H，在方程2.8中计算噪声尺度需要一些额外的计算开销。我们可以通过在各种批次大小B下进行一系列线性搜索，测量 
 $∆L_{opt}(B)$ ，并将结果拟合到方程2.7中来估计它。这使我们能够估计 $B_{noise}$ ，并通过实证测试验证方程2.7是否真正适用于数据（我们在第3节中对这些局部测试进行了更详细的讨论）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 如果我们做一个（不现实的）假设，即优化问题具有完美的条件（即**Hessian矩阵是单位矩阵的倍数**），情况会变得更简单。如果是这种情况，那么方程2.8可以简化为：<br>

![formula9](images/large-batch-training-formula9.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个公式表明**噪声尺度等于各个梯度分量方差的总和，除以梯度的全局范数**。实质上，**它衡量了梯度与其方差相比的大小**。它也是估计梯度和真实梯度在L2空间中变得接近的尺度（具有非平凡的点积）的度量方式。预期的归一化L2距离由以下公式给出：<br>

![formula10](images/large-batch-training-formula10.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实际上，我们发现 $B_{simple}$ 和 $B_{noise}$ **通常只相差一个小的乘法常数因子**，尤其是当我们采用常见的训练方案改善条件时。在我们的实证工作中，有时我们会计算 $B_{noise}$ ，但主要计算 $B_{simple}$ ，因为它需要较少的计算开销。在附录A.1中，我们提供了一种极其简单的方法，在数据并行训练的情况下，几乎没有额外的开销来测量这个简化的噪声尺度(公式如下)。<br>

![formula-a-1-2](images/large-batch-training-formula-a-1-2.png)

## 2.3 数据/时间效率权衡的预测结果
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到目前为止，我们的分析仅涉及损失函数空间中的一个点。但在第3节中，我们将展示方程2.7非常准确地预测了**训练速度与批次大小的依赖关系**，即使是对于在损失函数空间中涵盖许多点的完整训练运行。通过对方程2.7在多个优化步骤中进行平均（请参见附录D），我们找到了**训练速度和数据效率**之间的简单关系：<br>

![formula11](images/large-batch-training-formula11.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这里，S和 $S_{min}$ 分别表示达到特定性能水平所需的实际步骤数和最小可能步骤数，E和 $E_{min}$ 分别表示达到相同性能水平所需的实际处理的训练样例数和最小可能训练样例数。由于我们使用固定的batch size进行训练，我们有 $E_{tot} = B*S_{tot}$ 。我们通过对上述方程进行经验拟合来定义临界批次大小，如下所示：<br>

![formula12](images/large-batch-training-formula12.png)

**使得近似泰勒级数下降最多的最优步长为 $\epsilon = \frac{g^Tg}{g^THg}$ ,最坏情况下 g 与 H 最大特征值 $λ_{max}$ 对应的特征向量对齐，则最优步长是 $\frac{1}{λ_{max}}$ , 当我们要最小化的函数能用二次函数很好地近似的情况下，Hessian矩阵的特征值决定了学习率的量级** <br>

![formula5](images/large-batch-training-formula5.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相对于ε最小化这个方程得到：<br>

![formula6](images/large-batch-training-formula6.png)

作为最优步长，这将产生从带噪声的梯度中获得的最佳损失改善：<br>

![formula7](images/large-batch-training-formula7.png)

如上所述，我们将噪声尺度定义为：

![formula8](images/large-batch-training-formula8.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;请注意，我们对噪声尺度的定义**与完整训练集的大小无关**。如果我们使用的步长大于两倍的 $ε_{opt}$ ，损失可能会增加，导致发散，如图3所示。<br>

![figure3](images/large-batch-training-figure3.png)

*(图3：较大的批次大小平均产生更接近真实梯度的估计梯度。当估计梯度接近真实梯度时，可以使用较大的步长，以便每步可以取得更多进展。左图：在小批次大小下使用较大步长可能导致不稳定，如二次损失所示。右图：方程2.6预测，在噪声尺度B之后，较大的批次大小变得不那么有帮助，训练速度降至最大可能速度的50%。）* <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;尽管上述推导中存在许多无根据的假设，但我们将发现方程2.7和2.8对大批量训练的行为提供了有益的指导，即使使用其他优化器（包括动量法、Adam和RMSProp）也是如此。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于噪声尺度与学习率的依赖关系的讨论，请参阅附录C中关于训练的“temperature”的部分。<br>

**推论和简化**
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;方程2.7暗示着当Batch远小于噪声尺度时，即 $B << B_{noise}$ 时，分母中的第二项占主导(denominator)地位，因此增加批次大小B线性增加了损失的进展。这是**小批次的阶段**，其中Batch size的增加**线性加速了训练**。相比之下，当 $B >= B_{noise}$ 时，第一项占主导地位，因此增加B几乎对损失的进展没有影响。这是大batch的阶段，增加批次大小不会加速训练，**只会浪费计算资源**；两者之间的切换发生在 $B ≈ B_{noise}$ 处（参见图3）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于存在Hessian矩阵H，在方程2.8中计算噪声尺度需要一些额外的计算开销。我们可以通过在各种批次大小B下进行一系列线性搜索，测量 
 $∆L_{opt}(B)$ ，并将结果拟合到方程2.7中来估计它。这使我们能够估计 $B_{noise}$ ，并通过实证测试验证方程2.7是否真正适用于数据（我们在第3节中对这些局部测试进行了更详细的讨论）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 如果我们做一个（不现实的）假设，即优化问题具有完美的条件（即**Hessian矩阵是单位矩阵的倍数**），情况会变得更简单。如果是这种情况，那么方程2.8可以简化为：<br>

![formula9](images/large-batch-training-formula9.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个公式表明**噪声尺度等于各个梯度分量方差的总和，除以梯度的全局范数**。实质上，**它衡量了梯度与其方差相比的大小**。它也是估计梯度和真实梯度在L2空间中变得接近的尺度（具有非平凡的点积）的度量方式。预期的归一化L2距离由以下公式给出：<br>

![formula10](images/large-batch-training-formula10.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实际上，我们发现 $B_{simple}$ 和 $B_{noise}$ **通常只相差一个小的乘法常数因子**，尤其是当我们采用常见的训练方案改善条件时。在我们的实证工作中，有时我们会计算 $B_{noise}$ ，但主要计算 $B_{simple}$ ，因为它需要较少的计算开销。在附录A.1中，我们提供了一种极其简单的方法，在数据并行训练的情况下，几乎没有额外的开销来测量这个简化的噪声尺度(公式如下)。<br>

![formula-a-1-2](images/large-batch-training-formula-a-1-2.png)

## 2.3 数据/时间效率权衡的预测结果
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到目前为止，我们的分析仅涉及损失函数空间中的一个点。但在第3节中，我们将展示方程2.7非常准确地预测了**训练速度与批次大小的依赖关系**，即使是对于在损失函数空间中涵盖许多点的完整训练运行。通过对方程2.7在多个优化步骤中进行平均（请参见附录D），我们找到了**训练速度和数据效率**之间的简单关系：<br>

![formula11](images/large-batch-training-formula11.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这里，S和 $S_{min}$ 分别表示达到特定性能水平所需的实际步骤数和最小可能步骤数，E和 $E_{min}$ 分别表示达到相同性能水平所需的实际处理的训练样例数和最小可能训练样例数。由于我们使用固定的batch size进行训练，我们有 $E_{tot} = B*S_{tot}$ 。我们通过对上述方程进行经验拟合来定义临界批次大小，如下所示：<br>

![formula12](images/large-batch-training-formula12.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们的模型预测Bcrit ≈ Bnoise，其中Bnoise在训练中适当地进行平均处理（请参见附录D）。请注意，在训练过程中，噪声规模可能会有显著的变化，因此关键批量大小也取决于我们将模型训练到的性能水平。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在序列时间与总计算之间的结果权衡曲线呈现为图1中的双曲线形状。优化的目标是以最小的S和E达到给定的性能水平，但如图1所示，**存在权衡(trade off)**，因为非常小的S可能需要非常大的E，反之亦然。当我们选择 $B = B_{crit}$ 时，方程2.11的两边都是1，因此训练需要通过训练数据的次数是最优数据效率（小批量）运行的**两倍**，优化步骤是最优时间效率（大批量）运行的**两倍**。<br>



## 2.6 总结
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总结起来，我们的模型对于大批量训练提出了以下预测：<br>
- 神经网络训练的**速度和效率**之间的权衡由批处理大小控制，并遵循2.11式的形式。<br>
- 描述成本/时间权衡的关键Batch size $B_{crit}$ 可以通过测量梯度噪声尺度在数量级上进行预测，最简化形式为2.9式中的Bsimple。<br>
- 梯度噪声尺度在训练过程中可以有显著变化，这表明关键批处理大小还取决于所选择的模型性能水平。<br>
- 梯度噪声尺度通过训练的“温度”与学习率相关，但在调优良好的训练过程中保持一致（参见附录C）。<br>

