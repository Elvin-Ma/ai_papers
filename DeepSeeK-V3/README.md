# 0 摘要
- [论文地址](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf)
- [DeepSeek-V2](https://arxiv.org/pdf/2405.04434)
- [DeepSeekMOE](https://arxiv.org/pdf/2401.06066)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们推出了DeepSeek-V3，这是一个强大的混合专家（MoE）语言模型，总参数量达到6710亿，每个词汇激活370亿参数。为了实现高效推理和成本效益高的训练，DeepSeek-V3采用了多头潜在注意力（MLA）和DeepSeekMoE架构，这些架构在DeepSeek-V2中已得到了充分验证。此外，DeepSeek-V3开创性地采用了无辅助损失的负载均衡策略，并设定了多词汇预测训练目标，以实现更强的性能。我们在14.8万亿个多样且高质量的词汇上对DeepSeek-V3进行了预训练，随后进行了监督微调和强化学习阶段，以充分发挥其能力。综合评估显示，DeepSeek-V3的性能超越了其他开源模型，并达到了与领先的闭源模型相当的水平。尽管性能卓越，但DeepSeek-V3的完整训练仅需278.8万H800 GPU小时。此外，其训练过程非常稳定。在整个训练过程中，我们没有遇到任何不可恢复的损失尖峰，也没有进行任何回滚。模型检查点可在  https://github.com/deepseek-ai/DeepSeek-V3 上获取。<br>

![figure1](images/figure1.png)

# 1 introduce
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;近年来，大型语言模型（LLM）经历了快速迭代和演进（Anthropic, 2024；Google, 2024；OpenAI, 2024a），逐步缩小了与通用人工智能（AGI）之间的差距。除了闭源模型外，开源模型也在取得显著进展，包括DeepSeek系列（DeepSeek-AI, 2024a,b,c；Guo等, 2024）、LLaMA系列（AI@Meta, 2024a,b；Touvron等, 2023a,b）、Qwen系列（Qwen, 2023, 2024a,b）和Mistral系列（Jiang等, 2023；Mistral, 2024），它们都在努力追赶闭源模型的步伐。为了进一步拓展开源模型的能力边界，我们扩大了模型规模，并推出了DeepSeek-V3，这是一个拥有6710亿参数的大型混合专家（MoE）模型，其中每个标记会激活370亿参数。<br>

展望未来，我们始终致力于实现模型的高性能与低成本。因此，在架构方面，DeepSeek-V3继续采用多头潜在注意力（MLA）（DeepSeek-AI，2024c）以提高推理效率，并采用DeepSeekMoE（Dai等人，2024）以实现成本效益高的训练。这两种架构已在DeepSeek-V2（DeepSeek-AI，2024c）中得到验证，证明了它们在保持模型稳健性能的同时，能够实现高效的训练和推理。除了基础架构外，我们还实施了另外两种策略以进一步提升模型能力。首先，DeepSeek-V3开创性地采用了无辅助损失策略（Wang等人，2024a）进行负载均衡，旨在最小化因促进负载均衡而对模型性能产生的不利影响。其次，DeepSeek-V3采用了多标记预测训练目标，我们观察到这一策略能够提升模型在评估基准上的整体表现。<br>

为了实现高效训练，我们支持FP8混合精度训练，并对训练框架进行了全面优化。低精度训练已成为一种颇具前景的高效训练解决方案（Dettmers等人，2022；Kalamkar等人，2019；Narang等人，2017；Peng等人，2023b），其发展与硬件能力的进步密切相关（Luo等人，2024；Micikevicius等人，2022；Rouhani等人，2023a）。在本文中，我们引入了一种FP8混合精度训练框架，并**首次在超大规模模型上验证了其有效性**。通过支持FP8**计算和存储**，我们既加快了训练速度，又减少了GPU内存使用。在训练框架方面，我们设计了**DualPipe算法**以实现高效的流水线并行，该算法减少了流水线气泡，并通过**计算与通信的重叠**隐藏了训练过程中的大部分通信开销。这种重叠确保了在模型进一步扩展(fine-grained)时，只要我们保持恒定的计算与通信比率，仍然可以**在节点间使用细粒度专家**，同时实现近乎零的alltoall通信开销。此外，我们还开发了高效的跨节点alltoall通信内核，以充分利用InfiniBand（IB）和NVLink带宽。此外，我们还对**内存占用**进行了精心优化，使得**训练DeepSeek-V3时无需使用成本高昂的张量并行**。通过这些努力，我们实现了高效的训练。<br>


在预训练阶段，我们在14.8T高质量且多样化的token上训练了DeepSeek-V3。预训练过程非常稳定，在整个训练过程中，我们没有遇到任何无法恢复的损失尖峰，也没有需要进行回滚的情况。接下来，我们对DeepSeek-V3进行了两阶段的上下文长度扩展。在第一阶段，最大上下文长度被扩展到32K，而在第二阶段，则进一步扩展到128K。之后，我们进行了后训练，包括对DeepSeek-V3基础模型进行监督微调（SFT）和强化学习（RL），以使其与人类偏好保持一致，并进一步释放其潜力。在后训练阶段，我们从DeepSeekR1系列模型中提炼推理能力，同时谨慎地保持模型准确性与生成长度之间的平衡。<br>

![table1](images/table1.png)


