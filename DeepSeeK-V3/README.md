# 0 摘要
- [论文地址](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf)
- [DeepSeek-V2](https://arxiv.org/pdf/2405.04434)
- [DeepSeekMOE](https://arxiv.org/pdf/2401.06066)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们推出了DeepSeek-V3，这是一个强大的混合专家（MoE）语言模型，总参数量达到6710亿，每个词汇激活370亿参数。为了实现高效推理和成本效益高的训练，DeepSeek-V3采用了多头潜在注意力（MLA）和DeepSeekMoE架构，这些架构在DeepSeek-V2中已得到了充分验证。此外，DeepSeek-V3开创性地采用了无辅助损失的负载均衡策略，并设定了多词汇预测训练目标，以实现更强的性能。我们在14.8万亿个多样且高质量的词汇上对DeepSeek-V3进行了预训练，随后进行了监督微调和强化学习阶段，以充分发挥其能力。综合评估显示，DeepSeek-V3的性能超越了其他开源模型，并达到了与领先的闭源模型相当的水平。尽管性能卓越，但DeepSeek-V3的完整训练仅需278.8万H800 GPU小时。此外，其训练过程非常稳定。在整个训练过程中，我们没有遇到任何不可恢复的损失尖峰，也没有进行任何回滚。模型检查点可在  https://github.com/deepseek-ai/DeepSeek-V3 上获取。<br>

![figure1](images/figure1.png)

# 1 introduce
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;近年来，大型语言模型（LLM）经历了快速迭代和演进（Anthropic, 2024；Google, 2024；OpenAI, 2024a），逐步缩小了与通用人工智能（AGI）之间的差距。除了闭源模型外，开源模型也在取得显著进展，包括DeepSeek系列（DeepSeek-AI, 2024a,b,c；Guo等, 2024）、LLaMA系列（AI@Meta, 2024a,b；Touvron等, 2023a,b）、Qwen系列（Qwen, 2023, 2024a,b）和Mistral系列（Jiang等, 2023；Mistral, 2024），它们都在努力追赶闭源模型的步伐。为了进一步拓展开源模型的能力边界，我们扩大了模型规模，并推出了DeepSeek-V3，这是一个拥有6710亿参数的大型混合专家（MoE）模型，其中每个标记会激活370亿参数。<br>

展望未来，我们始终致力于实现模型的高性能与低成本。因此，在架构方面，DeepSeek-V3继续采用多头潜在注意力（MLA）（DeepSeek-AI，2024c）以提高推理效率，并采用DeepSeekMoE（Dai等人，2024）以实现成本效益高的训练。这两种架构已在DeepSeek-V2（DeepSeek-AI，2024c）中得到验证，证明了它们在保持模型稳健性能的同时，能够实现高效的训练和推理。除了基础架构外，我们还实施了另外两种策略以进一步提升模型能力。首先，DeepSeek-V3开创性地采用了无辅助损失策略（Wang等人，2024a）进行负载均衡，旨在最小化因促进负载均衡而对模型性能产生的不利影响。其次，DeepSeek-V3采用了多标记预测训练目标，我们观察到这一策略能够提升模型在评估基准上的整体表现。<br>

为了实现高效训练，我们支持FP8混合精度训练，并对训练框架进行了全面优化。低精度训练已成为一种颇具前景的高效训练解决方案（Dettmers等人，2022；Kalamkar等人，2019；Narang等人，2017；Peng等人，2023b），其发展与硬件能力的进步密切相关（Luo等人，2024；Micikevicius等人，2022；Rouhani等人，2023a）。在本文中，我们引入了一种FP8混合精度训练框架，并**首次在超大规模模型上验证了其有效性**。通过支持FP8**计算和存储**，我们既加快了训练速度，又减少了GPU内存使用。在训练框架方面，我们设计了**DualPipe算法**以实现高效的流水线并行，该算法减少了流水线气泡，并通过**计算与通信的重叠**隐藏了训练过程中的大部分通信开销。这种重叠确保了在模型进一步扩展(fine-grained)时，只要我们保持恒定的计算与通信比率，仍然可以**在节点间使用细粒度专家**，同时实现近乎零的alltoall通信开销。此外，我们还开发了高效的跨节点alltoall通信内核，以充分利用InfiniBand（IB）和NVLink带宽。此外，我们还对**内存占用**进行了精心优化，使得**训练DeepSeek-V3时无需使用成本高昂的张量并行**。通过这些努力，我们实现了高效的训练。<br>


在预训练阶段，我们在14.8T高质量且多样化的token上训练了DeepSeek-V3。预训练过程非常稳定，在整个训练过程中，我们没有遇到任何无法恢复的损失尖峰，也没有需要进行回滚的情况。接下来，我们对DeepSeek-V3进行了两阶段的上下文长度扩展。在第一阶段，最大上下文长度被扩展到32K，而在第二阶段，则进一步扩展到128K。之后，我们进行了后训练，包括对DeepSeek-V3基础模型进行监督微调（SFT）和强化学习（RL），以使其与人类偏好保持一致，并进一步释放其潜力。在后训练阶段，我们从DeepSeekR1系列模型中提炼推理能力，同时谨慎地保持模型准确性与生成长度之间的平衡。<br>


我们对DeepSeek-V3进行了全面的基准测试评估。尽管其训练成本较为经济，但综合评估结果显示，DeepSeek-V3-Base已成为目前可用的最强大的开源基础模型，尤其在代码和数学方面表现突出。其聊天版本也优于其他开源模型，并在一系列标准和开放式基准测试中取得了与领先闭源模型（包括GPT-4o和Claude-3.5-Sonnet）相媲美的性能。

最后，我们再次强调DeepSeek-V3的经济训练成本，具体数据见表1。这一成本是通过我们优化的算法、框架和硬件协同设计实现的。在预训练阶段，训练DeepSeek-V3处理每万亿个token仅需180K H800 GPU小时，即在我们拥有2048块H800 GPU的集群上需要3.7天。因此，我们的预训练阶段在不到两个月内完成，总共消耗了2664K GPU小时。加上上下文长度扩展的119K GPU小时和后训练的5K GPU小时，DeepSeek-V3的完整训练过程仅消耗了2.788M GPU小时。假设H800 GPU的租赁价格为每小时2美元，我们的总训练成本仅为557.6万美元。需要注意的是，上述成本仅包括DeepSeek-V3的正式训练费用，并不包含先前在架构、算法或数据方面进行的研究和消融实验的成本。

![table1](images/table1.png)

我们的主要贡献包括：<br>

**Architecture架构：创新的负载均衡策略和训练目标**<br>
- 在DeepSeek-V2的高效架构基础上，我们开创了一种**无辅助损失的负载均衡策略**，最大限度地减少了因鼓励负载均衡而导致的性能下降(增加损失函数会导致precision降低)。<br>
- 我们研究了**多令牌预测（MTP）目标**，并证明其对模型性能有益。它还可以用于**推测性解码，以加速推理过程**。<br>

**预训练：追求极致的训练效率** <br>

- 我们设计了一个FP8混合精度训练框架，并首次在超大规模模型上验证了FP8训练的可行性和有效性。

- 通过算法、框架和硬件的协同设计，我们克服了跨节点MoE（混合专家）训练中的通信瓶颈，实现了**计算与通信的近乎完全重叠**。这显著提高了我们的训练效率，降低了训练成本，使我们能够在不增加额外开销的情况下进一步扩展模型规模。

- 我们仅以2.664M H800 GPU小时的经济成本，完成了DeepSeek-V3在14.8T token上的预训练，打造出目前最强的开源基础模型。预训练后的后续训练阶段仅需0.1M GPU小时。

**后训练：从DeepSeek-R1进行知识蒸馏**

- 我们引入了一种创新方法，将长链思维（Chain-of-Thought, CoT）模型，特别是DeepSeek R1系列模型之一的推理能力，蒸馏到标准大型语言模型（LLM）中，尤其是DeepSeek-V3。我们的流程巧妙地将R1的验证和反思模式融入DeepSeek-V3，并显著提高了其推理性能。同时，我们还保持了DeepSeek-V3输出风格和长度的可控性。<br>

**核心评估结果总结** <br>

- 知识：（1）在教育类基准测试，如MMLU、MMLU-Pro和GPQA上，DeepSeek-V3的表现超越了所有其他开源模型，在MMLU上得分88.5，在MMLU-Pro上得分75.9，在GPQA上得分59.1。其性能与领先的闭源模型，如GPT-4o和Claude-Sonnet-3.5相当，缩小了开源模型与闭源模型在该领域的差距。（2）在事实性基准测试中，DeepSeek-V3在SimpleQA和中文SimpleQA上均表现出优于其他开源模型的性能。虽然在英文事实性知识（SimpleQA）方面略逊于GPT-4o和Claude-Sonnet-3.5，但在中文事实性知识（中文SimpleQA）方面则超越了这些模型，凸显了其在中文事实性知识方面的优势。

- 代码、数学和推理：（1）在所有非长链思维（non-long-CoT）的开源和闭源模型中，DeepSeek-V3在数学相关基准测试上取得了最优性能。值得注意的是，它在特定基准测试，如MATH-500上，甚至超越了o1-preview，展示了其强大的数学推理能力。（2）在编码相关任务上，DeepSeek-V3在编码竞赛基准测试，如LiveCodeBench中脱颖而出，巩固了其在该领域的领先地位。对于工程相关任务，虽然DeepSeek-V3的表现略低于Claude-Sonnet-3.5，但仍以显著优势超越了所有其他模型，展示了其在各种技术基准测试中的竞争力。<br>

在本文的余下部分，我们首先详细介绍我们的DeepSeek-V3模型架构（第2节）。随后，我们介绍我们的基础设施，包括计算集群、训练框架、对FP8训练的支持、推理部署策略，以及我们对未来硬件设计的建议。接下来，我们描述我们的预训练过程，包括训练数据的构建、超参数设置、长上下文扩展技术、相关评估以及一些讨论（第4节）。之后，我们讨论我们在后训练方面所做的努力，包括监督式微调（SFT）、强化学习（RL）、相应的评估及讨论（第5节）。最后，我们总结本工作，讨论DeepSeek-V3存在的局限性，并提出未来研究的潜在方向（第6节）。<br>

