# Squence to Sequence Learning with Neural Networks

# 0 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度神经网络（Deep Neural Networks，DNNs）是一种强大的模型，能够在复杂的学习任务上取得出色的性能。尽管在有大量标记的训练集可用时，DNNs表现良好，但它们不能用于将序列映射到序列。在本文中，我们提出了一种通用的端到端序列学习方法，它对序列结构的假设最小。我们的方法使用多层长短期记忆网络（Long Short-Term Memory，LSTM）将输入序列映射到一个固定维度的向量，然后使用另一个深层LSTM从向量中解码目标序列。我们的主要结果是，在WMT'14数据集上的英法翻译任务中，LSTM产生的翻译在整个测试集上的BLEU得分为34.8，其中LSTM的BLEU得分在词汇外的词上受到惩罚。此外，LSTM在长句子上也没有困难。相比之下，基于短语的统计机器翻译（phrase-based SMT）系统在同一数据集上的BLEU得分为33.3。当我们使用LSTM对上述SMT系统生成的1000个翻译候选进行重新排序时，其BLEU得分提高到36.5，接近此任务的先前最佳结果。LSTM还学到了合理的短语和句子表示，对词序敏感，并且相对不受主动语态和被动语态的影响。最后，我们发现将所有源语句的单词顺序颠倒（但不包括目标语句）显著提高了LSTM的性能，因为这样做引入了源语句和目标语句之间许多短期依赖关系，从而使优化问题更容易解决。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度神经网络（Deep Neural Networks，DNNs）是非常强大的机器学习模型，能够在诸如语音识别[13, 7]和视觉对象识别[19, 6, 21, 20]等困难问题上取得出色的性能。DNNs之所以强大，是因为它们可以进行任意的并行计算，且所需步骤数量适中。DNNs的强大之处在于，它们能够使用仅包含2个隐藏层的二次规模来对N个N位数字进行排序[27]，这是其能力的一个令人惊讶的例子。因此，虽然神经网络与传统的统计模型相关，但它们学习了一种复杂的计算过程。此外，只要标记的训练集提供了足够的信息来指定网络的参数，就可以使用监督反向传播来训练大规模的DNNs。因此，如果存在一个大规模DNN的参数设置可以取得良好结果（例如，因为人类可以迅速解决该任务），监督反向传播将找到这些参数并解决问题。<br>

尽管深度神经网络（DNNs）具有灵活性和强大性，但它们只能应用于能够通过固定维度的向量合理编码输入和目标的问题。这是一个重要的限制，因为许多重要的问题最好用**长度未知**的序列来表达。例如，语音识别和机器翻译都是顺序问题。同样，问答问题也可以看作是将表示问题的单词序列映射到表示答案的单词序列。因此，清楚的是，学习将sequences to sequences的领域无关方法将会很有用。<br>

对于DNNs来说，序列构成了一个挑战，因为DNNs要求输入和输出的维度是已知且固定的。在本文中，我们展示了长短期记忆（Long Short-Term Memory，LSTM）架构[16]的直接应用可以解决一般的序列到序列问题。这个想法是使用一个LSTM逐个时间步骤地读取输入序列，以获取一个大的固定维度的向量表示，然后使用另一个LSTM从该向量中提取输出序列（图1）。第二个LSTM本质上是一个基于循环神经网络的语言模型[28, 23, 30]，只是它是以输入序列为条件的。由于输入和相应输出之间存在相当大的时间延迟（图1），LSTM成功学习长期时间依赖关系的能力使其成为这种应用的自然选择。<br>

已经有许多相关的尝试利用神经网络解决一般的序列到序列学习问题。我们的方法与Kalchbrenner和Blunsom [18]密切相关，他们是第一个将整个输入句子映射到向量的人，并且与Cho等人 [5]相关，尽管后者仅用于重新评分由基于短语的系统产生的假设。Graves [10]引入了一种新颖的可微分注意机制，使神经网络能够关注其输入的不同部分，Bahdanau等人 [2]成功地将这个思想的一个优雅的变体应用于机器翻译。连接主义序列分类（Connectionist Sequence Classification）是另一种将序列映射到序列的流行技术，但它假设输入和输出之间存在单调的对齐关系[11]。<br>

![figure1](images/seq2seq-figure.jpg)
*(图1：我们的模型读取一个输入句子“ABC”，并生成一个输出句子“WXYZ”。模型在输出句子的结束标记后停止进行预测。请注意，LSTM是以相反的顺序读取输入句子的，因为这样做会在数据中引入许多短期依赖关系，从而大大简化优化问题。)* <br>

该工作的主要结果如下所示。在WMT'14英法翻译任务中，我们通过直接从一个由5个深度LSTM（每个LSTM含有384M参数和8,000维状态）组成的集合中提取翻译，并使用简单的从左到右的束搜索解码器(beamsearch decoder)，获得了34.81的BLEU分数。这是目前使用大型神经网络进行直接翻译取得的最好结果。与之相比，该数据集上基于短语的统计机器翻译（SMT）基准的BLEU分数为33.30 [29]。34.81的BLEU分数是由一个词汇量为80k的LSTM实现的，因此当参考翻译中包含这80k词汇之外的词汇时，分数会受到惩罚。这个结果表明，一个相对未经优化的小词汇量神经网络架构，在改进空间还很大的情况下，优于基于短语的SMT系统。<br>

最后，我们使用LSTM对同一任务的公开可用的SMT(Statistical Machine Translation)基准模型的1000个最佳翻译列表进行重新评分[29]。通过这样做，我们获得了36.5的BLEU分数，比基准模型提高了3.2个BLEU点，接近此任务的先前最佳发布结果（37.0 [9]）。<br>
令人惊讶的是，尽管其他研究人员在相关架构上的最新经验表明LSTM在处理非常长的句子时会出现问题[26]，但我们的LSTM在处理长句子时并没有遇到困难。我们之所以能够处理长句子，是因为我们在训练和测试集中**反转了源语句的单词顺序**，而没有反转目标语句的单词顺序。通过这样做，我们引入了许多短期依赖关系，使得优化问题变得更简单（参见第2节和第3.3节）。因此，随机梯度下降（SGD）可以学习到在处理长句子时没有困难的LSTM。在源语句中反转单词顺序的简单技巧是本工作的关键技术贡献之一。<br>

LSTM的一个有用特性是它学会将长度可变的输入句子映射为一个固定维度的向量表示。考虑到翻译通常是源语言句子的释义，翻译目标鼓励LSTM找到能够捕捉其含义的句子表示，因为具有相似含义的句子彼此接近，而不同含义的句子则会相距较远。定性评估支持这一观点，显示我们的模型能够理解词序并且对主动和被动语态具有一定的不变性。<br>

# 2 模型
循环神经网络（Recurrent Neural Network，RNN）[31, 28]是前馈神经网络在处理序列数据时的一种**自然推广**。给定一个输入序列 $(x_{1}, \dots, x_{T})$, 标准的RNN通过迭代以下方程来计算一个输出序列 $(x_{1}, \dots, x_{T})$ :

$$h_{t} = sigm(W^{hx}x_{t} + W^{hh}h_{t-1})$$

$$y_{t} = W^{yh}h_{t}$$

RNN在已知输入与输出之间的对齐关系的情况下，能够轻松执行sequences to sequence。然而，对于输入和输出序列**长度不同**且存在**复杂且非单调关系**的问题，如何应用RNN并不明确。
在一般的序列学习中，最简单的策略是使用一个RNN将输入序列映射到一个**固定**大小的向量，然后再使用另一个RNN将向量映射到目标序列（这种方法也被Cho等人采用）[5]。尽管从原理上讲这可能有效，因为RNN可以获得所有相关的信息，但由于产生了**长期依赖关系**（图1）[14, 4, 16, 15]，训练RNN会变得困难。然而，长短期记忆（Long Short-Term Memory，LSTM）[16]已被证明可以学习具有长期依赖关系的问题，因此在这种情况下，LSTM可能会取得成功。<br>

LSTM的目标是估计条件概率 $p(y_{1},\dots, y_{T'} | x_{1},\dots, x_{T})$ ，其中 $(x_{1}, \dots, x_{T})$ 是输入序列, $y_{1}, \dots, y_{T'}$ 是其对应的输出序列，其长度T'可能与T不同。LSTM通过首先获取输入序列 $(x_{1},\dots, x_{T})$ 的**固定维度表示v**来计算这个条件概率，该表示v是LSTM的最后一个隐藏状态，然后使用标准的LSTM-LM公式来计算 $y1, \dots, y_{T'}$ 的概率，其中初始隐藏状态设定为 $x_{1}, \dots, x_{T}$ 的表示v。<br>

![formula1](images/seq2seq-formula1.jpg)

在这个方程中，每个 $p(y_{t} | v, y_{1},\dots, y_{t−1})$ 的分布用词汇表中所有单词的softmax表示。我们使用了来自Graves [10]的LSTM公式。需要注意的是，我们要求每个句子以特殊的句子结束符"<EOS>"结束，这使得模型能够定义一种对所有**可能长度**的序列的分布。整体方案如图1所示，其中显示的LSTM计算了"A"、"B"、"C"、"<EOS>"的表示，并使用这个表示来计算"W"、"X"、"Y"、"Z"、"<EOS>"的概率。<br>

![figure1](images/seq2seq-figure1.jpg)

我们实际的模型与上述描述有三个重要的区别。首先，我们使用了两个不同的LSTM：一个用于输入序列，另一个用于输出序列，因为这样做可以增加模型参数的数量，而计算成本几乎可以忽略不计，并且可以自然地同时训练多个语言对的LSTM [18]。其次，我们发现深层的LSTM比浅层的LSTM表现更好，因此我们选择了一个有四层的LSTM。第三，我们发现将输入句子的**单词顺序反转**非常有价值。例如，不是将句子a、b、c映射为句子α、β、γ，而是要求LSTM将c、b、a映射为α、β、γ，其中α、β、γ是a、b、c的翻译。这样，a就与α非常接近，b与β相对接近，依此类推，这个事实使得SGD能够在输入和输出之间建立“通信”。我们发现这种简单的数据转换极大地提高了LSTM的性能。<br>

# 3 实验
我们以两种方式将我们的方法应用于WMT'14英法机器翻译任务。我们使用它直接翻译输入句子，而不使用参考的统计机器翻译系统，并且我们使用它对统计机器翻译 baseline 的n-best列表进行重新评分。我们报告了这些翻译方法的准确性，展示了样本翻译，并可视化了生成的句子表示。<br>

## 3.1 数据集详细信息
我们使用了WMT'14英法数据集。我们在一个由348M个法语单词和304M个英语单词组成的12M个句子子集上训练我们的模型，这是[29]中的一个干净的“选定”子集。我们选择了这个翻译任务和这个特定的训练集子集，是因为 baseline SMT [29]提供了一个经过分词处理的训练和测试集，以及1000个最佳翻译列表。<br>

由于典型的神经语言模型依赖于每个单词的向量表示，我们为两种语言使用了**一个固定的词汇表**。我们对源语言使用了最常见的160,000个单词，对目标语言使用了最常见的80,000个单词。每个词汇表外的单词都被替换为一个特殊的“UNK”标记。<br>

## 3.2 解码和重新评分
我们实验的核心是在许多**句子对**上训练一个大型深层LSTM。我们通过最大化给定源语句S的正确翻译T的**对数概率(log probability)** 来训练它，因此训练目标是：

$$1 /|\mathcal{S}| \sum_{(T, S) \in \mathcal{S}} \log p(T \mid S)$$

其中S是训练集。一旦训练完成，我们通过根据LSTM找到最有可能的翻译结果来生成翻译：

$$\hat{T}=\arg \max _{T} p(T \mid S)$$

我们使用一个简单的从左到右的束搜索解码器(Beam search decoder)来寻找最有可能的翻译结果，它维护了一个小的假设集合B，其中每个假设是某个翻译的前缀。在每个时间步，我们将束中的每个假设扩展为词汇表中的每个可能单词。这大大增加了假设的数量，因此我们根据模型的对数概率丢弃除了B个最可能的假设之外的所有假设。一旦一个假设附加了"<EOS>"符号，它就会从束中移除，并添加到完成假设的集合中。尽管这个解码器是近似的，但它很容易实现。有趣的是，即使使用束大小(beam size)为1的情况下，我们的系统也表现良好，而束大小为2的情况下可以获得束搜索(beam search)的大部分好处（表1）。<br>

我们还使用LSTM对基线系统[29]生成的1000个最佳翻译列表进行重新评分。为了重新评分一个n-best列表，我们使用我们的LSTM计算每个假设的对数概率，并将其得分和LSTM的得分进行平均。<br>

## 3.3 反转源语句(Reversing the Source Sentences)
虽然LSTM能够解决具有长期依赖的问题，但我们发现当源语句反转时（目标语句不反转），LSTM的学习效果大大提高。通过反转源语句，LSTM的测试困惑度从5.8降低到4.7，解码翻译的测试BLEU分数从25.9提高到30.6。<br>

虽然我们对这一现象没有完全的解释，但我们认为这是由于引入了许多**短期依赖关系**到数据集中。通常，当我们将源语句和目标语句连接在一起时，源语句中的每个单词与其在目标语句中对应的单词**相距较远**。因此，该问题具有较大的“最小时间滞后”[17]。通过反转源语句中的单词，源语言和目标语言中相应单词之间的**平均距离**保持不变。然而，源语言中的**前几个单词**现在与目标语言中的**前几个单词**非常接近，因此问题的最小时间滞后大大减小。因此，反向传播**更容易**在源语句和目标语句之间“**建立通信**”，从而显著提高了整体性能。<br>

最初，我们认为反转输入句子只会导致对目标句子的前部分做出更自信的预测，并对后部分的预测不够自信。然而，训练时使用反转源语句的LSTM在长句子上的表现要比训练时使用原始源语句的LSTM好得多（见第3.7节），这表明反转输入句子会导致LSTM更好地利用记忆。<br>

# 3.4 训练细节
我们发现LSTM模型的训练相当容易。我们使用了具有4层的深层LSTM，每层有1000个单元和1000维的词嵌入，输入词汇表包含160,000个词，输出词汇表包含80,000个词。因此，深层LSTM使用8000个实数来表示一个句子。我们发现深层LSTM明显优于浅层LSTM，每增加一层可以将困惑度降低近10％，可能是由于其更大的隐藏状态。我们在每个输出处使用了一个包含80,000个单词的朴素softmax。结果的LSTM模型具有384M个参数，其中64M个是纯循环连接（32M用于“编码器”LSTM，32M用于“解码器”LSTM）。完整的训练细节如下：<br>
- 我们使用均匀分布在-0.08到0.08之间初始化了LSTM的所有参数。
- 我们使用没有动量的随机梯度下降，学习率固定为**0.7**。在经过5个epoch后，我们开始每半个epoch将学习率减半。我们总共训练了**7.5个**epoch。
- 我们使用128个序列作为梯度的批次，并将其除以批次的大小（即128）。
- 尽管LSTM倾向于不受梯度消失问题的影响，但它们**可能会出现梯度爆炸**。因此，我们对**梯度的范数**强制施加了硬约束[10, 25]，当梯度的范数超过阈值时**进行缩放**。对于每个训练批次，我们计算s = kgk2，其中g是梯度除以128。如果s > 5，则将g设置为5g/s。
- 不同的句子长度不同。大多数句子较短（例如，长度为20-30），但有些句子很长（例如，长度> 100），因此随机选择的128个训练句子的小批量将包含许多短句子和少数长句子，结果，小批量中的大部分计算都是浪费的。为了解决这个问题，我们确保小批量中的所有句子长度大致相同，从而加快了2倍的速度。

## 3.5 并行化
在单个GPU上使用前面部分中的配置实现的深层LSTM的C++版本的处理速度约为每秒1,700个单词。这对于我们的目的来说速度太慢了，因此我们使用了一台配备了8个GPU的机器对我们的模型进行了并行化处理。LSTM的每一层都在不同的GPU上执行，并在计算完成后立即将其激活值传递给下一个GPU/层。我们的模型有4层LSTM，每层都位于一个单独的GPU上。其余的4个GPU用于并行化softmax，因此每个GPU负责与一个1000×20000的矩阵相乘。使用128的小批量大小，该实现的处理速度达到了每秒6,300个（英文和法文）单词。使用这种实现的训练大约需要10天的时间。<br>

## 3.5 试验结果
实验结果表明，研究团队使用了大小写敏感的BLEU分数[24]来评估翻译的质量。他们使用了multi-bleu.pl1工具对分词后的预测结果和真实结果进行了BLEU分数的计算。这种评估方式与之前的研究[5]和[2]一致，并且复现了[29]中的33.3分数。然而，如果按照这种方式评估最佳的WMT'14系统[9]（其预测结果可以从statmt.org\matrix下载），得到的分数是37.0，**高于** statmt.org\matrix报告的35.8分。<br>

![figure1](images/seq2seq-table1.jpg)

实验结果在表1和表2中进行了呈现。研究团队通过使用不同的随机初始化和随机小批量顺序的LSTM集成获得了最佳结果。尽管LSTM集成的解码翻译结果并没有超过最佳的WMT'14系统，但这是**第一次纯神经翻译系统在大规模机器翻译任务上**以相当大的优势击败了基于短语的统计机器翻译baseline，尽管它无法处理词汇表外的词汇。如果将LSTM用于对基线系统的1000个最佳翻译列表进行重新评分，其BLEU分数与最佳的WMT'14结果相差不到0.5个BLEU点。<br>

![figure2](images/seq2seq-table2.jpg)


# 原文链接
[原文链接](https://arxiv.org/abs/1409.3215)



