# Squence to Sequence Learning with Neural Networks

# 0 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度神经网络（Deep Neural Networks，DNNs）是一种强大的模型，能够在复杂的学习任务上取得出色的性能。尽管在有大量标记的训练集可用时，DNNs表现良好，但它们不能用于将序列映射到序列。在本文中，我们提出了一种通用的端到端序列学习方法，它对序列结构的假设最小。我们的方法使用多层长短期记忆网络（Long Short-Term Memory，LSTM）将输入序列映射到一个固定维度的向量，然后使用另一个深层LSTM从向量中解码目标序列。我们的主要结果是，在WMT'14数据集上的英法翻译任务中，LSTM产生的翻译在整个测试集上的BLEU得分为34.8，其中LSTM的BLEU得分在词汇外的词上受到惩罚。此外，LSTM在长句子上也没有困难。相比之下，基于短语的统计机器翻译（phrase-based SMT）系统在同一数据集上的BLEU得分为33.3。当我们使用LSTM对上述SMT系统生成的1000个翻译候选进行重新排序时，其BLEU得分提高到36.5，接近此任务的先前最佳结果。LSTM还学到了合理的短语和句子表示，对词序敏感，并且相对不受主动语态和被动语态的影响。最后，我们发现将所有源语句的单词顺序颠倒（但不包括目标语句）显著提高了LSTM的性能，因为这样做引入了源语句和目标语句之间许多短期依赖关系，从而使优化问题更容易解决。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度神经网络（Deep Neural Networks，DNNs）是非常强大的机器学习模型，能够在诸如语音识别[13, 7]和视觉对象识别[19, 6, 21, 20]等困难问题上取得出色的性能。DNNs之所以强大，是因为它们可以进行任意的并行计算，且所需步骤数量适中。DNNs的强大之处在于，它们能够使用仅包含2个隐藏层的二次规模来对N个N位数字进行排序[27]，这是其能力的一个令人惊讶的例子。因此，虽然神经网络与传统的统计模型相关，但它们学习了一种复杂的计算过程。此外，只要标记的训练集提供了足够的信息来指定网络的参数，就可以使用监督反向传播来训练大规模的DNNs。因此，如果存在一个大规模DNN的参数设置可以取得良好结果（例如，因为人类可以迅速解决该任务），监督反向传播将找到这些参数并解决问题。<br>

尽管深度神经网络（DNNs）具有灵活性和强大性，但它们只能应用于能够通过固定维度的向量合理编码输入和目标的问题。这是一个重要的限制，因为许多重要的问题最好用**长度未知**的序列来表达。例如，语音识别和机器翻译都是顺序问题。同样，问答问题也可以看作是将表示问题的单词序列映射到表示答案的单词序列。因此，清楚的是，学习将sequences to sequences的领域无关方法将会很有用。<br>

对于DNNs来说，序列构成了一个挑战，因为DNNs要求输入和输出的维度是已知且固定的。在本文中，我们展示了长短期记忆（Long Short-Term Memory，LSTM）架构[16]的直接应用可以解决一般的序列到序列问题。这个想法是使用一个LSTM逐个时间步骤地读取输入序列，以获取一个大的固定维度的向量表示，然后使用另一个LSTM从该向量中提取输出序列（图1）。第二个LSTM本质上是一个基于循环神经网络的语言模型[28, 23, 30]，只是它是以输入序列为条件的。由于输入和相应输出之间存在相当大的时间延迟（图1），LSTM成功学习长期时间依赖关系的能力使其成为这种应用的自然选择。<br>

已经有许多相关的尝试利用神经网络解决一般的序列到序列学习问题。我们的方法与Kalchbrenner和Blunsom [18]密切相关，他们是第一个将整个输入句子映射到向量的人，并且与Cho等人 [5]相关，尽管后者仅用于重新评分由基于短语的系统产生的假设。Graves [10]引入了一种新颖的可微分注意机制，使神经网络能够关注其输入的不同部分，Bahdanau等人 [2]成功地将这个思想的一个优雅的变体应用于机器翻译。连接主义序列分类（Connectionist Sequence Classification）是另一种将序列映射到序列的流行技术，但它假设输入和输出之间存在单调的对齐关系[11]。<br>

![figure1](images/seq2seq-figure.jpg)
*(图1：我们的模型读取一个输入句子“ABC”，并生成一个输出句子“WXYZ”。模型在输出句子的结束标记后停止进行预测。请注意，LSTM是以相反的顺序读取输入句子的，因为这样做会在数据中引入许多短期依赖关系，从而大大简化优化问题。)* <br>

该工作的主要结果如下所示。在WMT'14英法翻译任务中，我们通过直接从一个由5个深度LSTM（每个LSTM含有384M参数和8,000维状态）组成的集合中提取翻译，并使用简单的从左到右的束搜索解码器(beamsearch decoder)，获得了34.81的BLEU分数。这是目前使用大型神经网络进行直接翻译取得的最好结果。与之相比，该数据集上基于短语的统计机器翻译（SMT）基准的BLEU分数为33.30 [29]。34.81的BLEU分数是由一个词汇量为80k的LSTM实现的，因此当参考翻译中包含这80k词汇之外的词汇时，分数会受到惩罚。这个结果表明，一个相对未经优化的小词汇量神经网络架构，在改进空间还很大的情况下，优于基于短语的SMT系统。<br>

最后，我们使用LSTM对同一任务的公开可用的SMT(Statistical Machine Translation)基准模型的1000个最佳翻译列表进行重新评分[29]。通过这样做，我们获得了36.5的BLEU分数，比基准模型提高了3.2个BLEU点，接近此任务的先前最佳发布结果（37.0 [9]）。<br>
令人惊讶的是，尽管其他研究人员在相关架构上的最新经验表明LSTM在处理非常长的句子时会出现问题[26]，但我们的LSTM在处理长句子时并没有遇到困难。我们之所以能够处理长句子，是因为我们在训练和测试集中**反转了源语句的单词顺序**，而没有反转目标语句的单词顺序。通过这样做，我们引入了许多短期依赖关系，使得优化问题变得更简单（参见第2节和第3.3节）。因此，随机梯度下降（SGD）可以学习到在处理长句子时没有困难的LSTM。在源语句中反转单词顺序的简单技巧是本工作的关键技术贡献之一。<br>

LSTM的一个有用特性是它学会将长度可变的输入句子映射为一个固定维度的向量表示。考虑到翻译通常是源语言句子的释义，翻译目标鼓励LSTM找到能够捕捉其含义的句子表示，因为具有相似含义的句子彼此接近，而不同含义的句子则会相距较远。定性评估支持这一观点，显示我们的模型能够理解词序并且对主动和被动语态具有一定的不变性。<br>

# 2 模型
循环神经网络（Recurrent Neural Network，RNN）[31, 28]是前馈神经网络在处理序列数据时的一种**自然推广**。给定一个输入序列 $(x_{1}, \dots, x_{T})$, 标准的RNN通过迭代以下方程来计算一个输出序列 $(x_{1}, \dots, x_{T})$ :

$$h_{t} = sigm(W^{hx}x_{t} + W^{hh}h_{t-1})$$

$$y_{t} = W^{yh}h_{t}$$

RNN在已知输入与输出之间的对齐关系的情况下，能够轻松执行sequences to sequence。然而，对于输入和输出序列**长度不同**且存在**复杂且非单调关系**的问题，如何应用RNN并不明确。
在一般的序列学习中，最简单的策略是使用一个RNN将输入序列映射到一个**固定**大小的向量，然后再使用另一个RNN将向量映射到目标序列（这种方法也被Cho等人采用）[5]。尽管从原理上讲这可能有效，因为RNN可以获得所有相关的信息，但由于产生了**长期依赖关系**（图1）[14, 4, 16, 15]，训练RNN会变得困难。然而，长短期记忆（Long Short-Term Memory，LSTM）[16]已被证明可以学习具有长期依赖关系的问题，因此在这种情况下，LSTM可能会取得成功。<br>

LSTM的目标是估计条件概率 $p(y_{1},\dots, y_{T'} | x_{1},\dots, x_{T})$ ，其中 $(x1, . . . , xT)$ 是输入序列，$y1, \dots, y_{T'}$ 是其对应的输出序列，其长度T'可能与T不同。LSTM通过首先获取输入序列 $(x_{1},\dots, x_{T})$ 的固定维度表示v来计算这个条件概率，该表示v是LSTM的最后一个隐藏状态，然后使用标准的LSTM-LM公式来计算 $y1, \dots, y_{T'}$ 的概率，其中初始隐藏状态设定为 $x_{1}, . . . , x_{T}$ 的表示v。<br>

![formula1](images/seq2seq-formula1.jpg)


# 原文链接
[原文链接](https://arxiv.org/abs/1409.3215)



