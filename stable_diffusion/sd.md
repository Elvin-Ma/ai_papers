# High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models(stable-diffusion) 高分辨率图像合成与潜在扩散模型
- [论文链接](https://arxiv.org/pdf/2112.10752.pdf)

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过将图像形成过程分解为去噪自动编码器的顺序应用，扩散模型（DMs）在图像数据及其他领域上实现了最先进的合成结果。此外，它们的构造允许引入**指导机制**以控制图像生成过程而无需重新训练。然而，由于这些模型通常直接在像素空间中操作，优化强大的DMs往往需要消耗数百个GPU天的时间，而inference由于顺序评估而变得昂贵。为了在有限的计算资源上进行DM训练并保持其质量和灵活性，我们将它们应用于强大的预训练自动编码器的潜在空间。与以前的工作相比，训练扩散模型在这样的表示上首次实现了在复杂性降低和细节保留之间接近最优点的目标，极大地提高了视觉保真度。通过在模型架构中引入交叉注意力层，我们将扩散模型转化为强大而灵活的生成器，可以接受文本或边界框等通用条件输入，并以卷积方式实现高分辨率合成。我们的潜在扩散模型（LDMs）在图像修复和类条件图像合成方面取得了新的最先进成绩，并在各种任务上表现出极具竞争力的性能，包括文本到图像合成、无条件图像生成和超分辨率，同时与基于像素的DMs相比，显著降低了计算要求。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;图像合成(Image synthesis)是计算机视觉领域中最近发展最引人注目的领域之一，但也是计算需求最大的领域之一。特别是对于复杂自然场景的高分辨率合成，目前主要通过扩展基于似然的模型来实现，这些模型可能包含数十亿个参数的自回归（AR）变换器[66,67]。相比之下，GAN（生成对抗网络）[3, 27, 40]的有希望结果主要局限于具有相对有限变化性的数据，因为它们的对抗学习过程**不容易**扩展到建模复杂的多模态分布。最近，扩散模型[82]，由一系列去噪自编码器构建而成，在图像合成[30,85]以及其他领域[7,45,48,57]上取得了令人印象深刻的结果，并在类别条件图像合成[15,31]和超分辨率[72]方面定义了最先进的水平。此外，即使是**无条件的扩散模型**也可以轻松应用于修复和着色等任务[85]，或者基于笔划的合成[53]，而其他类型的生成模型[19,46,69]则不行。作为基于似然的模型，它们不会出现GAN的模式崩溃和训练不稳定问题，并且通过充分利用参数共享，它们可以模拟高度复杂的自然图像分布，而无需像AR模型[67]那样涉及数十亿个参数。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**民主化高分辨率图像合成** 扩散模型属于基于似然的模型类别，其模式覆盖行为使其倾向于在建模数据的不可感知细节上消耗过多的容量（因此需要计算资源）[16, 73]。尽管重新加权的变分目标[30]旨在通过对初始去噪步骤进行欠采样来解决这个问题，但扩散模型仍然需要大量的计算资源，因为在RGB图像的高维空间中训练和评估这样的模型需要重复的函数评估（和梯度计算）。例如，训练最强大的扩散模型通常需要数百个GPU天（例如，在[15]中需要150-1000个V100天），在噪声版本的输入空间上进行重复评估也使得推理变得昂贵，因此生成5万个样本需要大约5天[15]在单个A100 GPU上。这对于研究界和普通用户有两个影响：首先，训练这样的模型需要大量的计算资源，只有少部分领域内的人才能够获得，并且会留下巨大的碳足迹[65, 86]。其次，评估已经训练好的模型在时间和内存方面也很昂贵，因为同一模型架构必须按顺序运行大量步骤（例如，在[15]中是25-1000步）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了增加这个强大模型类的可访问性，同时减少其显著的资源消耗，需要一种方法来降低训练和采样的计算复杂度。因此，**降低扩散模型的计算需求而不损害其性能**对于提高其可访问性至关重要。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**转向潜空间**  我们的方法始于对已经训练好的扩散模型在像素空间的分析：图2展示了一个训练好的模型的失真率权衡。与任何基于似然的模型一样，学习可以大致分为两个阶段：首先是感知压缩阶段，该阶段去除高频细节但仍学习了少量语义变化。在第二阶段，实际的生成模型学习了数据的语义和概念组合（语义压缩）。因此，我们的目标是首先找到一个感知等效但计算上更合适的空间，在该空间中我们将训练用于高分辨率图像合成的扩散模型。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;按照常见的做法[11, 23, 66, 67, 96]，我们将训练分为两个不同的阶段：首先，我们训练一个自编码器，它提供了一个低维（因此高效）的表征空间，该空间在感知上等同于数据空间。重要的是，与先前的工作[23,66]不同，我们不需要依赖过度的空间压缩，因为我们在学习得到的潜空间中训练扩散模型，这个潜空间在空间维度上具有更好的可扩展性属性。降低的复杂性还使得从潜空间中通过单个网络传递进行高效的图像生成成为可能。我们将得到的模型类称为潜空间扩散模型(LDMs: latent diffusion model)。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方法的一个显著优势是我们只需要对通用的自编码(autoencoding)阶段进行**一次训练**，因此可以将其重复使用于多个扩散模型的训练或者用于探索可能完全不同的任务[81]。这使得可以高效地探索大量的扩散模型，用于各种图像到图像和文本到图像的任务。对于后者，我们设计了一个架构，将transformer连接到扩散模型的UNet骨干[71]，并支持任意类型的基于token的条件机制，详见第3.3节。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文的贡献总结如下：<br>
1. 与纯基于transformer的方法[23, 66]相比，我们的方法在处理更高维度数据时具有更好的可扩展性，因此可以：(a)在提供比先前工作更准确和详细的重建的压缩级别上工作（参见图1），以及(b)高效地应用于百万像素图像的高分辨率合成。<br>
2. 我们在多个任务（无条件图像合成、修复、随机超分辨率）和数据集上实现了竞争性的性能，同时显著降低了计算成本。与基于像素的扩散方法相比，我们还显著降低了推理成本。<br>
3. 与先前的工作[93]相比，该工作同时学习了编码器/解码器架构和基于分数的先验，而我们的方法不需要对重建能力和生成能力进行精细的加权。这确保了非常准确的重建，并且对潜空间的正则化要求非常低。<br>
4. 我们发现，对于密集条件任务，例如超分辨率、修复和语义合成，我们的模型可以以卷积的方式应用，并生成大型、一致的约为 $1024^{2}$ 像素的图像。<br>
5. 此外，我们设计了一个**基于交叉注意力的通用条件机制**，实现了**多模态训练**。我们使用它来训练类别条件、文本到图像和layout到图像的模型。<br>
6. 最后，我们在 https : //github.com/CompVis/latent-diffusion 上发布了预训练的潜空间扩散和自编码模型，这些模型在除了扩散模型的训练之外的各种任务中可能是可重用的 [81]。

# 2 相关工作
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;图像合成(image Synthesis)的生成模型面临着高维数据的独特挑战。生成对抗网络（GAN）[27]可以高效地采样具有良好感知质量的高分辨率图像[3, 42]，但难以优化[2, 28, 54]，并且难以捕捉完整的数据分布[55]。相比之下，基于似然的方法强调良好的密度估计，从而使得优化过程更加可控。变分自编码器（VAE）[46](https://arxiv.org/pdf/1312.6114.pdf)和基于流的模型[18, 19]可以有效地合成高分辨率图像[9, 44, 92]，但样本质量不及GAN。而自回归模型（ARM）[6, 10, 94, 95]在密度估计方面表现出色，但计算复杂的架构[97]和顺序采样过程限制了它们在低分辨率图像上的应用。由于基于像素的图像表示包含几乎不可感知的高频细节[16,73]，最大似然训练在对其建模时耗费了不成比例的容量，导致训练时间较长。为了应对更高分辨率的图像，一些两阶段方法[23,67,101,103]使用ARM **对一个压缩的潜在图像空间进行建模，而不是直接对原始像素进行建模**。 <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最近，扩散概率模型（DM）[82]在密度估计[45]和样本质量[15]方面取得了最先进的结果。这些模型的生成能力源于当它们的底层神经骨干以UNet[15, 30, 71, 85]的形式实现时，对类似图像的数据的归纳偏好的自然契合。通常情况下，使用重新加权的目标函数[30]进行训练可以达到最佳的合成质量。在这种情况下，DM对应于一个有损压缩器，并且允许在图像质量和压缩能力之间进行权衡。然而，**在像素空间中评估和优化这些模型的缺点是推理速度较慢且训练成本非常高**。虽然前者可以通过先进的采样策略[47, 75, 84]和分层方法[31, 93]部分解决，但在高分辨率图像数据上的训练始终需要计算昂贵的梯度。我们提出的LDMs可以解决这两个缺点，它们**在更低维度的压缩潜空间上进行操作**。这使得训练计算成本更低，并且推理速度加快，几乎不会降低合成质量（参见图1）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了弥补各种生成方法的缺点，很多研究[11, 23, 67, 70, 101, 103]采用了两阶段方法将不同方法的优势结合起来，构建更高效和性能更好的模型。VQ-VAEs [67, 101]使用自回归模型在离散化的潜空间上学习表达丰富的先验分布。[66]将这种方法扩展到文本到图像生成，通过学习离散化图像和文本表示的联合分布。更一般地，[70]使用有条件可逆网络在不同领域的潜空间之间提供通用的转换。与VQ-VAEs不同，VQGANs [23, 103]采用了第一阶段的对抗性和感知性目标，将自回归Transformer扩展到更大的图像。然而，可行的ARM训练所需的高压缩比会引入数十亿可训练参数[23, 66]，限制了此类方法的整体性能，而更低的压缩率则以高计算成本为代价[23, 66]。我们的工作避免了这种权衡，因为我们提出的LDMs由于其卷积骨干，在更高维度的潜空间上更温和地进行扩展。因此，我们可以自由选择适当的压缩级别，在学习强大的第一阶段的同时，不过度依赖生成扩散模型进行感知上的压缩，并保证高保真度的重建（参见图1）。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然已经存在同时学习编码/解码模型和基于评分的先验模型的方法，例如联合学习方法[93]或分别学习方法[80]，但前者仍需要在重建和生成能力之间进行困难的加权选择[11]，并且在我们的方法（第4节）的性能上被超越，而后者则侧重于高度结构化的图像，如人脸。<br>

# 3 方法
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了降低训练扩散模型用于高分辨率图像合成的计算需求，我们观察到，尽管扩散模型可以通过对应的损失项进行欠采样来忽略感知上不相关的细节[30]，但它们仍然需要在像素空间中进行昂贵的函数评估，这导致了巨大的计算时间和能源资源需求。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们提出通过引入压缩学习阶段和生成学习阶段的明确分离来避免(circumvent)这个缺点（见图2）。为了实现这一点，我们利用一个自编码模型(auto-encoder model)，它学习了一个**在感知上等价于图像空间的空间**，但具有显著降低的计算复杂性。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方法具有几个优点：(i) 通过离开高维图像空间，我们获得了计算效率更高的扩散模型，因为采样是在低维空间上进行的。(ii) 我们利用了扩散模型从UNet架构[71]继承的归纳偏好，使其在具有空间结构的数据上特别有效，因此减轻了之前方法[23, 66]所需要的过于激进的、降低质量的压缩级别的需求。(iii) 最后，我们获得了**通用压缩模型**，其潜空间可以用于训练多个生成模型，也可以用于其他下游应用，例如基于单图像的CLIP引导合成[25]。<br>

## 3.1. 感知图像压缩(Perceptual Image Compression)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们的感知压缩模型基于先前的工作[23]，由一个自编码器组成，通过感知损失[106]和基于补丁的对抗目标[33]的组合进行训练。这样可以通过强制局部真实性将重建限制在图像流形上，并避免仅依赖像素空间损失（如L2或L1目标）引入的模糊性。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更确切地说，给定一个在RGB空间中的图像 $x∈R^{H×W×3}$ ，编码器 E 将 x 编码为潜在表示 z = E(x)，解码器D从潜在表示中重建图像，给出 
 $\tilde{x}=\mathcal{D}(z)=\mathcal{D}(\mathcal{E}(x))$ ，其中 $z∈R^{h×w×c}$ 。需要注意的是，编码器通过因子 f = H/h = W/w 对图像进行下采样，我们探究不同的下采样因子 $f = 2^{m}$ ，其中m ∈ N。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了避免任意高方差的潜空间，我们尝试了两种不同类型的正则化方法。**第一种变体是KL-reg.**，对学习到的潜在空间施加了轻微的KL惩罚，使其接近标准正态分布，类似于VAE [46, 69]。而**VQ-reg**则在解码器中使用了向量量化层 [96]。这个模型可以看作是VQGAN [23]，但是量化层被解码器吸收了。由于我们**后续的扩散模型是针对我们学到的二维潜在空间z = E(x)的结构设计的**，我们可以使用相对温和的压缩率并实现非常好的重建效果。这与之前的工作[23, 66]形成对比，之前的工作依赖于对学习空间z进行任意的一维排序，以自回归方式对其分布进行建模，从而忽略了z的很多内在结构。因此，**我们的压缩模型更好地保留了x的细节**（参见Tab. 8）。完整的目标函数和训练细节可以在补充材料中找到。<br>

## 3.2 Latent Diffusion Models
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;扩散模型[82]是一种概率模型，旨在通过逐渐去噪一个符合正态分布的变量来学习数据分布p(x)，这**相当于学习长度为T的固定马尔可夫链的逆过程**。对于图像合成，最成功的模型[15,30,72]依赖于对p(x)的变分下界的重新加权变体，这类似于去噪得分匹配[85]。这些模型可以被解释为一系列等权重的去噪自编码器 $\epsilon_{\theta}\left(x_{t}, t\right)$ . t = 1 ... T，它们**被训练用于预测其输入 $x_{t}$ 的去噪变体**，其中 $x_{t}$ 是输入x的含噪版本。相应的目标可以简化为（参见B节）<br>

![algorithm1](images/algorithm1.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过我们训练的由Encoder和Decoder组成的感知压缩模型，我们现在可以访问一个高效(efficient)、低维(low-dimensional)的潜在空间，其中高频(high-frequency)、难以察觉(imperceptible)的细节被抽象化了。与高维像素空间(pixel space)相比，这个空间更适合基于似然的生成模型，因为它们现在可以（i）专注于数据的重要**语义部分**，（ii）在一个低维、计算效率更高的空间中进行训练。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与之前依赖于高度压缩的离散潜在空间中的自回归、基于注意力的Transformer模型的工作[23,66,103]不同，我们可以利用我们的模型提供的图像特定的归纳偏好。这包括能够主要使用二维卷积层构建底层UNet，并进一步通过重新加权的边界将目标聚焦在感知上最相关的位上，其形式如下：<br>

![algorithm2](images/algorithm2.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们模型的神经骨干部分 $\epsilon_{\theta}(o, t)$ 是一个基于**时间条件**的UNet [71]。由于前向过程是固定的，在训练过程中可以高效地从Encoder中获取 $z_{t}$ ，并且从p(z)中解码得到的样本可以通过一次Decoder转换到图像空间。<br>

## 3.3 条件机制
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;类似于其他类型的生成模型[56, 83]，扩散模型原则上能够建模形式为p(z|y)的条件分布。这可以通过引入一个**条件去噪自编码器**  $\epsilon_{\theta}(x_{t}, t, y)$ 来实现，并为通过输入y（如文本[68]、语义地图[33, 61]或其他图像到图像的转换任务[34]）控制合成过程铺平了道路。<br>






