# 混合精度训练(MIXED PRECISION TRAINING)

# 摘要：
增加神经网络的大小通常可以提高准确性，但也会增加训练模型的内存和计算需求。我们引入了一种使用半精度浮点数训练深度神经网络的方法，既不会丢失模型准确性，也不需要修改超参数。这几乎将内存需求减少了一半，并且在最新的GPU上加快了计算速度。权重、激活和梯度以IEEE半精度格式存储。由于该格式的范围比单精度要**窄**，我们提出了**三种**防止关键信息丢失的技术。首先，我们建议在每个优化器步骤(step)之后保持(maintaining)一个单精度的权重副本用于梯度的累积（此副本在前向传播和反向传播时舍入(rounded)为半精度）。其次，我们提出了**损失缩放(loss-scaling)来保留具有较小幅度的梯度值**。第三，我们使用半精度算术进行计算，并将计算结果累积到单精度的输出中，然后在存储到内存之前将这些单精度的输出转换为半精度。我们证明了所提出的方法在各种任务和现代大规模（超过1亿个参数）的模型架构上都适用，这些模型在大型数据集上进行训练。<br>

# 1 引言
深度学习在许多不同的应用中取得了进展，包括图像识别（He等，2016a），语言建模（Jozefowicz等，2016），机器翻译（Wu等，2016）和语音识别（Amodei等，2016）。这些成果中有两个关键趋势：越来越大的训练数据集和越来越复杂的模型。例如，在Hannun等人（2014）中使用的神经网络有1100万个参数，而在Amodei等人（2016）的双向RNN模型中增长到约6700万个参数，最新的单向门控循环单元（GRU）模型中进一步增长到1.16亿个参数。<br>
较大的模型通常需要更多的计算和内存资源来进行训练。通过使用降低精度的表示和算术运算，可以降低这些需求。任何程序的性能（速度），包括神经网络的训练和推理，都受到三个因素之一的限制：**算术带宽、内存带宽或延迟**。降低精度可以解决其中两个限制因素。通过使用较少的位数存储相同数量的值，可以降低内存带宽压力。在支持降低精度数学运算的处理器上，算术运算的时间也可以降低。例如，最近的GPU中，**半精度数学运算的吞吐量比单精度高2倍到8倍**。除了提高速度，降低精度的格式还可以减少训练所需的内存量。<br>
现代深度学习训练系统通常使用单精度（FP32）格式。在本文中，我们解决了在保持模型准确性的同时使用降低精度进行训练的问题。具体而言，我们使用IEEE半精度格式（FP16）对各种神经网络进行训练。由于FP16格式的动态范围比FP32更窄，我们引入了三种技术来防止模型准确性损失：**在FP32中保持权重的主副本**，**通过损失缩放来最小化梯度值为零**，以及**在FP32中进行累积的FP16算术(算术运算)**。使用这些技术，我们证明了可以训练各种网络架构和应用，并达到与FP32训练相当的准确性。实验结果包括卷积网络和循环网络架构，用于分类、回归和生成任务。应用领域包括图像分类、图像生成、目标检测、语言建模、机器翻译和语音识别。所提出的方法不需要对模型或训练超参数进行任何更改。<br>

# 2 相关工作
关于使用降低精度训练卷积神经网络（CNN）的研究已经有很多发表的论文。Courbariaux等人（2015）提出了使用二值权重进行训练，而其他张量和算术运算则采用完整精度。Hubara等人（2016a）扩展了这项工作，将激活值也进行了二值化，但梯度存储和计算仍使用单精度。Hubara等人（2016b）考虑了将权重和激活值量化为2、4和6位，而梯度则为实数。Rastegari等人（2016）将所有张量（包括梯度）进行了二值化。然而，所有这些方法在训练更大的CNN模型进行ILSVRC分类任务（Russakovsky等，2015）时都会导致非常明显的准确性损失。Zhou等人（2016）将权重、激活值和梯度量化为不同的位数，以进一步提高结果的准确性。但这仍然会导致一定的准确性损失，并且需要在每个网络上搜索位宽配置，对于较大的模型来说可能不实际。Mishra等人通过在流行的CNN中将层的宽度增加一倍或三倍，改进了之前对权重和激活值量化所达到的Top-1准确性。然而，梯度仍然使用单精度进行计算和存储，而量化模型的准确性低于加宽基准模型的准确性。Gupta等人（2015）**证明了16位定点表示可以用于在MNIST和CIFAR-10数据集上训练CNN而不会损失准确性**。目前还不清楚这种方法在在大型数据集上训练的较大CNN上的效果如何，以及它是否适用于循环神经网络（RNNs）。<br>
也有几个提议对RNN训练进行量化。He等人（2016c）训练了GRU（Cho等，2014）和长短期记忆（LSTM）（Hochreiter和Schmidhuber，1997）单元的量化变体，以减少权重和激活值的位数，尽管会略微损失准确性。目前还不清楚他们的结果是否适用于需要更大网络来处理更大数据集的情况。Hubara等人（2016b）提出了另一种在不改变RNN结构的情况下量化RNN的方法。Ott等人（2016）提出了另一种量化RNN的方法。他们评估了在不同的RNN模型（用于语言建模和语音识别）中对权重进行二值化、三值化和指数化量化的效果。所有这些方法都保持了梯度在单精度中不变，因此在反向传播过程中的计算成本没有改变。<br>
这篇论文提出的技术与上述方法在三个方面有所不同。首先，我们在前向和反向传播中使用了降低精度（在我们的情况下是FP16）的所有张量和算术运算。其次，没有调整超参数（如层宽）。最后，与单精度基准模型相比，使用这些技术训练的模型**不会损失准确性**。我们证明了这种技术在使用大规模数据集训练的最先进模型的各种应用中都是有效的。<br>

# 3 实施细节
我们介绍了使用FP16进行训练的关键技术，同时保持与FP32训练相匹配的模型准确性：单精度主权重和更新(updates --> grad)、损失缩放以及将FP16乘积累积到FP32中。使用这些技术进行训练的结果将在第4节中呈现。<br>

## 3.1 FP32权重的主副本
在混合精度训练中，**权重、激活值和梯度都以FP16格式存储**。为了与FP32网络的准确性相匹配，**在优化器步骤中使用FP32的主权重副本，并将其与权重梯度进行更新**。在每个迭代中，FP16的主权重副本用于前向和反向传播，减少了FP32训练所需的存储和带宽。图1说明了这个混合精度训练过程。<br>

![figure1](images/mixed-precision-figure1.jpg)

虽然并非所有网络都需要FP32主权重，但有两个可能的原因解释为什么一些网络需要它。<br>
-**溢出错误** 一个解释是更新（权重梯度乘以学习率）变得太小，无法用FP16表示 - 任何幅度小于 $2^{−24}$ 的值将在FP16中变为零。从图2b可以看出，**大约有5%的权重梯度值的指数小于−24**.这些小值梯度在与学习率相乘时会变为零，并对模型的准确性产生不利影响。使用单精度副本来进行更新可以解决这个问题并恢复准确性。<br>
*注释：fp16低精度带来的两个问题：1.溢出错误：fp16能表示的最小数为 $2^{-24}$, 比这个数还小时出现下溢出；2. 舍入误差：fp16的数据间隔是 $2^{-14}$ 。* <br>

![figure2](images/mixed-precision-figure2.jpg)

- **舍入误差** 另一个解释是权重值与权重更新值(weight update value)之间的**比值非常大**。在这种情况下，即使权重更新(weight update)在FP16中可表示，当加法操作将其(梯度更新)右移以使二进制点与权重(weight)对齐时，它可能仍然变为零。这种情况可能发生在归一化的权重值的幅度至少比权重更新大2048倍的情况下。由于FP16具有10 bit 的尾数(mantissa)位，隐式位(10个小数位的第一个位置)必须右移11个或更多位置才能可能变为零（在某些情况下，舍入可以恢复值）。在比值大于2048的情况下，隐式位将右移12个或更多位置。这将导致权重更新变为零，无法恢复。对于非归一化的数字，更大的比值将产生相同的效果。同样，可以通过在FP32中计算更新(weight updtate)来抵消这种效应。<br>
*注释：2^(-14)~2^16之间有30个区间：2^(-14)~2^(-13), 2^(-13)~2^(-12), ......, 2^(-1)~1, 1~2, 2~4, ......, 2^15~2^16。，不管区间的大小，float都会把这个区间分为2^m即1024份；造成每个范围内数据间隔不同；当两个相差很多的数相加时，要服从最大数所在区间的间隔，这样小的数很容易被忽略造成舍入误差, 因此：混合精度训练时乘法可以用低精度，而加法必须要用高精度。* <br>
*参考：https://blog.csdn.net/guanyonglai/article/details/127058613* <br>

为了说明需要使用FP32主权重副本，我们使用普通话语音模型(Mandarin speech model)对800小时的语音数据集进行了20个epoch的训练（更详细的描述在第4.3节中）。如图2a所示，在进行FP16前向和反向传递后，更新FP32主权重副本可以达到与FP32训练结果相匹配的效果，而直接更新FP16权重则导致了80%的相对准确性损失。<br>
尽管保留额外的权重副本与单精度训练相比会使权重的内存需求增加50%，但对整体内存使用的影响要小得多。在训练过程中，内存消耗主要由激活值(activation)所主导，这是由于更大的批量大小和每个层的激活值被保存以便在反向传播过程中重复使用。由于激活值也以半精度格式（FP16）存储，因此用于训练深度神经网络的总体内存消耗大约减少了一半。<br>

# 3.2 损失缩放
FP16指数偏置(b)将归一化值指数的范围调整为[-14, 15]，而实际上梯度值往往被小的幅度所主导（负指数）。例如，考虑图3显示的**激活梯度值**的直方图，该直方图是在Multibox SSD检测器网络（Liu等，2015a）的FP32训练过程中收集到的。请注意，**很大一部分FP16可表示范围未被使用，而许多值低于最小可表示范围，并变为零**。通过**缩放梯度**，可以将其移动到更广的可表示范围内，并保留否则将变为零的值。这个特定的网络在不进行梯度缩放时发散，但将梯度缩放因子设为8（将指数增加3）足以达到与FP32训练相匹配的准确性。这表明在这个模型的训练中，幅度小于 $2^{-27}$ 的激活梯度值是无关紧要的，但在[ $2^{-27}$ ，$2^{-27}$ )范围内的值是重要的，并需要保留。<br>

![figure2](images/mixed-precision-figure2.jpg)

一种有效的方法是在进行反向传播之前，通过在前向传递中计算的**损失值进行缩放**，将梯度值移动到FP16可表示的范围内。通过链式法则，反向传播确保所有梯度值按相同的比例进行缩放。这在反向传播过程中不需要额外的操作，并且可以防止相关的梯度值变为零。在权重更新之前，必须对权重梯度进行缩放以保持更新幅度与FP32训练中的一致性。最简单的方法是在反向传播**后**但梯度剪裁或其他与梯度相关的计算之**前**进行这种缩放，确保不需要调整任何超参数（如梯度剪裁阈值、权重衰减等）。<br>

