# 混合精度训练(MIXED PRECISION TRAINING)

# 摘要：
增加神经网络的大小通常可以提高准确性，但也会增加训练模型的内存和计算需求。我们引入了一种使用半精度浮点数训练深度神经网络的方法，既不会丢失模型准确性，也不需要修改超参数。这几乎将内存需求减少了一半，并且在最新的GPU上加快了计算速度。权重、激活和梯度以IEEE半精度格式存储。由于该格式的范围比单精度要窄，我们提出了三种防止关键信息丢失的技术。首先，我们建议在每个优化器步骤之后保持一个单精度的权重副本，累积梯度（此副本在前向传播和反向传播时舍入为半精度）。其次，我们提出了损失缩放来保留具有较小幅度的梯度值。第三，我们使用半精度算术将累积到单精度输出，并在存储到内存之前将其转换为半精度。我们证明了所提出的方法在各种任务和现代大规模（超过1亿个参数）的模型架构上都适用，这些模型在大型数据集上进行训练。
