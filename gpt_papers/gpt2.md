# 语言模型是无监督多任务学习器（Language Models are Unsupervised Multitask Learners）

# 摘要
在自然语言处理任务中，例如问答、机器翻译、阅读理解和摘要，通常采用监督学习来处理特定任务的数据集。我们证明，当训练在一个称为WebText的数百万个网页的新数据集上时，语言模型开始学习这些任务而不需要任何明确的监督。当根据文档和问题条件，模型生成的答案达到CoQA数据集的F1 score 达到55时，它超过了4个基线系统中的3个，而不需要使用127,000多个训练样例。模型的容量对zero-shot任务迁移的成功至关重要，并且提高容量会以对数线性方式提高各种任务的性能。我们最大的模型GPT-2是一个1.5B参数变换器，在zero-shot设置下，在8个测试的语言建模数据集中的7个达到了最先进的结果，但仍然适合WebText。模型的样本反映了这些改进，包含连贯的段落文本。这些发现表明，建立语言处理系统的一条有希望的道路，它们从自然现象中学习如何执行任务。<br>

# 1 介绍
人工智能系统现在通过使用大型数据集、高容量模型和监督学习（Krizhevsky等，2012）（Sutskever等，2014）（Amodei等，2016），已经能够在期望的任务中表现出色。然而，这些系统很脆弱，对于数据分布（Recht等，2018）和任务规范（Kirkpatrick等，2017）的微小变化都很敏感。目前的系统更多的是狭窄的专家，而不是能干的通才。我们希望移向更通用的系统，它们能够执行许多任务，最终不需要手动创建和标记每个任务的训练数据集。<br>
现在建立机器学习系统的主流方法是收集一组训练样例，这些样例演示了所需任务的正确行为，训练系统模仿这些行为，然后在独立同分布(identically distributed)的留出样例(held-out)上测试其性能。这对于狭窄专家(narrow experts)的进步非常有用。然而，字幕模型的行为常常不稳定（Lake等，2017），阅读理解系统（Jia和Liang，2017），以及图像分类器（Alcorn等，2018）在多样性和可能输入的变化上的表现不一致，凸显了这种方法的一些不足。<br>
我们怀疑，在单一领域数据集上进行单一任务训练的普遍性是当前系统泛化能力不足的主要原因。当前架构要想朝着具有更稳健系统的方向发展很可能需要在广泛的领域和任务上进行训练和性能评估。最近，已经提出了几个基准测试，例如GLUE（Wang等，2018）和decaNLP（McCann等，2018），已开始研究这一问题。<br>
多任务学习（Caruana，1997）是提高性能的一种有前景的框架。然而，在自然语言处理领域，多任务训练仍处于初级阶段。最近的研究报告了温和的性能改进（Yogatama等，2019），到目前为止最有雄心壮志的两个尝试分别在总共10对和17对（数据集，目标）上进行了训练（McCann等，2018）（Bowman等，2018）。从元学习的角度来看，每个（数据集，目标）对是从数据集和目标的分布中采样得到的单个训练样本。当前的机器学习系统需要数百到数千个样本才能产生良好泛化性能的函数。这表明，多任务训练可能需要同样多的有效训练对才能实现其在当前方法下的潜力。使用当前的技术手段来继续扩大数据集的创建和目标的设计以达到可能需要的程度将非常困难。这促使我们探索执行多任务学习的其他设置。<br>
目前在语言任务(LM)上表现最佳的系统使用预训练(pre-train)和监督微调(supervised fine-tune)相结合。这种方法有着悠久的历史，并且趋向于更灵活的迁移(transfer)形式。首先，词向量被学习并用于任务特定架构的输入（Mikolov等，2013）（Collobert等，2011），然后，递归网络的上下文表示被传递（Dai和Le，2015）（Peters等，2018）。最近的研究表明，任务特定架构不再必要，而传递多个自注意力块(self-attention block)就足够了（Radford等，2018）（Devlin等，2018）。<br>
这些方法仍然需要有监督训练才能执行任务。当只有很少或没有有监督数据可用时，另一条研究线路展示了语言模型在执行特定任务方面的潜力，例如常识推理（Schwartz等，2017）和情感分析（Radford等，2017）。<br>
在本论文中，我们将这两个研究方向联系起来，并延续了更通用的迁移(transfer)方法的趋势。我们展示了语言模型可以在零样本(zero-shot)设置下执行下游(down-stream)任务——无需进行任何参数或架构修改。我们通过突出语言模型在零样本(zero-shot)设置下执行各种任务的能力，证明了这种方法的潜力。根据任务的不同，我们取得了有希望、有竞争力且达到了最先进水平的结果。<br>

# 2 方法
我们方法的核心是语言建模。语言建模通常被构建为从一组示例 $(x_{1}, x_{2}, ..., x_{n})$  中无监督地估计分布，其中每个示例由可变长度的符号序列 $(s_{1}, s_{2}, ..., s_{n})$ 组成。由于语言具有天然的顺序性，将符号的联合概率因子分解为条件概率的乘积是常见的做法（Jelinek和Mercer，1980）（Bengio等，2003）：
$$p(x)=\prod_{i=1}^{n} p\left(s_{n} \mid s_{1}, \ldots, s_{n-1}\right)$$


