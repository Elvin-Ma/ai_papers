# 语言模型是无监督多任务学习器（Language Models are Unsupervised Multitask Learners）

# 摘要
在自然语言处理任务中，例如问答、机器翻译、阅读理解和摘要，通常采用监督学习来处理特定任务的数据集。我们证明，当训练在一个称为WebText的数百万个网页的新数据集上时，语言模型开始学习这些任务而不需要任何明确的监督。当根据文档和问题条件，模型生成的答案达到CoQA数据集的F1 score 达到55时，它超过了4个基线系统中的3个，而不需要使用127,000多个训练样例。模型的容量对zero-shot任务迁移的成功至关重要，并且提高容量会以对数线性方式提高各种任务的性能。我们最大的模型GPT-2是一个1.5B参数变换器，在zero-shot设置下，在8个测试的语言建模数据集中的7个达到了最先进的结果，但仍然适合WebText。模型的样本反映了这些改进，包含连贯的段落文本。这些发现表明，建立语言处理系统的一条有希望的道路，它们从自然现象中学习如何执行任务。<br>

# 1 介绍
人工智能系统现在通过使用大型数据集、高容量模型和监督学习（Krizhevsky等，2012）（Sutskever等，2014）（Amodei等，2016），已经能够在期望的任务中表现出色。然而，这些系统很脆弱，对于数据分布（Recht等，2018）和任务规范（Kirkpatrick等，2017）的微小变化都很敏感。目前的系统更多的是狭窄的专家，而不是能干的通才。我们希望移向更通用的系统，它们能够执行许多任务，最终不需要手动创建和标记每个任务的训练数据集。<br>
现在建立机器学习系统的主流方法是收集一组训练样例，这些样例演示了所需任务的正确行为，训练系统模仿这些行为，然后在独立同分布(identically distributed)的留出样例(held-out)上测试其性能。这对于狭窄专家(narrow experts)的进步非常有用。然而，字幕模型的行为常常不稳定（Lake等，2017），阅读理解系统（Jia和Liang，2017），以及图像分类器（Alcorn等，2018）在多样性和可能输入的变化上的表现不一致，凸显了这种方法的一些不足。<br>
我们怀疑，在单一领域数据集上进行单一任务训练的普遍性是当前系统泛化能力不足的主要原因。当前架构要想朝着具有更稳健系统的方向发展很可能需要在广泛的领域和任务上进行训练和性能评估。最近，已经提出了几个基准测试，例如GLUE（Wang等，2018）和decaNLP（McCann等，2018），已开始研究这一问题。<br>
多任务学习（Caruana，1997）是提高性能的一种有前景的框架。然而，在自然语言处理领域，多任务训练仍处于初级阶段。最近的研究报告了温和的性能改进（Yogatama等，2019），到目前为止最有雄心壮志的两个尝试分别在总共10对和17对（数据集，目标）上进行了训练（McCann等，2018）（Bowman等，2018）。从元学习的角度来看，每个（数据集，目标）对是从数据集和目标的分布中采样得到的单个训练样本。当前的机器学习系统需要数百到数千个样本才能产生良好泛化性能的函数。这表明，多任务训练可能需要同样多的有效训练对才能实现其在当前方法下的潜力。使用当前的技术手段来继续扩大数据集的创建和目标的设计以达到可能需要的程度将非常困难。这促使我们探索执行多任务学习的其他设置。<br>
目前在语言任务(LM)上表现最佳的系统使用预训练(pre-train)和监督微调(supervised fine-tune)相结合。这种方法有着悠久的历史，并且趋向于更灵活的迁移(transfer)形式。首先，词向量被学习并用于任务特定架构的输入（Mikolov等，2013）（Collobert等，2011），然后，递归网络的上下文表示被传递（Dai和Le，2015）（Peters等，2018）。最近的研究表明，任务特定架构不再必要，而传递多个自注意力块(self-attention block)就足够了（Radford等，2018）（Devlin等，2018）。<br>
这些方法仍然需要有监督训练才能执行任务。当只有很少或没有有监督数据可用时，另一条研究线路展示了语言模型在执行特定任务方面的潜力，例如常识推理（Schwartz等，2017）和情感分析（Radford等，2017）。<br>
在本论文中，我们将这两个研究方向联系起来，并延续了更通用的迁移(transfer)方法的趋势。我们展示了语言模型可以在零样本(zero-shot)设置下执行下游(down-stream)任务——无需进行任何参数或架构修改。我们通过突出语言模型在零样本(zero-shot)设置下执行各种任务的能力，证明了这种方法的潜力。根据任务的不同，我们取得了有希望、有竞争力且达到了最先进水平的结果。<br>

# 2 方法
我们方法的核心是语言建模。语言建模通常被构建为从一组示例 $(x_{1}, x_{2}, ..., x_{n})$  中无监督地估计分布，其中每个示例由可变长度的符号序列 $(s_{1}, s_{2}, ..., s_{n})$ 组成。由于语言具有天然的顺序性，将符号的联合概率因子分解为条件概率的乘积是常见的做法（Jelinek和Mercer，1980）（Bengio等，2003）：<br>
$$p(x)=\prod_{i=1}^{n} p\left(s_{n} \mid s_{1}, \ldots, s_{n-1}\right)\ldots\ldots(1)$$

这种方法允许从概率分布p(x)以及形如 $p(s_{n−k}, ..., s_{n} | s_{1}, ..., s_{n−k−1})$ 的任何条件概率中进行可行的采样和估计。近年来，计算这些条件概率的模型的表达能力有了显著提高，例如自注意力架构，如Transformer（Vaswani等，2017）。<br>
以一个概率框架来表达学习执行单个任务可以被解释为估计一个条件分布p(output|input)。由于一个通用系统应该能够执行多种不同的任务，即使是对于相同的输入，它不仅应该根据输入条件，还应该根据要执行的任务进行条件建模，即p(output|input, task)。这在多任务学习和元学习的设置中已经得到了各种形式的规范化。任务条件往往在架构层面上实现，比如在（Kaiser等，2017）中的任务特定编码器和解码器中，或者在算法层面上实现，比如MAML（Finn等，2017）的内部和外部循环优化框架。但正如McCann等（2018）所示，语言提供了一种灵活的方式来将指定的任务、输入和输出，表示为一系列符号。例如，一个翻译训练示例可以写成序列(translate to french, english text, french text)。同样，一个阅读理解训练示例可以写成(answer the question, document, question, answer)。McCann等（2018）证明了可以训练一个单一模型，即MQAN，以推断和执行具有这种格式的示例上的多种不同任务。<br>
原则上，语言建模也能够学习McCann等人（2018）的任务，而无需明确监督哪些符号是要预测的输出。由于有监督的目标与无监督的目标相同，只是在序列的子集上进行评估，因此无监督目标的全局最小值也是有监督目标的全局最小值。在这种简化的设置中，与密度估计作为一种原则性的训练目标(training objective)相关的问题（如Sutskever等人，2015中讨论的）被暂时回避了。相反，问题变成了我们是否能够在实践中将无监督目标优化至收敛。初步实验证实，足够大的语言模型能够在这种玩具式(toy)的设置中进行多任务学习，但学习速度比显式监督方法要慢得多。<br>
虽然从上述明确设定的状态到“自然语言混乱环境”的转变是一个重要的步骤，但Westen（2016）在对话的背景下提出，需要开发能够直接从自然语言中学习的系统，并展示了一个概念验证：通过对教师输出(teacher output)进行前向预测，学习问答任务而无需奖励信号。虽然对话是一种有吸引力的方法，但我们担心它过于限制性。互联网包含大量的信息，可以被 passively（被动地）获取，无需进行交互式通信。我们的猜测是，具备足够容量的语言模型将开始学会推断和执行在自然语言序列中展示的任务，以更好地预测它们，而不管它们的获取方式如何。如果语言模型能够做到这一点，实际上就是在执行无监督的多任务学习。我们通过在各种任务上分析语言模型在zero-shot（零样本）设置下的性能来测试是否如此。

## 2.1 训练数据集(Training Dataset)
大部分以前的工作都是在单一领域的文本上训练语言模型，比如新闻文章（Jozefowicz等，2016）、维基百科（Merity等，2016）或小说（Kiros等，2015）。我们的方法鼓励构建尽可能大而多样的数据集，以收集各种领域和情境中的任务的自然语言演示。<br>
一个有希望的多样化且几乎无限的文本来源是像Common Crawl这样的网络爬取数据。尽管这些存档比当前的语言模型数据集大了几个数量级，但它们存在着显著的数据质量问题。Trinh和Le（2018）在他们对常识推理的研究中使用了Common Crawl，但指出了大量“内容大部分难以理解”的文档。在我们对Common Crawl进行初步实验时，我们也观察到了类似的数据问题。Trinh和Le（2018）在他们的最佳结果中使用了Common Crawl的一个小子样本，该子样本仅包含与他们目标数据集（Winograd Schema Challenge）最相似的文档。虽然这是一种在特定任务上提高性能的务实方法，但我们希望避免事先对要执行的任务做出假设。<br>
![table1](./images/gpt2_table2.jpg)

相反，我们创建了一个强调文档质量的新的网络爬取数据。为了做到这一点，我们只爬取了经过人工筛选/过滤的网页。手动筛选完整的网络爬取数据将是异常昂贵的，因此作为起点，我们从社交媒体平台Reddit上爬取了所有至少获得3个声望（karma）的外部链接。这可以被视为其他用户是否认为链接有趣、有教育意义或仅仅是有趣的启发式指标。<br>
得到的数据集名为WebText，它包含了这4500万个链接的文本子集。为了从HTML响应中提取文本，我们使用了Dragnet（Peters和Lecocq，2013）和Newspaper1内容提取器的组合。本文中呈现的所有结果都使用了WebText的初步版本，该版本不包括2017年12月之后创建的链接，并且经过去重和一些基于启发式的清理，包含了略多于800万个文档，总计40 GB的文本。我们从WebText中删除了所有维基百科的文档，因为维基百科是其他数据集的常见数据源，并且由于训练数据与测试评估任务有重叠，可能会使分析变得复杂。<br>

## 2.2 输入表示(Input Representation)
一个通用的语言模型（LM）应该能够计算（并生成）任意字符串的概率。当前的大规模LM包括预处理步骤，如小写处理、分词和未知词标记，这些步骤限制了可建模字符串的范围。虽然将Unicode字符串处理为UTF-8字节序列可以优雅地满足这一要求，例如Gillick等人（2015）的工作，但是当前的字节级LM在大规模数据集（如One Billion Word Benchmark，Al-Rfou等人，2018）上无法与词级LM相竞争。我们在尝试在WebText上训练标准字节级LM时也观察到了类似的性能差距。<br>

**注释：**
*字节级LM（Byte-level LM）：字节级LM将文本视为字节序列，其中每个字符都被编码为一个字节。这种方法将文本划分为最小的单元，即字节，不考虑其语义或词汇含义。字节级LM在处理多语言和特殊字符时具有优势，因为它们可以轻松处理各种Unicode字符，无论其复杂性如何。然而，由于字节级LM忽略了词汇信息，因此它们可能无法很好地捕捉到词语之间的语义关系。*
*词级LM（Word-level LM）：词级LM将文本视为词的序列，其中每个词被视为一个单独的单位。它基于词汇信息，可以更好地捕捉到语义关系和上下文含义。词级LM通常需要进行预处理步骤，如分词和词汇表构建，以将文本划分为单词。这种方法在许多自然语言处理任务中表现良好，例如机器翻译、文本生成和文本分类。*

字节对编码（Byte Pair Encoding，BPE）（Sennrich等人，2015）是字符级和词级语言建模之间的一种实用的中间方法，它在常见符号序列上有效地插值了词级输入，而在不常见的符号序列上使用字符级输入。尽管它的名字中包含"byte"（字节），但参考的BPE实现通常操作的是Unicode码点，而不是字节序列。为了建模所有的Unicode字符串，这些实现需要包括完整的Unicode符号空间。这将导致一个庞大的基础词汇表，包含136,690个符号，还没有添加任何多符号标记。与通常使用的32,000到64,000个标记词汇表相比，这个规模过大。相比之下，字节级版本的BPE只需要一个大小为256的基础词汇表。然而，直接将BPE应用于字节序列会导致子优化的合并，因为BPE使用基于频率的贪婪启发式方法来构建标记词汇表。我们观察到BPE包含了许多常见单词的多个版本，比如dog，因为它们有多种变体，如dog. dog! dog?。这导致了有限词汇槽和模型容量的次优分配。为了避免这种情况，我们阻止BPE在任何字节序列中跨字符类别合并。我们对空格做了一个例外，这显著提高了压缩效率，同时只对多个词汇标记中的单词进行了最小的碎片化。<br>

**注释**
*BPE算法的核心思想是通过反复合并最频繁出现的符号对来构建更大的符号。*
*贪婪合并的策略，它只基于当前频率进行决策，并没有考虑到未来可能的合并效益*
*子优化的合并（suboptimal merging）是指在合并符号对时，采用了基于频率的贪婪启发式方法，可能导致生成的合并结果并非最优的情况。*
*次优分配（suboptimal allocation）是指在标记词汇表构建过程中，由于合并操作的顺序或策略导致了对有限词汇槽和模型容量的次优分配的情况。*
*词汇槽（vocabulary slot）是指在自然语言处理中，用于存储和表示词汇表的一种数据结构或内存空间*

这种输入表示方法使我们能够结合基于词级语言模型的实证优势和基于字节级方法的通用性。由于我们的方法可以为任何Unicode字符串分配概率，这使得我们可以在任何数据集上评估我们的语言模型，而无论其预处理、分词或词汇大小如何。<br>

## 2.3 模型
我们在我们的语言模型中使用了基于Transformer（Vaswani等，2017）的架构。该模型在很大程度上遵循了OpenAI GPT模型（Radford等，2018）的细节，但进行了一些修改。层归一化（Ba等，2016）被移动到每个子块的输入处，类似于预激活残差网络（He等，2016），并在最终的自注意力块之后添加了额外的层归一化。使用了一种修改的初始化方法，考虑了模型深度中残差路径的累积。在初始化时，我们通过一个因子 $1 / \sqrt{N}$ (其中N是残差层的数量）来缩放残差层的权重。词汇表扩展到50,257个标记。我们还将上下文大小从512个标记增加到1024个标记，并使用更大的批量大小为512。



