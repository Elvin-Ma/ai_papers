# 变分自编码器(Auto-Encoding Variational Bayes)

- [论文链接](https://arxiv.org/pdf/1312.6114.pdf)

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在具有无法计算**后验分布**和大型数据集的**连续潜变量的有向概率模型**中，我们如何进行高效的推理和学习？我们引入了一种**随机变分推理和学习算法**，该算法可适用于大型数据集，并且在一些温和的可微条件下，甚至在无法计算的情况下也能工作。我们的贡献有两个方面。首先，我们展示了**变分下界的重新参数化可以产生一个下界估计器**，可以**直接使用标准随机梯度方法进行优化**。其次，我们展示了对于**具有连续潜变量的独立同分布数据集，通过使用所提出的下界估计器将推理模型（也称为识别模型）拟合到无法计算的后验分布中**，可以实现特别高效的后验推理。理论优势在实验结果中得到了体现。<br>

*(注释：有向概率模型（Directed Probabilistic Models）是一种概率图模型，也被称为贝叶斯网络（Bayesian Networks）或有向无环图（Directed Acyclic Graphs，DAGs）。它们用于建模随机变量之间的因果关系，其中变量之间的依赖关系通过有向边表示。）*

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在具有无法计算**后验分布**的连续潜变量和/或参数的有向概率模型中，我们如何进行高效的近似推理和学习？变分贝叶斯（VB）方法涉及对无法计算的后验的近似优化。不幸的是，常见的均场方法需要对近似后验的期望进行解析求解，而在一般情况下，这些解析求解也是无法计算的。我们展示了如何通过重新参数化变分下界，得到一个简单且可微的无偏估计器，用于下界的SGVB（随机梯度变分贝叶斯）估计器可以用于几乎任何具有连续潜变量和/或参数的模型中的高效近似后验推理，并且可以使用标准的随机梯度上升技术进行简单优化。<br>
*(注释：无法计算后验分布的模型指**解析计算**后验分布的积分或概率密度函数变得非常困难甚至不可行，需要使用近似推理方法，如变分推理、蒙特卡洛方法或优化方法来近似计算后验分布或其下界。）*

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;针对独立同分布（i.i.d.）数据集和每个数据点的连续潜变量，我们提出了**自动编码变分贝叶斯（AutoEncoding VB，AEVB）算法**。在AEVB算法中，通过使用SGVB估计器来优化一个识别模型，我们使推理和学习变得特别高效，这个识别模型可以使用简单的祖先抽样来进行非常高效的近似后验推理，从而允许我们有效地学习模型参数，而无需对每个数据点进行昂贵的迭代推理方案（如MCMC）。学习得到的近似后验推理模型还可以用于识别、去噪、表示和可视化等多种任务。**当识别模型使用神经网络时，我们得到了变分自编码器（Variational Auto-Encoder）**。<br>

# 2 方法
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本节中的策略可以用于推导具有连续潜变量的各种有向图模型的下界估计器（随机目标函数）。在这里，我们将限制在常见情况下，即具有每个数据点的独立同分布（i.i.d.）数据集和 latent variables，并且我们希望对（全局）参数执行最大似然（ML）或最大后验概率（MAP）推理，并对潜变量执行变分推理。例如，将此场景扩展到对全局参数执行变分推理是很简单的；该算法放在附录中，但对该情况的实验留待将来进行。请注意，我们的方法可以应用于在线、非平稳的设置，例如流式数据，但在这里我们为简单起见假设数据集是固定的。<br>

*(下界估计器是一种在概率推断中使用的技术，用于近似计算无法直接求解的目标函数或概率分布。它通过最大化一个辅助目标函数来逼近真实目标函数，并提供了一个可优化的下界。)* <br>
*(变分是指函数、分布或模型在某个空间中的变化或差异。在变分推理和变分自编码器中，我们利用这种变分来近似计算后验分布或实现数据的生成和重构。)* <br>

![figure1](images/vae-figure1.png)

图1：考虑的有向图模型类型。实线表示生成模型 $p_{θ}(z)p_{θ}(x|z)$ ，虚线表示对于难以处理的后验分布 $p_{θ}(z|x)$ 的变分近似 $q_{φ}(z|x)$ 。变分参数 $φ$  与生成模型参数 $θ$ 联合学习。<br>







