# PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel（在完全分片数据并行上的扩展经验）

摘要
众所周知，大型模型在各个领域都有潜在的优越性能。尽管机器学习系统研究领域取得了显著进展，使得大型模型的开发和探索成为可能，但这些能力仍然仅限于少数高级用户和行业领导者，从而为广大社区访问和利用这些技术设置了一道隐形的技术壁垒。本文介绍了PyTorch完全分片数据并行（FSDP）作为大型模型训练的行业级解决方案。FSDP与PyTorch的几个关键核心组件（包括张量实现、调度系统和CUDA内存缓存分配器）密切协同设计，以提供非侵入式的用户体验和高效的训练效率。此外，FSDP还原生地融合了一系列技术和设置，以优化各种硬件配置下的资源利用。实验结果表明，FSDP能够在TFLOPS方面实现与分布式数据并行相当的性能，同时支持显著更大的模型，并具有近线性的可扩展性。
