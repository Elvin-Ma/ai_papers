# PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel（在完全分片数据并行上的扩展经验）

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;众所周知，大型模型在各个领域都有潜在的优越性能。尽管机器学习系统研究领域取得了显著进展，使得大型模型的开发和探索成为可能，但**这些能力仍然仅限于少数高级用户和行业领导者**，从而为广大社区访问和利用这些技术设置了一道隐形的技术壁垒。本文介绍了PyTorch完全分片数据并行（FSDP）作为大型模型训练的行业级解决方案。FSDP与PyTorch的几个关键核心组件（包括张量实现、调度系统(dispatcher system)和CUDA内存缓存分配器）密切协同设计，以提供非侵入式(non-intrusive)的用户体验和高效的训练效率。此外，FSDP还原生地融合了一系列技术和设置，以优化各种硬件配置下的资源利用。实验结果表明，FSDP能够在TFLOPS方面实现与**分布式数据并行(DDP)相当的性能**，同时支持显著更大的模型，并具有近线性的可扩展性。<br>

*(代码：https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/fully_sharded_data_parallel.py/.）*

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;神经网络模型的规模正在以前所未有的速度增长，为各个领域的突破提供了便利。在问世之初，具有 1750 亿参数的 GPT-3 [3] 模型在几乎所有自然语言处理任务中创下了新纪录。构建在 GPT 模型之上的产品应用 [23] 迅速展示了它们改变整个行业的潜力。现代大规模推荐模型 [19, 33] 可以超过 1 万亿个参数，其中包括快速增长的密集层组件。这些模型驱动着每天为数十亿用户提供服务的应用程序。随着大型神经网络不断突破科学和技术的限制，一种工业级工具能够以高效的方式简化这些模型的训练，将有助于加快进展的速度。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;近年来，学术界引入并研究了许多先进的方法来扩大神经网络模型。管道并行性 [6, 8, 11, 15, 20] 将一个模型实例划分为多个阶段，并将这些阶段分布在多个设备上，激活值和梯度在阶段之间进行通信。张量并行性 [9, 21, 31, 32] 对模型参数进行分片，对每个设备进行部分计算，并在所需的层边界进行激活值的通信。零冗余并行性 [27, 28, 30] 也对参数进行分片，但按需通信参数以恢复其未分片的形式，并在每个设备上执行模型。上述技术作为实现在各种应用中训练大型神经网络的基本构建块。然而，仍然存在两个挑战。首先，其中一些方法与特定的模型架构紧密集成，限制了它们作为训练大型模型的**通用解决方案**的使用。其次，其中一些技术是基于不断演进的底层机器学习框架的内部接口构建的，这使得它们容易受到框架实现的变化的影响。因此，与机器学习框架的核心功能共同设计一个本地解决方案更加稳健和高效。此外，以**可组合和可定制的方式**构建这样的解决方案还有可能促进学术界未来的创新。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文介绍了 PyTorch [24] Fully Sharded Data Parallel (FSDP) 方法，通过对模型参数进行分片，实现了大规模模型的训练。FSDP 算法受到 DeepSpeed 中的 ZeroRedundancyOptimizer [27, 28] 技术的启发，但经过修改设计和实现，与 PyTorch 的其他组件相一致。FSDP 将一个模型实例分解为较小的单元，并对每个单元内的所有参数进行**扁平化和分片**。在计算之前，分片的参数会按需通信和恢复(recovered)，然后立即丢弃(immediately discarded)。这种方法确保 FSDP **一次只需要处理一个单元的参数**，极大地降低了内存消耗。FSDP 的设计和实现面临以下挑战。<br>

- **用户体验**对于广泛采用至关重要。在开发之前的 PyTorch 分布式训练功能（如 DistributeDataParallel，DDP）[14]时，我们观察到将分布式训练的用户体验与本地训练的体验保持一致可以显著降低学习门槛。像 DDP 这样的技术要求在每个设备上复制模型，这意味着整个模型可以在目标设备上构建。然而，尽管 FSDP 可以很容易地采用 DDP 的 API 设计，但大型模型可能无法适应一个 GPU 设备，因此**无法高效地初始化**。<br>
- **硬件异构性**常存在于现代 GPU 集群，在每台机器上将互连划分为高带宽岛屿和机器之间的低带宽网状结构。此外，机架(rack)或机柜(pod)级别可能还有进一步的层次结构。因此，FSDP 的设计必须适应这种异构性并进行相应的优化。<br>
- **资源利用**通常与资本和运营支出紧密相关，特别是对于依赖大型 GPU 集群来驱动关键系统的公司。为了确保在分布式训练期间 GPU 设备保持充分利用，关键是将非计算操作引起的停机时间最小化。<br>



