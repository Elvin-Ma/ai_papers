# PyTorch Distributed: Experiences on Accelerating Data Parallel Training

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文介绍了PyTorch分布式数据并行模块的设计、实现和评估。PyTorch是一个广泛采用的科学计算包，用于深度学习研究和应用。深度学习的最新进展表明大型数据集和大型模型的价值，这需要能够将模型训练扩展到更多的计算资源上。数据并行性已经成为分布式训练的流行解决方案，因为它具有直观的原理和广泛的适用性。一般而言，分布式数据并行技术在**每个计算资源上复制模型**，独立生成梯度，然后在每次迭代中通信这些梯度以保持模型副本的一致性。尽管这种技术在概念上很简单，但**计算和通信之间的微妙依赖关系使得优化分布式训练效率变得非常复杂**。从v1.5版本开始，PyTorch本地提供了多种技术来加速分布式数据并行，包括**梯度分桶、计算与通信的重叠以及跳过梯度同步**等。评估结果表明，适当配置时，PyTorch分布式数据并行模块在使用256个GPU时实现了近线性的可扩展性.<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度神经网络（DNN）已经为各种应用提供了动力，包括图像识别[20]、语言翻译[15]、异常检测[16]、内容推荐[38]、药物发现[33]、艺术生成[28]、游戏玩法[18]和自动驾驶汽车[13]等。许多应用通过使用更大的模型和更大的数据集来优化以追求更高的智能，这需要分布式训练系统的进一步发展。在现有的解决方案中，分布式数据并行是一种主导策略，因为它对系统的**干扰最小**。本文介绍了PyTorch v1.5中分布式数据并行包的设计、实现和评估。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;训练DNN模型通常需要反复进行三个步骤[26]：前向传播计算损失，反向传播计算梯度，以及优化器步骤更新参数。数据并行性的概念普遍适用于这样的框架。应用程序可以创建多个模型副本，每个模型副本都在一部分训练数据上工作，并**独立执行前向传播和反向传播**。然后，模型副本可以根据算法同步它们的梯度或更新的参数。在应用程序端纯粹构建一个可工作的数据并行版本理论上是可能的，因为它只需要在每次迭代中插入适当的通信。然而，为了挤出最后一丝性能，需要在设计和调优上投入大量的工作。在平台端提供本地的分布式数据并行API可以帮助应用程序开发人员专注于优化他们的模型，而平台开发团队可以持续透明地提高训练速度。提供一个通用的分布式数据并行包面临三个挑战。<br>
- **数学等价性**：数据并行的目的是加速在**大型数据集**上的训练。应用程序期望获得与在没有模型复制的情况下进行本地训练时相同的结果模型。这要求尽管是分布式的，但**在数学上等价于本地训练**。
- **非侵入性和拦截式API**：应用程序开发通常从本地模型开始，然后在需要时扩展。为了避免在过渡期间出现巨大的困难，API必须对应用程序代码**非侵入性**。另一方面，API需要允许内部实现及时拦截信号以进行通信和系统优化。
- **高性能**：数据并行训练受到计算和通信之间微妙依赖关系的影响。设计和实现必须探索解决方案空间，以将更多资源高效地转化为更高的训练吞吐量。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PyTorch提供了分布式数据并行作为一个nn.Module类，应用程序在构造时将它们的模型作为子模块提供。为了保证数学等价性，所有副本从相同的初始模型参数值开始，并在训练迭代中同步梯度以保持参数的一致性。为了最小化对应用程序的干扰，实现暴露了与用户模型相同的前向传播API，允许应用程序无需额外的代码更改，无缝地将用户模型的后续出现替换为分布式数据并行模型对象。设计中集成了几种技术来提供高性能的训练，包括**梯度分桶、计算与通信的重叠以及跳过同步**。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们在一个独立的32-GPU集群上进行了评估，并在一个更大的共享授权(entitlement)中使用了256个GPU。我们开发了基准测试来评估分布式包在不同规模下的性能，以深入了解不同优化技术和配置的性能影响。实验还涵盖了NCCL和Gloo通信库之间的比较。结果表明：<br>
1. **通信是训练延迟(latency)的主要因素**，并且其影响随着模型规模的增大而增加；
2. **桶大小对通信效率有很大影响**，如果适当配置，可以实现超过2倍的加速；
3. **适当跳过同步操作可以显著减少平摊的通信开销，而不明显降低收敛速度。**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文介绍的技术首次在PyTorch v1.1中发布。在过去的一年中，我们看到这些技术在内部和外部都得到了广泛采用。在Facebook内部，从2020年5月11日到2020年6月5日期间的工作负载研究显示，在各种应用程序中，包括语音、视觉、移动视觉、翻译等，超过60%的 production GPU hours 都用于PyTorch分布式数据并行包。本文有三个主要贡献。首先，本文揭示了一个被广泛采用的工业级分布式训练解决方案的设计和实现。其次，本文强调了以前工作中忽视的实际注意事项（例如，由于复数图形）。第三，我们分享了从为内部团队和开源社区用户提供服务中收集到的性能调优经验，并总结了未来改进的几个方向。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文的剩余部分组织如下。第2节简要介绍了PyTorch和数据并行性。第3节详细介绍了PyTorch分布式数据并行模块的设计。第4节和第5节分别介绍了实现和评估。然后，第6节讨论了所学到的经验和未来改进的机会，第7节调查了相关工作。最后，第8节总结了本文。<br>

# 2 背景
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在深入讨论分布式训练之前，让我们简要讨论使用PyTorch进行本地模型训练的实现和执行方式。然后，我们解释和证明了数据并行的概念，并描述了通信原语。<br>

## 2.1 PyTorch
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PyTorch将值(values)组织成张量（Tensors），它们是具有丰富的数据操作算子的通用n维数组。模块（Module）定义了从输入值到输出值的转换，其在前向传播期间的行为由其forward成员函数指定。一个模块可以包含张量作为参数。例如，线性模块（Linear Module）包含一个权重参数和一个偏差参数，其forward函数通过将输入与权重相乘并添加偏差来生成输出。应用程序通过在自定义的forward函数中连接本地模块（如线性模块、卷积模块等）和函数（如relu、池化等）来组合自己的模块。典型的训练迭代包括前向传播（使用输入和标签生成损失）、反向传播（计算参数的梯度）和优化器步骤（使用梯度更新参数）。更具体地说，在前向传播过程中，PyTorch构建自动求导图来记录执行的操作。然后，在反向传播过程中，它使用自动求导图进行反向传播以生成梯度。最后，优化器将梯度应用于更新参数。训练过程重复这三个步骤，直到模型收敛。<br>

## 2.2 数据并行性
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PyTorch提供了几种工具来促进分布式训练，包括: <br>
- DataParallel用于在**同一台机器上**使用多个GPU进行**单进程多线程**数据并行训练;
- DistributedDataParallel用于在**多个GPU和多台机器上**进行**多进程**数据并行训练;
- 以及RPC [6]用于通用分布式模型并行训练（例如参数服务器 [27]）。
本文重点介绍DistributedDataParallel。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据并行性通过**在优化器步骤之前传递梯度**来实现分布式训练，以确保所有模型副本的参数都**使用完全相同的梯度更新**，从而使模型副本在迭代中保持一致。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**参数平均**是另一种常用的扩展模型(scale-out)训练的技术。类似地，它可以在多台机器上启动多个进程，但与同步梯度不同，参数平均直接计算所有模型参数的平均值。这**发生在本地优化器步骤之后**，这意味着参数平均可以完全作为辅助步骤实现，不需要与本地训练步骤交互，这非常方便和清晰地**解耦了分布式训练和本地迭代的代码**。参数平均也存在一些注意事项。<br>

- 参数平均与本地训练相比，可能会产生截然不同的结果，有时会对模型的准确性产生不利影响。根本原因在于，参数平均在数学上与在本地处理所有输入数据并不等价，特别是当优化器依赖于过去的局部梯度值（例如动量）时。由于不同的模型副本可能看到不同的梯度，参数平均和本地训练相比可能会产生截然不同的结果，有时对模型的准确性有害。其根本原因是，参数平均在数学上与本地处理所有输入数据并不等效，特别是当优化器依赖于过去的局部梯度值（例如动量）时。由于不同的模型副本可能会看到不同的梯度，优化器中的状态可能逐渐发散，导致梯度下降方向冲突。这可能导致在从局部优化的模型切换到大规模部署的模型时，性能出现无法解释的差异。<br>
- 参数平均的结构将计算（即反向传播）和通信（即计算平均值）分为**不重叠**的阶段，使用优化器的**step()函数作为硬分界点**。无论我们如何强化计算或通信的优化，一种类型的资源在任何给定的时间点都会处于空闲状态，放弃了大量的性能优化机会。<br>
*(注释：因为反向传播需要知道weight的值）*

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于上述基本陷阱(pitfalls)，我们决定使用数据并行来实现分布式训练，以**同步梯度(gradient)而不是参数(parameters)**。请注意，应用程序仍然可以轻松地使用PyTorch构建参数平均。事实上，第3.3节中描述的集合通信功能是这种用例的适当解决方案。应用程序只需要显式地启动AllReduce操作，以相应地计算平均参数。<br>

## 2.3 AllReduce
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AllReduce是DistributedDataParallel使用的原始通信API，用于在所有进程之间计算梯度求和(sum)。它受到多个通信库的支持，包括NCCL [2]、Gloo [1]和MPI [4]。AllReduce操作要求每个参与的进程提供一个**大小相等的张量**，对来自所有进程的输入张量进行共同的算术操作（例如求和、乘积、最小值、最大值），并**将相同的结果张量返回给每个参与者**。一个简单的实现可以让每个进程将其输入张量广播到所有对等进程，然后独立地应用算术操作。然而，由于**AllReduce对分布式训练速度有重大影响**，通信库已经实现了更复杂、更高效的算法，例如基于环的AllReduce [2]和基于树的AllReduce [22]。由于一个AllReduce操作直到所有进程都加入才能开始，因此它被认为是一种**同步通信**，与**参数服务器 [27]** 中使用的点对点通信形成对比。<br>

# 3系统设计

![figure1](images/ddp-figure1.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PyTorch [30] 提供了一个DistributedDataParallel (DDP)模块，可以帮助在多个进程和机器上轻松并行化训练。在分布式训练中，每个进程都有自己的**本地模型副本**和**本地优化器**。就正确性而言，分布式数据并行训练和本地训练必须在数学上等价。DDP通过**确保所有模型副本从完全相同的模型状态开始**，并在每次反向传播后看到**相同的参数梯度**来保证正确性。因此，即使来自不同进程的优化器都是独立的，它们应该能够在每次迭代结束时将其本地模型副本带到相同的状态。图1展示了DDP的构建模块，包括Python API前端、C++梯度减少核心算法(gradient reduction core algorithm)，并采用c10d集合通信库。以下各节按照这个堆栈图的自顶向下顺序来介绍。<br>
*(注释：对于具有固有随机性的优化器，不同的进程可以使用相同的随机种子来初始化它们的状态。)* <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第3.1节介绍了API设计原则。第3.2节解释了PyTorch分布式数据并行训练中使用的梯度减少(gradient reduction)技术。最后，第3.3节讨论了DDP的集合通信后端。<br>

## 3.1 API
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在设计API时，我们制定了两个设计目标，以实现所需的功能。<br>
- 非侵入性：API必须对应用程序是非侵入性的。应用程序开发人员通常从编写本地训练脚本开始，在单台机器上达到资源限制时才扩展。此时，要求开发人员重写整个应用程序以启用分布式数据并行训练是不可接受的。相反，开发人员应该能够最小限度地修改并重用本地训练脚本。
- 拦截性(Interceptive)：API需要允许实现(implementation)拦截各种信号并及时触发适当的算法。分布式数据并行旨在通过使用更多计算资源加速训练。这个过程需要在计算和通信中进行微妙(subtle)的优化，以实现最佳性能。因此，API必须尽可能多地向内部实现暴露优化机会。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据上述要求，我们将**分布式数据并行(DDP)实现为一个nn.Module**，它将**本地模型(local model)作为构造函数参数**，并在反向传播中透明地同步梯度(synchronizes)。下面的代码片段展示了使用DDP模块的示例。该示例使用nn.Linear层在第10行创建一个本地模型。然后，在第11行将本地模型转换为分布式训练模型，并在第12行设置优化器。第14到23行是典型的前向传播、反向传播和优化器步骤的实现。在这个玩具(toy)分布式训练示例中，第11行是将本地训练应用程序转换为分布式应用程序的唯一区别，这满足了非侵入性的要求。它还满足了拦截性的要求。构造函数允许DDP检查模型结构和参数。**构造完成后，本地模型将被分布式模型替换**，然后可以轻松拦截(Intercept)forward()调用以执行相应的操作。对于反向传播，DDP依赖于**反向钩子**来触发(trigger)梯度减少，当在损失张量上执行backward()时，它(gradient reduction)将由自动求导引擎调用.<br>

```python
1 import torch
2 import torch.nn as nn
3 import torch.nn.parallel as par
4 import torch.optim as optim
5
6 # initialize torch.distributed properly
7 # with init_process_group
8
9 # setup model and optimizer
10 net = nn.Linear (10 , 10)
11 net = par.DistributedDataParallel( net )  # 唯一区别
12 opt = optim.SGD(net.parameters() , lr =0.01)
13
14 # run forward pass
15 inp = torch.randn (20 , 10)
16 exp = torch.randn (20 , 10)
17 out = net(inp)
18
19 # run backward pass
20 nn.MSELoss()(out , exp ).backward ()
21
22 # update parameters
23 opt.step ()
```

## 3.2 梯度减少（gradient reduction）
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DDP中的梯度减少算法在过去的版本中有所发展。为了介绍当前实现的结构，让我们从一个简单的解决方案开始，逐渐引入更多的复杂性，并在PyTorch v1.5.0中得到当前版本。这也将解释为什么在3.1节中描述的相同简单API允许我们安装各种性能优化算法。<br>

### 3.2.1 一个简单的解决方案
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;正如在第3节开头提到的，DDP通过让所有训练进程(1)从相同的模型状态开始，(2)在每次迭代中使用相同的梯度来保证正确性。前者可以通过在DDP构造时将**模型状态**从一个进程**广播**到所有其他进程来实现。要实现后者，一个简单的解决方案可以在**本地反向传播之后、更新本地参数之前插入梯度同步阶段**。然而，3.1节中展示的API在这个阶段之间没有提供明确的入口，因为在backward()和step()之间没有任何内容。幸运的是，PyTorch的自动求导引擎接受自定义的反向传播钩子。DDP可以**注册自动求导钩子**，以**在每次反向传播后触发计算**。当钩子触发时(fired)，每个钩子都会扫描所有本地模型参数，并从每个参数中检索梯度张量。然后，它使用AllReduce集合通信调用在所有进程上计算每个参数的平均梯度，并将结果写回梯度张量中。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个简单的解决方案对于我们的目的是足够的，但存在两个性能问题：
- 集体通信(collective communication)在小张量上的性能较差，这在具有大量小参数的大型模型上尤为明显。
- 将梯度计算和同步(synchronization)分开会失去在计算和通信之间进行重叠的机会，因为它们之间存在硬边界(hard boundary)。
下面的章节将阐述解决上述两个问题的方法。<br>

### 3.2.2 梯度分桶

![figure2](images/ddp-figure2.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;梯度分桶的思想源于一个观察结果，即集体通信在大张量上的效率更高。图2(a)和(b)提供了一个定量的观点，展示了使用不同参数数量进行AllReduce的总执行时间，其中包括60M个torch.float32参数。为了最大化带宽利用率，**所有的AllReduce操作都是异步启动的（launched asynchronously）**，并且一起等待所有操作完成，模拟DDP的梯度减少算法。实验是在一台启用了NVLink [3]的服务器上进行的，配备了两个NVIDIA Quadro GP100 GPU。NCCL[2] AllReduce直接在CUDA输入张量上运行，而Gloo[1] AllReduce在CPU输入张量上运行，以消除**使用Gloo后端时在CUDA内存和CPU内存之间复制**的开销。从图中可以清楚地看到，对于NCCL和Gloo，使用更大的输入张量时总的通信时间显著减少。Gloo在每个输入张量大约500K个参数时达到最高速度，而对于具有20M参数GPU张量的NVLink上的NCCL来说，没有明显的饱和信号。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这些实验表明，DDP可以在每个梯度张量可用时不立即启动专用的AllReduce，而是等待一段时间，将多个梯度分桶到一个AllReduce操作中，从而实现更高的吞吐量和更低的延迟。这对于具有许多小参数的模型尤其有帮助。然而，DDP不应该在一个单独的AllReduce中通信所有的梯度，否则在计算结束之前将无法开始通信。图2(c)和(d)显示了包含大约60M参数的ResNet152[20]的GPU和CPU反向计算时间。X轴是准备好的梯度数量，Y轴是自反向传播开始以来经过的时间。在**GPU上的反向传播大约需要250毫秒才能完成，与NVLink上的NCCL的时间量级相同**。这个结论也适用于Gloo和CPU的反向传播。这些测量结果表明，通过使用相对较小的桶大小，DDP可以在后向传播过程中并行启动AllReduce操作，以实现通信与计算的重叠，这将在每次迭代的延迟上产生差异。<br>

![figure3](images/ddp-figure3.jpg)

### 3.2.3通信重叠计算
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;梯度的AllReduce操作可以在本地反向传播完成**之前**开始。通过使用分桶，DDP只需在启动通信之前等待同一个桶中的所有内容。在这种设置下，仅在反向传播结束时触发AllReduce已经不再足够。它需要对**更频繁的信号**作出反应，并更及时地启动AllReduce。因此，DDP为每个梯度累加器注册一个自动求导钩子(autograd hook)。钩子在其对应的**梯度累加器更新梯度之后触发**，并检查其所属的桶。如果**同一个桶中的所有梯度的钩子都已经触发**，最后一个钩子将在该桶上触发一个异步的AllReduce操作。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有两个需要注意的问题。首先，所有进程的减少顺序(reducing order)必须相同，否则AllReduce的内容可能不匹配，导致错误的减少结果或程序崩溃。然而，PyTorch在每次前向传递中**动态**构建自动求导图，不同的进程可能无法就梯度准备的顺序达成一致。图3(a)展示了一个例子，其中两个垂直轴表示时间，虚线表示梯度何时准备就绪。在进程1中，四个梯度按顺序计算，但在进程2中，梯度 $g_{2}$ 在 $g_{3}$ 和 $g_{4}$ 之后计算。在这种情况下，如果所有进程在梯度准备就绪后立即进行AllReduce，那么AllReduce的内容将不匹配。因此，**所有进程必须使用相同的桶排序顺序**，并且在启动桶i+1之前，没有进程可以在桶i上启动AllReduce。**如果桶0是最后一个准备就绪的桶(桶0先开始)，通信就无法与计算重叠**。PyTorch v1.5.0通过使用model.parameters()的**相反顺序**作为桶排序顺序来解决了这个问题，假设层次很可能按照与前向传递中调用它们的顺序相同的顺序进行注册。因此，相反的顺序应该**近似**表示反向传播中的梯度计算顺序。诚然，这不是一个完美的解决方案，但是它是一个可以依靠的近似解决方案，几乎没有额外的工程开销。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二个需要注意的问题是，训练迭代可能只涉及模型中的一个子图，而且这个子图可能在每次迭代时都不同，这意味着**某些梯度在某些迭代中可能被跳过**。然而，由于**梯度到桶的映射是在构建时确定的**，这些缺失的梯度会**导致某些桶永远不会看到最终的自动求导钩子**，并且无法将桶标记为准备就绪。结果，反向传播可能会停滞不前。图3(b)展示了一个例子，在其中，与梯度g3对应的参数在某个迭代中被跳过，导致了g3的准备就绪信号缺失。为了解决这个问题，DDP从前向传递的输出张量（loss）开始遍历自动求导图，以找到所有参与的参数。这些参与的张量的准备就绪（readiness）是一个足够的信号，可以推断出反向传递的完成。因此，DDP可以避免等待其余参数梯度的到来，而是在前向传递结束时主动标记它们为准备就绪。需要注意的是，这个改变并不妨碍我们开发非侵入式的API，因为应用程序直接在DDP上调用前向函数，因此DDP可以在其成员函数中轻松插入这一步骤。 <br>

![algorithm](images/ddp-algorithm1.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;算法1展示了DDP的伪代码。构造函数包含两个主要步骤，**广播模型状态**和**安装自动求导钩子**。DDP的前向函数是本地模型前向传递的简单包装器，并**遍历自动求导图以在结束时标记未使用的参数**。**自动求导钩子以内部参数索引作为输入**，这有助于找到参数张量及其所属的桶。它**将局部梯度写入桶中的正确偏移位置**，然后启动异步的AllReduce操作(不同的桶可以同时开始)。伪代码中省略了一个额外的最终化步骤，该步骤等待AllReduce操作，并在反向传递结束时将值写回梯度中。图4阐明了DDP在前向传递和反向传递中如何与本地模型交互。<br>








