# PyTorch Distributed: Experiences on Accelerating Data Parallel Training

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文介绍了PyTorch分布式数据并行模块的设计、实现和评估。PyTorch是一个广泛采用的科学计算包，用于深度学习研究和应用。深度学习的最新进展表明大型数据集和大型模型的价值，这需要能够将模型训练扩展到更多的计算资源上。数据并行性已经成为分布式训练的流行解决方案，因为它具有直观的原理和广泛的适用性。一般而言，分布式数据并行技术在**每个计算资源上复制模型**，独立生成梯度，然后在每次迭代中通信这些梯度以保持模型副本的一致性。尽管这种技术在概念上很简单，但**计算和通信之间的微妙依赖关系使得优化分布式训练效率变得非常复杂**。从v1.5版本开始，PyTorch本地提供了多种技术来加速分布式数据并行，包括**梯度分桶、计算与通信的重叠以及跳过梯度同步**等。评估结果表明，适当配置时，PyTorch分布式数据并行模块在使用256个GPU时实现了近线性的可扩展性.<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度神经网络（DNN）已经为各种应用提供了动力，包括图像识别[20]、语言翻译[15]、异常检测[16]、内容推荐[38]、药物发现[33]、艺术生成[28]、游戏玩法[18]和自动驾驶汽车[13]等。许多应用通过使用更大的模型和更大的数据集来优化以追求更高的智能，这需要分布式训练系统的进一步发展。在现有的解决方案中，分布式数据并行是一种主导策略，因为它对系统的**干扰最小**。本文介绍了PyTorch v1.5中分布式数据并行包的设计、实现和评估。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;训练DNN模型通常需要反复进行三个步骤[26]：前向传播计算损失，反向传播计算梯度，以及优化器步骤更新参数。数据并行性的概念普遍适用于这样的框架。应用程序可以创建多个模型副本，每个模型副本都在一部分训练数据上工作，并**独立执行前向传播和反向传播**。然后，模型副本可以根据算法同步它们的梯度或更新的参数。在应用程序端纯粹构建一个可工作的数据并行版本理论上是可能的，因为它只需要在每次迭代中插入适当的通信。然而，为了挤出最后一丝性能，需要在设计和调优上投入大量的工作。在平台端提供本地的分布式数据并行API可以帮助应用程序开发人员专注于优化他们的模型，而平台开发团队可以持续透明地提高训练速度。提供一个通用的分布式数据并行包面临三个挑战。<br>
- **数学等价性**：数据并行的目的是加速在**大型数据集**上的训练。应用程序期望获得与在没有模型复制的情况下进行本地训练时相同的结果模型。这要求尽管是分布式的，但**在数学上等价于本地训练**。
- **非侵入性和拦截式API**：应用程序开发通常从本地模型开始，然后在需要时扩展。为了避免在过渡期间出现巨大的困难，API必须对应用程序代码**非侵入性**。另一方面，API需要允许内部实现及时拦截信号以进行通信和系统优化。
- **高性能**：数据并行训练受到计算和通信之间微妙依赖关系的影响。设计和实现必须探索解决方案空间，以将更多资源高效地转化为更高的训练吞吐量。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PyTorch提供了分布式数据并行作为一个nn.Module类，应用程序在构造时将它们的模型作为子模块提供。为了保证数学等价性，所有副本从相同的初始模型参数值开始，并在训练迭代中同步梯度以保持参数的一致性。为了最小化对应用程序的干扰，实现暴露了与用户模型相同的前向传播API，允许应用程序无需额外的代码更改，无缝地将用户模型的后续出现替换为分布式数据并行模型对象。设计中集成了几种技术来提供高性能的训练，包括**梯度分桶、计算与通信的重叠以及跳过同步**。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们在一个独立的32-GPU集群上进行了评估，并在一个更大的共享授权(entitlement)中使用了256个GPU。我们开发了基准测试来评估分布式包在不同规模下的性能，以深入了解不同优化技术和配置的性能影响。实验还涵盖了NCCL和Gloo通信库之间的比较。结果表明：<br>
1. **通信是训练延迟(latency)的主要因素**，并且其影响随着模型规模的增大而增加；
2. **桶大小对通信效率有很大影响**，如果适当配置，可以实现超过2倍的加速；
3. **适当跳过同步操作可以显著减少平摊的通信开销，而不明显降低收敛速度。**

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文介绍的技术首次在PyTorch v1.1中发布。在过去的一年中，我们看到这些技术在内部和外部都得到了广泛采用。在Facebook内部，从2020年5月11日到2020年6月5日期间的工作负载研究显示，在各种应用程序中，包括语音、视觉、移动视觉、翻译等，超过60%的 production GPU hours 都用于PyTorch分布式数据并行包。本文有三个主要贡献。首先，本文揭示了一个被广泛采用的工业级分布式训练解决方案的设计和实现。其次，本文强调了以前工作中忽视的实际注意事项（例如，由于复数图形）。第三，我们分享了从为内部团队和开源社区用户提供服务中收集到的性能调优经验，并总结了未来改进的几个方向。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文的剩余部分组织如下。第2节简要介绍了PyTorch和数据并行性。第3节详细介绍了PyTorch分布式数据并行模块的设计。第4节和第5节分别介绍了实现和评估。然后，第6节讨论了所学到的经验和未来改进的机会，第7节调查了相关工作。最后，第8节总结了本文。<br>

# 2 背景
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在深入讨论分布式训练之前，让我们简要讨论使用PyTorch进行本地模型训练的实现和执行方式。然后，我们解释和证明了数据并行的概念，并描述了通信原语。<br>

## 2.1 PyTorch
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PyTorch将值(values)组织成张量（Tensors），它们是具有丰富的数据操作算子的通用n维数组。模块（Module）定义了从输入值到输出值的转换，其在前向传播期间的行为由其forward成员函数指定。一个模块可以包含张量作为参数。例如，线性模块（Linear Module）包含一个权重参数和一个偏差参数，其forward函数通过将输入与权重相乘并添加偏差来生成输出。应用程序通过在自定义的forward函数中连接本地模块（如线性模块、卷积模块等）和函数（如relu、池化等）来组合自己的模块。典型的训练迭代包括前向传播（使用输入和标签生成损失）、反向传播（计算参数的梯度）和优化器步骤（使用梯度更新参数）。更具体地说，在前向传播过程中，PyTorch构建自动求导图来记录执行的操作。然后，在反向传播过程中，它使用自动求导图进行反向传播以生成梯度。最后，优化器将梯度应用于更新参数。训练过程重复这三个步骤，直到模型收敛。<br>

## 2.2 数据并行性
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PyTorch提供了几种工具来促进分布式训练，包括: <br>
- DataParallel用于在**同一台机器上**使用多个GPU进行**单进程多线程**数据并行训练;
- DistributedDataParallel用于在**多个GPU和多台机器上**进行**多进程**数据并行训练;
- 以及RPC [6]用于通用分布式模型并行训练（例如参数服务器 [27]）。
本文重点介绍DistributedDataParallel。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据并行性通过**在优化器步骤之前传递梯度**来实现分布式训练，以确保所有模型副本的参数都**使用完全相同的梯度更新**，从而使模型副本在迭代中保持一致。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**参数平均**是另一种常用的扩展模型(scale-out)训练的技术。类似地，它可以在多台机器上启动多个进程，但与同步梯度不同，参数平均直接计算所有模型参数的平均值。这**发生在本地优化器步骤之后**，这意味着参数平均可以完全作为辅助步骤实现，不需要与本地训练步骤交互，这非常方便和清晰地**解耦了分布式训练和本地迭代的代码**。参数平均也存在一些注意事项。<br>

- 参数平均与本地训练相比，可能会产生截然不同的结果，有时会对模型的准确性产生不利影响。根本原因在于，参数平均在数学上与在本地处理所有输入数据并不等价，特别是当优化器依赖于过去的局部梯度值（例如动量）时。由于不同的模型副本可能看到不同的梯度，参数平均和本地训练相比可能会产生截然不同的结果，有时对模型的准确性有害。其根本原因是，参数平均在数学上与本地处理所有输入数据并不等效，特别是当优化器依赖于过去的局部梯度值（例如动量）时。由于不同的模型副本可能会看到不同的梯度，优化器中的状态可能逐渐发散，导致梯度下降方向冲突。这可能导致在从局部优化的模型切换到大规模部署的模型时，性能出现无法解释的差异。<br>
- 参数平均的结构将计算（即反向传播）和通信（即计算平均值）分为**不重叠**的阶段，使用优化器的**step()函数作为硬分界点**。无论我们如何强化计算或通信的优化，一种类型的资源在任何给定的时间点都会处于空闲状态，放弃了大量的性能优化机会。<br>
*(注释：因为反向传播需要知道weight的值）*

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于上述基本陷阱，我们决定使用数据并行来实现分布式训练，以**同步梯度而不是参数**。请注意，应用程序仍然可以轻松地使用PyTorch构建参数平均。事实上，第3.3节中描述的集合通信功能是这种用例的适当解决方案。应用程序只需要显式地启动AllReduce操作，以相应地计算平均参数。<br>



