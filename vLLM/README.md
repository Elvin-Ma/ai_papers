# Efficient Memory Management for Large Language Model Serving with PagedAttention
- [论文链接](https://arxiv.org/pdf/2309.06180)

# 0 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对大型语言模型（LLMs）进行高吞吐量的服务需要一次性批量处理足够多的请求。然而，现有系统存在困难，因为每个请求的键-值缓存（KV缓存）内存很大并且会动态增长和收缩。当管理效率低下时，这种内存可能会因碎片化和冗余复制而被显著浪费，从而限制批处理大小。为了解决这个问题，我们提出了PagedAttention，这是一种受**传统虚拟内存和分页技术**启发的注意力算法，类似于操作系统中的技术。在此基础上，我们构建了vLLM，一个LLM服务系统，实现了（1）在KV缓存内存中几乎零浪费和（2）在请求内部和跨请求之间灵活共享KV缓存，以进一步减少内存使用。我们的评估显示，与现有的state-of-the-art系统（如FasterTransformer和Orca）相比，vLLM将流行的LLMs的吞吐量提高了2-4倍，并且具有相同水平的延迟。这种改进在序列更长、模型更大和更复杂的解码算法下更加明显。vLLM的源代码可以在https://github.com/vllm-project/vllm 公开获取。<br>

# 1 


# 2 

## 2.2 LLM服务与自回归生成
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一旦训练完成，LLMs通常作为**条件生成服务**而部署（例如，完成API [34]或聊天机器人 [19, 35]）。向LLM服务发送请求时，会提供一组输入提示标记（𝑥1, . . . , 𝑥𝑛），LLM服务根据方程式1生成一组输出标记（𝑥𝑛+1, . . . , 𝑥𝑛+𝑇）。我们将提示和输出列表的串联称为序列。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于方程式1的分解，LLM只能**逐个样本化和生成新的标记**，每个新标记的生成过程取决于该序列中所有先前标记，特别是它们的key和value向量。在这个顺序生成过程中，现有标记的键和值向量通常被缓存以生成未来的标记，称为KV缓存。请注意，**一个标记的KV缓存取决于其所有先前标记**。这意味着出现在序列中不同位置的相同标记的KV缓存将不同。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;考虑到一个请求提示，LLM服务中的生成计算可以分解为两个阶段：<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;提示阶段以整个用户提示（𝑥1, . . . , 𝑥𝑛）作为输入，并计算第一个新标记的概率 𝑃 (𝑥𝑛+1 | 𝑥1, . . . , 𝑥𝑛)。在此过程中，还会生成key 向量 𝑘1, . . . , 𝑘𝑛 和 value 向量 𝑣1, . . . , 𝑣𝑛。由于提示token 𝑥1, . . . , 𝑥𝑛 都是已知的，提示阶段的计算可以使用矩阵-矩阵乘法操作并行化。因此，这个阶段可以有效地利用GPU中固有的并行性。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自回归生成阶段按顺序生成剩余的新token。在第 𝑡 次迭代中，模型将一个token 𝑥𝑛+𝑡 作为输入，并使用key向量 𝑘1, . . . , 𝑘𝑛+𝑡 和value向量 𝑣1, . . . , 𝑣𝑛+𝑡 计算概率 𝑃 (𝑥𝑛+𝑡+1 | 𝑥1, . . . , 𝑥𝑛+𝑡)。请注意，在位置 1 到 𝑛 + 𝑡 - 1 处的键和值向量在前几次迭代中被缓存，此时仅计算新的key 和 value向量 𝑘𝑛+𝑡 和 𝑣𝑛+𝑡。这个阶段会在序列达到最大长度（由用户指定或由LLMs限制）或者发出序列结束符（<eos>）标记时完成。由于数据依赖性，**不同迭代的计算无法并行化**，通常使用矩阵-向量乘法，效率较低。因此，这个阶段严重未充分利用GPU计算，演变成memory-bound，单一请求latency的主因。<br>

## 2.3 LLM的批处理技术
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过对多个请求进行批处理(batching multiple request)，可以提高为LLMs提供服务时的计算利用率。由于这些请求共享相同的模型权重，将权重移动的开销在一个批次中分摊，当批处理大小足够大时，计算开销将压倒权重移动的开销。然而，将请求批处理到LLM服务中并非易事，原因有两点。首先，**请求可能在不同时间到达**。一个简单的批处理策略要么会让较早的请求等待较晚的请求，要么会延迟新到的请求，直到较早的请求完成，从而导致显著的排队延迟。其次，这些请求的输入和输出长度可能相差甚远（见图11）。一个直接的批处理技术会填充请求的输入和输出以使它们的**长度相等，浪费GPU计算和内存**。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为解决这个问题，提出了**细粒度(fine-grained)批处理机制，例如细胞批处理[16]和迭代级别调度[60]**。与传统方法在请求级别工作不同，这些技术在迭代级别操作。在每次迭代后，已完成的请求从批处理中移除，新请求被添加进来。因此，一个新请求可以在等待一个迭代后被处理，而不需要等待整个批次完成。此外，利用特殊的GPU核心，这些技术消除了填充输入和输出的需求。通过减少排队延迟和填充带来的低效率，细粒度批处理机制显著提高了LLM服务的吞吐量。<br>

# 3 在LLM服务中的3个内存挑战
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;尽管细粒度批处理减少了计算浪费，并使请求能够以更灵活的方式进行批处理，但可以批量处理的请求数量仍受限于GPU内存容量，特别是用于存储KV缓存的空间。换句话说，服务系统的吞吐量受到内存限制。要克服这种内存限制，需要解决内存管理中的以下挑战：<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Large KV Cache。随着请求数量的增加，KV Cache的大小迅速增长。以13B参数OPT模型[62]为例，单个token的KV缓存需要800 KB的空间，计算方法为 2(键和值向量）× 5120（隐藏状态大小）× 40（层数）× 2（每个FP16的字节）。由于OPT可以生成长达**2048个token**的序列，存储一个请求的KV Cache所需的内存可能高达1.6GB。并发GPU的存储容量在数十GB。即使将所有可用内存分配给KV Cache，也只能容纳少数**几十个请求**。此外，低效的内存管理还会进一步减小批处理大小，如图2所示。此外，考虑到当前的趋势，**GPU的计算速度增长快于内存容量[17]**。例如，从NVIDIA A100到H100，FLOPS增加了超过2倍，但GPU内存最大保持在80GB。因此，我们认为**内存将成为一个日益显著的瓶颈**。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**复杂的解码算法。** LLM服务提供一系列供用户选择的解码算法，每种算法对内存管理复杂性都有不同的影响。例如，当用户从**单个输入提示中请求多个随机样本时**，这是程序建议中的典型用例[18]，在我们的实验中，该提示部分(prompt part)的KV缓存占总KV Cache内存的**12%（§6.3）**，可以共享以最小化内存使用。另一方面，在自回归生成阶段，由于不同的样本结果及其对上下文和位置的依赖性，KV缓存应保持不共享。KV缓存共享的程度取决于所采用的具体解码算法。在像束搜索[49]这样更复杂的算法中，不同的请求束可以共享更大比例的KV缓存（最多可节省**55%内存**，见§6.3），并且共享模式随着解码过程的推进而演变。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**未知输入和输出长度的调度**。对LLM服务的请求在其输入和输出长度上表现出变化。这要求内存管理系统能够适应各种提示长度。此外，随着请求的输出长度在解码过程中增长，其KV缓存所需的内存也会扩大，可能会耗尽可用于新请求或现有提示的生成的内存。系统需要做出调度决策，例如从GPU内存中删除或交换出某些请求的KV缓存。<br>

## 3.1 现有系统中的内存管理
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于当前深度学习框架中的大多数操作符[33, 39]要求张量在连续内存中存储，先前的LLM服务系统[31, 60]也将一个请求的KV缓存存储为一个跨不同位置的连续张量。由于LLM的输出长度是不可预测的，它们会根据请求的**最大可能序列长度静态分配一块内存**，而不考虑请求的实际输入或最终输出长度。<br>

![figure3](images/figure3.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;图3展示了两个请求：请求A的最大可能序列长度为2048，请求B的最大长度为512。现有系统中的块预分配方案存在三个主要的内存浪费来源：
1.为未来token预留的**槽位(slots)**;
2.由于为潜在的最大序列长度提供过多空间而导致的**内部碎片化**;
3.以及来自内存分配器（如伙伴分配器）的**外部碎片化**。
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;外部碎片永远不会用于生成的token，这在提供请求之前就已知。内部碎片也保持未使用，但这只有在请求完成抽样后才会意识到。它们都是纯粹的内存浪费。尽管保留的内存最终会被使用，但为整个请求的持续时间保留这些空间，特别是当保留的空间很大时，会占据本来可以用来处理其他请求的空间。我们在图2中可视化了我们实验中内存浪费的平均百分比，揭示了**以前系统中实际有效内存可能低至20.4%**。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;尽管压缩[54]已被提出作为解决碎片化问题的潜在方案，但在对性能敏感的LLM服务系统中执行压缩是不切实际的，因为存在大量的KV缓存。即使进行了压缩，每个请求的预分配块空间也会阻止现有内存管理系统中特定于解码算法的内存共享。<br>

# 4 方法
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这项工作中，我们开发了一种**新的注意力算法:PagedAttention**，并构建了一个LLM服务引擎 vLLM，以解决第3节中概述的挑战。vLLM 的架构如图4所示。vLLM 采用了一个**集中式调度器来协调分布式GPU工作节点的执行**。KV缓存管理器以分页方式有效地管理KV缓存，由PagedAttention 实现。具体来说，**KV缓存管理器**通过**集中式调度器发送的指令来管理GPU工作节点上的物理KV缓存内存**。<br>

![figure4](images/figure4.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来，我们在第4.1节中描述PagedAttention算法。随后，在第4.2节中展示KV缓存管理器的设计以及它如何分别在第4.3节中促进PagedAttention。然后，我们展示这种设计如何促进各种解码方法的有效内存管理（第4.4节）并处理可变长度的输入和输出序列（第4.5节）。最后，我们展示了vLLM的系统设计如何在分布式设置中运作（第4.6节）。<br>

## 4.1 PagedAttention
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了解决第3节中的内存挑战，我们引入了PagedAttention，这是一种受传统操作系统中paging[25]概念启发的注意力算法。与传统的注意力算法不同，PagedAttention允许在非连续内存空间中存储连续的键和值。具体来说，PagedAttention将每个序列的KV缓存分成KV block。每个block包含了一定数量的token的key和value向量，我们将其表示为KV块大小（𝐵）。将key block 表示为 $𝐾_{𝑗} = (𝑘_{(𝑗−1)𝐵+1}, . . . , 𝑘_{𝑗𝐵})$ ，value block 表示为 $𝑉_{𝑗} = (𝑣{(𝑗−1)𝐵+1}, . . . , 𝑣_{𝑗𝐵})$ 。方程式4中的注意力计算可以转换为以下block wise 计算：<br>
![formula4](images/formula4.png)
其中, $𝐴_{𝑖𝑗} = (𝑎_{𝑖,(𝑗−1)𝐵+1}, . . . , 𝑎_{𝑖,𝑗𝐵})$ 是第𝑗个KV块上的注意力分数的行向量。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在注意力计算过程中，PagedAttention内核分别**识别和提取不同的KV block**。我们在图5中展示了PagedAttention的一个示例：key和value向量分布在三个块中，这三个块在物理内存上不是连续的。在每个时间点，内核将查询token（“forth”）的查询向量 $𝑞_{𝑖}$ 与一个块中的key向量 $𝐾_{𝑗}$ 进行乘积运算（例如，“Four score and seven”块0的键向量）以计算注意力分数 𝐴𝑖𝑗，然后将 𝐴𝑖𝑗 与一个块中的值向量 𝑉𝑗 进行乘积运算，得出最终的注意力输出 𝑜𝑖。<br>

![figure5](images/figure5.png)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总之，PagedAttention算法允许将KV块存储在非连续的物理内存中，从而在vLLM中实现了更灵活的分页内存管理。<br>

## 4.2 KV缓存管理器
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vLLM内存管理器背后的关键思想类似于操作系统中的虚拟内存[25]。操作系统将内存分成固定大小的页面，并将用户程序的逻辑页面映射到物理页面。**连续的逻辑页面可以对应于非连续的物理内存页面**，使用户程序可以访问内存，就像它是连续的一样。此外，物理内存空间无需提前完全保留，这使得操作系统能够根据需要动态分配物理页面。vLLM利用虚拟内存(virtual memory)背后的思想来管理LLM服务中的KV Cache。借助PagedAttention的支持，我们将KV Cache组织成固定大小的KV block，就像虚拟内存中的pages一样。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;请求的KV Cache被表示为一系列**逻辑KV block**，从左到右填充，随着新的token及其KV Cache的生成。最后一个KV block的未填充位置被保留供将来使用。在GPU工作节点上，块引擎分配一块连续的GPU DRAM，并将其划分为物理KV块（这也适用于CPU RAM进行交换；参见第4.5节）。KV块管理器还维护**block-table** - 每个请求的逻辑和物理KV块之间的映射。每个block table item **记录逻辑块的相应物理块以及填充位置的数量**。分离逻辑和物理KV块使得vLLM能够动态增长KV缓存内存，而无需提前为所有位置保留，这消除了现有系统中大部分的内存浪费，如图2所示。<br>


# 5 参考链接
- [csdn blog](https://blog.csdn.net/yjw123456/article/details/141090361)
- [vllm blog](https://blog.vllm.ai/2023/06/20/vllm.html)
