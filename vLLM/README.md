# Efficient Memory Management for Large Language Model Serving with PagedAttention
- [论文链接](https://arxiv.org/pdf/2309.06180)

# 0 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对大型语言模型（LLMs）进行高吞吐量的服务需要一次性批量处理足够多的请求。然而，现有系统存在困难，因为每个请求的键-值缓存（KV缓存）内存很大并且会动态增长和收缩。当管理效率低下时，这种内存可能会因碎片化和冗余复制而被显著浪费，从而限制批处理大小。为了解决这个问题，我们提出了PagedAttention，这是一种受**传统虚拟内存和分页技术**启发的注意力算法，类似于操作系统中的技术。在此基础上，我们构建了vLLM，一个LLM服务系统，实现了（1）在KV缓存内存中几乎零浪费和（2）在请求内部和跨请求之间灵活共享KV缓存，以进一步减少内存使用。我们的评估显示，与现有的state-of-the-art系统（如FasterTransformer和Orca）相比，vLLM将流行的LLMs的吞吐量提高了2-4倍，并且具有相同水平的延迟。这种改进在序列更长、模型更大和更复杂的解码算法下更加明显。vLLM的源代码可以在https://github.com/vllm-project/vllm 公开获取。<br>

# 1 


# 2 

## 2.2 LLM服务与自回归生成
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一旦训练完成，LLMs通常作为**条件生成服务**而部署（例如，完成API [34]或聊天机器人 [19, 35]）。向LLM服务发送请求时，会提供一组输入提示标记（𝑥1, . . . , 𝑥𝑛），LLM服务根据方程式1生成一组输出标记（𝑥𝑛+1, . . . , 𝑥𝑛+𝑇）。我们将提示和输出列表的串联称为序列。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于方程式1的分解，LLM只能**逐个样本化和生成新的标记**，每个新标记的生成过程取决于该序列中所有先前标记，特别是它们的key和value向量。在这个顺序生成过程中，现有标记的键和值向量通常被缓存以生成未来的标记，称为KV缓存。请注意，**一个标记的KV缓存取决于其所有先前标记**。这意味着出现在序列中不同位置的相同标记的KV缓存将不同。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;考虑到一个请求提示，LLM服务中的生成计算可以分解为两个阶段：<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;提示阶段以整个用户提示（𝑥1, . . . , 𝑥𝑛）作为输入，并计算第一个新标记的概率 𝑃 (𝑥𝑛+1 | 𝑥1, . . . , 𝑥𝑛)。在此过程中，还会生成key 向量 𝑘1, . . . , 𝑘𝑛 和 value 向量 𝑣1, . . . , 𝑣𝑛。由于提示token 𝑥1, . . . , 𝑥𝑛 都是已知的，提示阶段的计算可以使用矩阵-矩阵乘法操作并行化。因此，这个阶段可以有效地利用GPU中固有的并行性。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自回归生成阶段按顺序生成剩余的新token。在第 𝑡 次迭代中，模型将一个token 𝑥𝑛+𝑡 作为输入，并使用key向量 𝑘1, . . . , 𝑘𝑛+𝑡 和value向量 𝑣1, . . . , 𝑣𝑛+𝑡 计算概率 𝑃 (𝑥𝑛+𝑡+1 | 𝑥1, . . . , 𝑥𝑛+𝑡)。请注意，在位置 1 到 𝑛 + 𝑡 - 1 处的键和值向量在前几次迭代中被缓存，此时仅计算新的key 和 value向量 𝑘𝑛+𝑡 和 𝑣𝑛+𝑡。这个阶段会在序列达到最大长度（由用户指定或由LLMs限制）或者发出序列结束符（<eos>）标记时完成。由于数据依赖性，**不同迭代的计算无法并行化**，通常使用矩阵-向量乘法，效率较低。因此，这个阶段严重未充分利用GPU计算，演变成memory-bound，单一请求latency的主因。<br>

## 2.3 LLM的批处理技术
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过对多个请求进行批处理(batching multiple request)，可以提高为LLMs提供服务时的计算利用率。由于这些请求共享相同的模型权重，将权重移动的开销在一个批次中分摊，当批处理大小足够大时，计算开销将压倒权重移动的开销。然而，将请求批处理到LLM服务中并非易事，原因有两点。首先，**请求可能在不同时间到达**。一个简单的批处理策略要么会让较早的请求等待较晚的请求，要么会延迟新到的请求，直到较早的请求完成，从而导致显著的排队延迟。其次，这些请求的输入和输出长度可能相差甚远（见图11）。一个直接的批处理技术会填充请求的输入和输出以使它们的**长度相等，浪费GPU计算和内存**。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为解决这个问题，提出了细粒度(fine-grained)批处理机制，例如细胞批处理[16]和迭代级别调度[60]。与传统方法在请求级别工作不同，这些技术在迭代级别操作。在每次迭代后，已完成的请求从批处理中移除，新请求被添加进来。因此，一个新请求可以在等待一个迭代后被处理，而不需要等待整个批次完成。此外，利用特殊的GPU核心，这些技术消除了填充输入和输出的需求。通过减少排队延迟和填充带来的低效率，细粒度批处理机制显著提高了LLM服务的吞吐量。<br>


# 5 参考链接
- [csdn blog](https://blog.csdn.net/yjw123456/article/details/141090361)
- [vllm blog](https://blog.vllm.ai/2023/06/20/vllm.html)
