# GPipe: 使用micro-batch流水线(Micro-Batch Pipeline)轻松扩展并行性

# 0 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;扩展深度神经网络容量被认为是提高多个不同机器学习任务模型质量的有效方法。在许多情况下，将模型容量增加到单个加速器(GPU)的内存限制之外，需要开发特殊的算法或基础设施。这些解决方案通常是特定于体系结构(Megatron-LM)的，无法转移到其他任务中。为了解决高效且与**任务无关**的模型并行性需求，我们引入了GPipe，一个流水线并行性库(pipeline parallelism library)，允许扩展任何可以表示为层序列(a sequence of layers)的网络。通过在不同的加速器上(diff gpus)进行层子序列的流水线处理，GPipe能够高效地扩展各种不同网络的规模。此外，GPipe利用一种**新颖的批次分割流水线算法(batch splitting pipelining algorithm)**，在模型跨多个加速器进行分割时实现几乎线性加速。我们通过在两个具有不同网络架构的不同任务上训练大规模神经网络来展示GPipe的优势：（一）图像分类：我们训练了一个含有5.57亿参数的AmoebaNet模型，在ImageNet-2012上达到84.4%的top-1准确率；（二）多语言神经机器翻译：我们训练了一个单一的60亿参数、128层的Transformer模型，涵盖100多种语言的语料库，实现了比所有双语模型更好的质量。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在过去的十年中，深度学习取得了巨大的进展，部分原因是由于开发了一些方法，促进了神经网络有效容量的扩展。这一趋势在图像分类领域最为明显，随着模型**容量的增加，ImageNet上的准确率也得到了提高**（图1a）。在自然语言处理领域也可以观察到类似的现象（图1b），简单的浅层句子表示模型(simple shallow  sentence representations models))[1, 2]被更深和更大的对应模型[3, 4]超越。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;尽管更大的模型为多个领域带来了显著的质量提升，但扩展(scaling)神经网络也带来了重大的实际挑战。硬件限制，包括加速器（GPU或TPU）上的内存限制和通信带宽，迫使用户将较大的模型分成不同的分区，并将不同的分区分配给不同的加速器。然而，**设计和实现高效的模型并行算法非常困难**，这往往需要从容量扩展、灵活性（或特定任务和架构的特定性）和训练效率之间做出艰难的选择。因此，大多数高效的模型并行算法都是特定于体系结构和任务的。随着深度学习应用的增多，对可靠且灵活的基础设施的需求不断增加，使研究人员能够轻松地为各种机器学习任务扩展神经网络。<br>

![figure1](images/gpipe-figure1.jpg)

*(图1：(a)近年来代表性的最先进图像分类模型在ImageNet 2012验证数据集[5]上的top-1准确率与模型大小之间存在强相关性[6, 7, 8, 9, 10, 11, 12]。模型容量增加了36倍。红点表示550M参数的AmoebaNet模型的top-1准确率为84.4%。(b)与双语基线相比，在我们的大规模多语言内部语料库上，翻译质量（BLEU）随着模型大小的增加而改善的平均值。每个点T(L, H, A)表示具有L个编码器层和L个解码器层、前馈隐藏维度为H(hidden)和A(attention head)个注意力头的Transformer的性能。红点表示128层、60亿参数的Transformer的性能。)* <br>
*(注释：BLEU代表"Bilingual Evaluation Understudy"，是一种用于评估机器翻译结果质量的指标。BLEU: a Method for Automatic Evaluation of Machine Translation)* <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了解决这些挑战，我们引入了GPipe，这是一个灵活的库，能够高效地训练大规模神经网络。GPipe通过将模型在不同的加速器之间进行分区(partitioning)，并支持在每个加速器上重新材料化(re-materialization)的方式，使任意深度神经网络架构能够超越单个加速器的内存限制[13, 14]。使用GPipe，每个模型可以被指定为一系列的层，并且连续的层组可以被分割成单元(cells)。然后，每个单元(cells)被放置在一个独立的加速器(gpu or tpu)上。基于这种分割设置，我们提出了一种新颖的具有批次分割(batch splitting)的流水线并行算法。我们首先将一个训练样本的小批量(mini-batch)拆分为更小的micro-batch(micro-batches)，然后将每组micro-batch的执行进行流水线处理。我们采用同步(synchronous)小批量梯度下降进行训练，其中**梯度在一个小批量的所有micro-batch中累积**，并在小批量结束时应用。因此，使用GPipe的梯度更新在分区数量上是一致的，使得研究人员可以通过部署更多加速器轻松地训练越来越大的模型。GPipe**还可以与数据并行性结合**，进一步扩大训练规模。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们在图像分类和机器翻译任务上展示了GPipe的灵活性和高效性。对于图像分类任务，我们使用ImageNet 2012数据集上的480×480输入训练了AmoebaNet模型。通过增加模型的宽度，我们将参数数量扩展到了5.57亿，并且获得了84.4%的top-1验证准确率。在机器翻译任务中，我们使用103种语言（102种语言到英语）训练了一个单一的128层、60亿参数的多语言Transformer模型。我们展示了这个模型在100种语言对上胜过了分别训练的3.5亿参数的双语Transformer Big[15]模型。<br>

# 2 GPipe库
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们现在描述GPipe的接口和主要设计特点。这个开源库是在Lingvo [16]框架下实现的。GPipe的核心设计特点通常适用于其他框架[17, 18, 19]，可以在其他框架中实现。<br>

![figure2](images/gpipe-figure2.jpg)

图2: (a) 一个示例的神经网络，具有连续的层，被分割到四个加速器上。 $F_{k}$ 是第k个单元的组合前向计算函数。 $B_{k}$ 是反向传播函数，它依赖于来自上一层的 $B_{k+1}$ 和 $F_{k}$ 。(b) 朴素的模型并行策略由于网络的顺序依赖关系导致严重的低利用率。(c) 流水线并行将输入的mini-batch分割成较小的micro-match，使得不同的加速器可以同时处理不同的micro-batch。梯度在最后同步应用。<br>

## 2.1 接口
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;任何深度神经网络都可以定义为L层的序列。每一层 $L_{i}$ 由一个前向计算函数 $f_{i}$ 和相应的参数集 $w_{i}$ 组成。GPipe还允许用户指定一个可选的计算成本估计函数 $c_{i}$ 。给定分区数K，L层的序列可以被分割成K个复合层，或称为cell。令 $p_{k}$ 包含层i和j之间的连续层。与 $p_{k}$ 对应的参数集等于 $w_{i} 、w_{i+1}、 \dots、w_{j}$ 的并集，它的前向函数为 $F_{k}=f_{j} \circ \ldots \circ f_{i+1} \circ f_{i}$ 。相应的反向传播函数 $B_{k}$ 可以通过自动符号微分从 $F_{k}$ 中计算得到。成本估计器 $C_{k}$ 被设置为 $\Sigma_{l=i}^{j} c_{l}$ 。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GPipe接口非常简单和直观，用户只需要指定以下内容：
1. 模型分区的数量K;
2. micro-batch(micro-batch)的数量M，
3. 定义模型的L层序列和定义。
   
请参考补充材料中的示例。<br>

## 2.2 算法
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户一旦根据模型参数 $w_{i}$ 、前向计算函数 $f_{i}$ 和成本估计函数 $c_{i}$ 定义了网络中的层序列，GPipe将网络分割为K个cell，并将第k个cell放置在第k个加速器上。通信原语(Communication primitives)会**自动**插入到分区边界，以允许相邻分区之间的数据传输(data transfer)。分区算法通过最小化所有cell的估计成本的方差(minimizes the variance)，以便通过在所有分区之间**同步计算时间**来最大化流水线的效率。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在前向传播过程中，GPipe首先将大小为N的每个mini-batch分割成 **M个** 相等的micro-batch，然后通过K个加速器进行流水线处理。在反向传播过程中，根据前向传播过程中使用的相同模型参数，**计算每个micro-batch的梯度**。在每个mini-batch结束时，来自所有M个micro-batch的梯度**被累积并应用于更新所有加速器上的模型参数**。这一系列操作的顺序如图2c所示。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果网络中使用了批归一化（batch normalization）[20]，则在训练过程中，输入的足够统计量会在每个micro-batch和必要时在副本上进行计算[21]。我们还会跟踪**整个mini-batch(mini-batch)上**的足够统计量的移动平均值，以在评估过程(非训练)中使用。<br>


## 2.3 性能优化
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了减少激活内存需求，GPipe支持重新计算(re-materialization)[14]。在前向计算过程中，每个加速器只在分区边界处存储输出激活。在反向传播过程中，第k个加速器重新计算复合前向函数 $F_{k}$ 。因此，峰值激活内存需求降低到 $O(N + \frac{L}{K} \times \frac{N}{M})$ ，其中 $\frac{N}{M}$ 是micro-batch的大小， $\frac{L}{K}$ 是每个分区的层数。相比之下，如果不进行重新计算和分区，内存需求将为 $O(N \times L)$ ，因为计算梯度 $b_{i}$ 需要上一层的梯度 $b_{i+1}$ 和缓存的激活 $f_{i}(x)$ 。 <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如图2c所示，分区会引入一些加速器的空闲时间，我们称之为冒泡开销(bubble overhead)。这个冒泡时间在微步数M(mini-batch 被分割为M个 mocro-batch)上的平摊是 $O(\frac{K-1}{M + K -1}$ 。在我们的实验中，当 M ≥ 4 × K (K 个 加速器)时，我们发现冒泡开销可以忽略不计。部分原因是因为在反向传播过程中，重新计算可以提前调度，而无需等待较早层的梯度。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于在加速器之间**只需要在分区边界传递激活张量**，所以GPipe引入了低通信开销。因此，即使在没有高速互连的加速器上，我们也可以实现高效的扩展性能。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;图2c假设分区是均衡的。然而，不同层的内存需求和计算量通常存在**很大的不平衡**。在这种情况下，不完美的分区算法可能会导致负载不平衡。更好的分区算法有可能改善我们的启发式方法的性能。<br>

# 3 性能分析
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们使用两种非常不同类型的模型架构来评估GPipe的性能：AmoebaNet [12] 卷积模型和Transformer [15] 序列到序列模型。我们进行了实验来研究它们的可扩展性、效率和通信成本。<br>

![table1](images/gpipe-table1.jpg)

*(表格1：在不同场景下，GPipe支持的AmoebaNet的最大模型大小。Naive-1是指不使用GPipe的顺序版本。Pipeline-k表示在k个加速器上使用GPipe进行k个分区。AmoebaNet-D (L, D)：具有L个普通单元层和过滤器大小D的AmoebaNet模型。Transformer-L：具有L层，2048个模型和8192个隐藏维度的Transformer模型。每个模型参数需要12个字节，因为我们在训练过程中应用了RMSProp算法。)*

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们预计重计算和流水线并行性都将有利于内存利用率，从而使得适应巨型模型成为可能。我们在表格1中报告了在相当大的输入尺寸下GPipe能够支持的最大模型大小。对于AmoebaNet，我们在每个加速器具有8GB内存的Cloud TPUv2上运行了实验。我们使用了固定的输入图像大小为224×224和mini-batch大小为128。在没有GPipe的情况下，单个加速器可以训练高达8200万参数的AmoebaNet，受设备内存限制的约束。由于反向传播中的重新计算和批次分割，GPipe将中间激活的内存需求从6.26GB降低到3.46GB，使得**单个加速器上能够支持31800万参数**的模型。通过模型并行性，我们能够将AmoebaNet扩展到了**18亿参数，在8个加速器上进行训练**， 比没有GPipe时高出25倍。在这种情况下，由于AmoebaNet中模型参数在不同层之间分布不均衡，最大模型大小**并没有完全线性扩展**。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来，我们使用每个加速器核心具有**16GB**内存的Cloud TPUv3训练了Transformer模型。我们使用了固定的词汇大小为32k，序列长度为1024，批次大小为32。每个Transformer层的模型维度为2048，前向隐藏维度为8192，注意力头数为32。我们通过改变层数来扩展模型。重计算使得在单个加速器上可以训练一个大约**2.7倍**的更大模型。使用128个分区，GPipe使得Transformer模型可以扩展到83.9B参数，比单个加速器上的可能性增加了**298倍**。与AmoebaNet不同，对于Transformer，最大模型大小与加速器数量**呈线性扩展**，因为每个层具有相同数量的参数和输入大小。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了评估效率，我们在表格2中报告了使用不同分区数量和不同micro-batch数量的GPipe对AmoebaNet-D（18, 256）和Transformer-48进行的训练吞吐量的归一化结果。每个分区分配给一个独立的加速器。我们观察到，当micro-batch数量M至少是分区数量的**4倍**时，冒泡开销几乎可以忽略不计。对于Transformer模型，当它在分区数量的四倍加速器上进行分区时，可以实现3.5倍的加速。此外，由于Transformer层的计算均匀分布，训练吞吐量几乎与设备数量呈线性扩展。相反，由于AmoebaNet模型的计算分布不平衡，它的加速比是亚线性的。当M相对较小时，冒泡开销就不再可以忽略了。当**M为1时，实际上没有流水线并行性**，我们观察到，M = 1 时，无论使用多少个加速器，吞吐量都保持相对稳定，这表明任何给定时间只有一个设备在进行计算。<br>

![table2](images/gpipe-table2.jpg)

*(表格2：加速倍数与参数的关系表格，使用不同分区数量K和不同micro-batch数量M的GPipe在TPU上的归一化训练吞吐量。随着更多的micro-batch，性能提高。当M ≥ K时，Transformer模型的加速比几乎与加速器数量呈线性关系。如果需要，批次大小会进行调整以适应内存。)*

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了衡量GPipe的通信开销影响，我们在一台具有多个NVIDIA P100 GPU但没有NVLink的主机上运行了实验。跨GPU的数据传输必须通过PCI-E进行相对较慢的设备到主机和主机到设备传输。micro-batch数量M固定为32。如表3所示，当我们将分区数量从2增加到8时，我们观察到AmoebaNet-D（18, 128）的加速比为2.7倍。对于24层Transformer，加速比为3.3倍。与我们在配备高速互连的TPU上观察到的类似，存在类似的线性加速。由于GPipe仅在分区边界传输激活张量，设备之间的通信带宽不再成为模型并行性的瓶颈。<br>

![table3](images/gpipe-table3.jpg)

# 4 图像分类
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;作为概念验证，我们首先使用GPipe对AmoebaNet进行了扩展。我们增加了AmoebaNet中的通道数，并将输入图像大小扩展到480×480。我们使用与[12]中描述的相同超参数，在ImageNet 2012数据集上训练了这个拥有5.57亿参数的AmoebaNet-B(18, 512)。网络被分为4个分区。这个单一模型在单图裁剪的情况下达到了84.4%的top-1和97%的top-5验证准确率<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们进一步通过迁移学习[22, 23]在其他图像数据集上展示了巨型卷积网络的有效性。具体而言，我们使用预训练的ImageNet模型，在从通用到细粒度分类的各种目标数据集上进行微调。我们将最后一个softmax分类层的输出单元数更改为目标数据集中的类别数，并随机初始化新的softmax层。所有其他层都使用ImageNet预训练的初始化。在训练期间，网络的输入图像被调整为480×480，随机水平翻转，并使用cutout[24]进行数据增强。训练的超参数与ImageNet相同（我们在补充材料中提供了我们的训练设置的详细描述）。在表4中，我们报告了每个数据集进行了5次微调运行的平均单图裁剪测试准确率。我们的巨型模型在所有目标数据集上获得了有竞争力的结果。例如，CIFAR-10的错误率降低到了1%，CIFAR-100的错误率降低到了8.7%。这些结果证实了Kornblith等人的发现[25]，即更好的ImageNet模型具有更好的迁移性能。<br>

![table4](images/gpipe-table4.jpg)

*(表格4：使用在ImageNet 2012上首先训练，然后在其他数据集上进行微调的AmoebaNet-B (18, 512)的图像分类准确率。有关我们训练设置的详细描述，请参阅补充材料。我们的微调结果是在5次微调运行中进行平均的。来自Real等人[12]和Cubuk等人[26]的基准结果是直接从头开始训练的。*Mahajan等人的模型[27]在非公开的Instagram数据上进行了预训练，达到了85.4%的top-1准确率。Ngiam等人[28]通过使用私有数据集（JFT-300M）进行预训练获得了更好的结果。)*

# 5 大规模多语言机器翻译
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来，我们通过扩展自然语言处理（NLP）中使用的模型来展示GPipe的灵活性。由于有大量可用的平行语料库(corpora)，神经机器翻译（NMT）已成为用于NLP的任何架构的基准任务[33, 15, 34, 35, 36]。因此，我们在一个大规模的多语言NMT任务上继续进行GPipe实验。我们使用包含102种语言和英语的平行文档语料库，总共包含250亿个训练示例，每种语言的范围从10^4到10^9 [37]。该数据集通过跨越从数据稀缺（低资源）到数据丰富（高资源）的多种语言，为可扩展性实验提供了一个真实的测试基础。我们首次展示了足够大的NMT模型可以**同时学习100多种语言对之间的映射**，并且在所有语言上的性能都优于双语模型。这进一步凸显了拥有高效灵活的模型并行工具的重要性。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们的比较基于在该语料库中训练的单个Transformer [15]的性能。我们通过两个维度来扩展架构，以展示GPipe的灵活性：（i）沿深度增加模型中的层数，（ii）沿宽度增加前馈层中的隐藏维度和多头注意力层中的注意力头数（以及注意力通道数），类似于Shazeer等人[34]的方法。有关我们的数据集、基准线、训练配置和优化超参数的详细描述，请参阅补充材料。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们从一个标准的400M参数的Transformer Big模型T(6, 8192, 16)1开始，如Chen等人[35]所述，词汇表大小为64k。在图3中，我们将其性能与一个13亿参数的深层模型T(24, 8192, 16)、一个13亿参数的宽模型T(12, 16384, 32)、一个30亿参数的模型T(32, 16384, 32)和一个60亿参数的模型T(64, 16384, 32)进行了比较。所有模型都同时在所有语言对上进行训练，使用了多语言BERT2[3]中采用的基于温度的采样方法。T(12, 16384, 32)、T(24, 8192, 32)、T(32, 16384, 32)和T(64, 16384, 32)分别在2、4、8和16个加速器上进行分区。<br>
*(注释：1T(L, H, A) --> Transformer model with L encoder layers and L decoder layers, hidden dimension of H and A attention heads. The model dimension is fixed to 1024.)* <br>
*(https://github.com/google-research/bert/blob/master/multilingual.md)* <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从图3中，我们可以观察到将模型容量从4亿参数增加到13亿参数会显著提高所有语言的性能。将模型从13亿参数扩展到60亿参数显示出进一步的改进，特别是对于高资源语言，尽管当将模型从13亿参数扩展到30亿和60亿参数时，**递减收益也可观察到**。接下来，我们将根据这些大规模实验的结果讨论一些我们的经验发现。<br>

![figure3](images/gpipe-figure3.jpg)

*(图3：随着多语种模型容量的增加，跨所有语言的翻译质量变化。语言按照从左到右训练数据集大小递减的顺序排列。T(L, H, A)表示具有L个编码器层和L个解码器层，隐藏层维度为H，注意力头数为A的Transformer模型的性能。我们注意到，增加模型容量，从400M参数（T(6, 8192, 16)）到1.3B参数（T(24, 8192, 16)），再到6B参数（T(64, 16384, 32)），在所有语言上都显著提高了质量。当与双语基准线进行比较时，我们还注意到对于低资源语言（图表右侧），质量改进非常大，突显了训练多语种模型所带来的显著迁移收益。)* <br>
*(低资源语言（Low-resource languages）是指具有非常有限的相关资源（例如数据、语料库、工具和研究人员）的语言)* <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度与宽度的权衡：我们在我们的多语种设置中研究了深度与宽度之间的权衡，并比较了1.3B宽模型T(12, 16384, 32)和1.3B深模型T(24, 8192, 16)的性能。虽然这两个模型在高资源语言（图3左侧）上的质量非常相似，但**深度更深的模型在低资源语言上取得了巨大的优势**，表明**增加模型深度可能更有利于泛化能力**。此外，与400M模型相比，对于低资源语言（图3右侧），1.3B深模型的质量改进几乎与高资源语言的改进程度相当大，这表明**增加深度可能潜在地增加到低资源任务的迁移程度**。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度模型的训练难题：虽然深度增加了神经网络的表示(representational)能力，但也增加了优化问题的复杂性。在我们的大规模(large-scale)实验中，我们遇到了严重的训练难题，这是由于尖锐的激活（正峰度）和数据噪声的组合引起的。我们观察到，在经过几千个步骤的训练后，模型的预测会变得极度尖锐，并对噪声非常敏感，这经常导致非有限(无限)或大梯度，最终破坏了学习进展。
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了解决这些问题，我们采用了两种方法：<br>
1. 根据Zhang等人的研究[38]，我们将所有Transformer前馈层的参数初始化缩小，缩小模型的层数倍。
2. 当逻辑预测（softmax预激活）的幅度超过一定值时，我们对其进行**剪裁**。这两种方法的组合使我们能够减轻由模型深度缩放引起的训练不稳定性。

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大批量训练：由于其简单性，数据并行是扩展神经网络训练的主要方法[39, 40]。我们通过显著增加用于标准Transformer Big训练的批量大小来测试大批量训练的极限。从每批260K个标记开始，我们将有效批量大小增加到4M，并观察高资源语言对德英（其他语言对也可以观察到类似趋势）的验证损失和BLEU分数。此处使用的优化参数与之前的实验相同。据我们所知，每批4M个标记是迄今为止用于训练NMT模型的文献中使用的最大批量大小[41]。表5显示，随着批量大小的增加，这两个指标都显著改善。我们相信进一步增加批量大小可能会带来更多的改进。<br>

![table5](images/gpipe-table5.jpg)

# 6个设计特点和权衡
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;已经提出了几种方法来实现高效的大规模模型并行。然而，每种方法都选择了自己的权衡(trade-off)，使其适用于在特定硬件约束下扩展特定架构。在这里，我们强调了几种模型并行方法涉及的各种设计选择和权衡，并以GPipe为基准，比较了在不同硬件约束和架构变体下的灵活性、可扩展性和效率。模型并行的核心思想是将网络划分为不同的计算单元，然后将它们放置在不同的设备上[42, 43, 44, 45]。从概念上讲，这支持将广泛的模型扩展到巨大的容量。然而，这些方法通常存在硬件利用率低和设备通信瓶颈(communication bottlenecks.)的问题。**单程序多数据（SPMD）** 和**流水线并行性**已被提出作为应对这些挑战的解决方案。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Mesh-Tensorflow [34]遵循SPMD范例，将用于数据并行的单指令多数据（SIMD）方法**扩展到其他张量维度**。SPMD允许将每个计算分布在多个设备上，使用户能够将单个矩阵乘法的规模（单个层的模型参数）与加速器的数量成**线性比例扩展**。然而，这也引入了加速器之间高通信开销的问题，原因是需要使用类似AllReduce的操作来合并每个并行化矩阵乘法的输出。该方法仅适用于加速器之间连接速度较高的情景。此外，SPMD限制了可以有效扩展的操作类型，将其使用限制在特定的网络架构和机器学习任务集上。例如，在这种范例下，沿卷积层的通道维度进行划分是不高效的，因为通道实际上是完全连接的，而沿空间维度的划分则需要复杂的技术来处理边界区域。尽管SPMD允许通过使每个操作更小来扩展模型的深度，但它要求将每层分割到更多的加速器上，从而进一步增加了设备间的通信开销。<br>
*(注释：SPMD（Single Program Multiple Data）：SPMD是一种并行计算范式，它允许多个处理单元（如处理器、加速器等）同时执行相同的程序，但每个处理单元处理的数据可以不同。每个处理单元可以有自己的控制流和数据，但它们执行的是相同的指令序列。SPMD适用于在不同的数据集上执行相似的计算任务，允许每个处理单元独立地处理自己的数据)* <br>
*(注释：SIMD（Single Instruction Multiple Data）：SIMD是一种并行计算范式，它允许一条指令同时作用于多个数据元素。在SIMD中，多个处理单元同时执行相同的指令，但每个处理单元处理的数据不同。每个处理单元执行的是相同的操作，但是操作的输入数据可以是不同的。SIMD适用于在大规模数据集上执行相同的计算操作，允许同时对多个数据元素执行相同的操作。)* <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其他方法尝试利用基于流水线并行的方法来扩展神经网络[46, 47]。应用于神经网络训练的最新流水线并行方法是**PipeDream [48]** ，其目标是减少参数服务器[49]的通信开销。PipeDream将前向传播的执行流程进行流水线化，并将其**与反向传播交替进行**，以最大程度地提高硬件利用率。然而，这种设计存在由**异步反向更新引入的权重陈旧性**。为避免由权重陈旧性引起的优化问题，PipeDream要求在**每个加速器上维护多个版本化的模型参数副本**，以便准确计算梯度更新，这限制了用户扩展到更大模型的能力。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GPipe引入了一种新型的流水线并行性，它**在应用单个同步梯度更新之前对micro-batch的执行进行流水线化**。我们的**批次分割流水线并行算法**结合了重新材料化(re-materialization)技术，可以扩展到大量的micro-batch。这样可以最小化冗余开销，而**无需进行异步梯度更新**。GPipe使用户能够将模型大小与所使用的加速器数量线性扩展。**与SPMD不同**，当扩展模型时，流水线并行仅引入**很少的额外通信开销**。设备间的通信仅在每个micro-batch的分区边界处发生，并且引入的通信开销很小，使得GPipe在无法使用高速设备互连的情况下仍然有用。然而，**目前的GPipe假设单个层适应单个加速器的内存要求(Megatron-LM 每这个要求)**。此外，micro-batch分割需要复杂的策略来支持需要**跨批次进行计算的层**（例如，**BatchNorm在训练过程中使用micro-batch（micro-batch）的统计信息，但在评估时累积mini-batch（mini-batch）的统计信息**）。

# 7 结论
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在本工作中，我们介绍了GPipe，一个用于训练大规模神经网络的可扩展模型并行库。我们提出并实现了一种新颖的**批次分割流水线并行算法**，该算法使用**同步梯度更新**，实现了高硬件利用率和训练稳定性下的模型并行。我们利用GPipe来训练大规模的卷积和基于Transformer的模型，并在图像分类和多语言机器翻译任务上展示了强大的实证结果。我们强调了GPipe库的三个关键特点：<br>
1. 效率：通过使用新颖的批次分割流水线化算法，GPipe在设备数量增加时几乎实现了线性加速。
2. 灵活性：GPipe支持任何可以表示为层序列的深层网络结构。
3. 可靠性：GPipe利用同步梯度下降，并保证在任意分区数量下的一致训练。








