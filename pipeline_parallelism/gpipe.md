# GPipe: 使用微批次流水线(Micro-Batch Pipeline)轻松扩展并行性

# 0 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;扩展深度神经网络容量被认为是提高多个不同机器学习任务模型质量的有效方法。在许多情况下，将模型容量增加到单个加速器(GPU)的内存限制之外，需要开发特殊的算法或基础设施。这些解决方案通常是特定于体系结构(Megatron-LM)的，无法转移到其他任务中。为了解决高效且与**任务无关**的模型并行性需求，我们引入了GPipe，一个流水线并行性库(pipeline parallelism library)，允许扩展任何可以表示为层序列(a sequence of layers)的网络。通过在不同的加速器上(diff gpus)进行层子序列的流水线处理，GPipe能够高效地扩展各种不同网络的规模。此外，GPipe利用一种**新颖的批次分割流水线算法**，在模型跨多个加速器进行分割时实现几乎线性加速。我们通过在两个具有不同网络架构的不同任务上训练大规模神经网络来展示GPipe的优势：（一）图像分类：我们训练了一个含有5.57亿参数的AmoebaNet模型，在ImageNet-2012上达到84.4%的top-1准确率；（二）多语言神经机器翻译：我们训练了一个单一的60亿参数、128层的Transformer模型，涵盖100多种语言的语料库，实现了比所有双语模型更好的质量。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在过去的十年中，深度学习取得了巨大的进展，部分原因是由于开发了一些方法，促进了神经网络有效容量的扩展。这一趋势在图像分类领域最为明显，随着模型**容量的增加，ImageNet上的准确率也得到了提高**（图1a）。在自然语言处理领域也可以观察到类似的现象（图1b），简单的浅层句子表示模型(simple shallow  sentence representations models))[1, 2]被更深和更大的对应模型[3, 4]超越。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;尽管更大的模型为多个领域带来了显著的质量提升，但扩展(scaling)神经网络也带来了重大的实际挑战。硬件限制，包括加速器（GPU或TPU）上的内存限制和通信带宽，迫使用户将较大的模型分成不同的分区，并将不同的分区分配给不同的加速器。然而，**设计和实现高效的模型并行算法非常困难**，这往往需要从容量扩展、灵活性（或特定任务和架构的特定性）和训练效率之间做出艰难的选择。因此，大多数高效的模型并行算法都是特定于体系结构和任务的。随着深度学习应用的增多，对可靠且灵活的基础设施的需求不断增加，使研究人员能够轻松地为各种机器学习任务扩展神经网络。<br>

![figure1](images/gpipe-figure1.jpg)

*(图1：(a)近年来代表性的最先进图像分类模型在ImageNet 2012验证数据集[5]上的top-1准确率与模型大小之间存在强相关性[6, 7, 8, 9, 10, 11, 12]。模型容量增加了36倍。红点表示550M参数的AmoebaNet模型的top-1准确率为84.4%。(b)与双语基线相比，在我们的大规模多语言内部语料库上，翻译质量（BLEU）随着模型大小的增加而改善的平均值。每个点T(L, H, A)表示具有L个编码器层和L个解码器层、前馈隐藏维度为H(hidden)和A(attention head)个注意力头的Transformer的性能。红点表示128层、60亿参数的Transformer的性能。)* <br>
*(注释：BLEU代表"Bilingual Evaluation Understudy"，是一种用于评估机器翻译结果质量的指标。BLEU: a Method for Automatic Evaluation of Machine Translation)* <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了解决这些挑战，我们引入了GPipe，这是一个灵活的库，能够高效地训练大规模神经网络。GPipe通过将模型在不同的加速器之间进行分区(partitioning)，并支持在每个加速器上重新材料化((re-materialization))的方式，使任意深度神经网络架构能够超越单个加速器的内存限制[13, 14]。使用GPipe，每个模型可以被指定为一系列的层，并且连续的层组可以被分割成单元(cells)。然后，每个单元(cells)被放置在一个独立的加速器(gpu or tpu)上。基于这种分割设置，我们提出了一种新颖的具有批次分割(batch splitting)的流水线并行算法。我们首先将一个训练样本的小批量(mini-batch)拆分为更小的微批次(micro-batches)，然后将每组微批次的执行进行流水线处理。我们采用同步(synchronous)小批量梯度下降进行训练，其中**梯度在一个小批量的所有微批次中累积**，并在小批量结束时应用。因此，使用GPipe的梯度更新在分区数量上是一致的，使得研究人员可以通过部署更多加速器轻松地训练越来越大的模型。GPipe**还可以与数据并行性结合**，进一步扩大训练规模。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们在图像分类和机器翻译任务上展示了GPipe的灵活性和高效性。对于图像分类任务，我们使用ImageNet 2012数据集上的480×480输入训练了AmoebaNet模型。通过增加模型的宽度，我们将参数数量扩展到了5.57亿，并且获得了84.4%的top-1验证准确率。在机器翻译任务中，我们使用103种语言（102种语言到英语）训练了一个单一的128层、60亿参数的多语言Transformer模型。我们展示了这个模型在100种语言对上胜过了分别训练的3.5亿参数的双语Transformer Big[15]模型。<br>

# 2 GPipe库
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们现在描述GPipe的接口和主要设计特点。这个开源库是在Lingvo [16]框架下实现的。GPipe的核心设计特点通常适用于其他框架[17, 18, 19]，可以在其他框架中实现。<br>

![figure2](images/gpipe-figure2.jpg)

图2: (a) 一个示例的神经网络，具有连续的层，被分割到四个加速器上。 $F_{k}$ 是第k个单元的组合前向计算函数。 $B_{k}$ 是反向传播函数，它依赖于来自上一层的 $B_{k+1}$ 和 $F_{k}$ 。(b) 朴素的模型并行策略由于网络的顺序依赖关系导致严重的低利用率。(c) 流水线并行将输入的小批次分割成较小的微批次，使得不同的加速器可以同时处理不同的微批次。梯度在最后同步应用。<br>

## 2.1 接口
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;任何深度神经网络都可以定义为L层的序列。每一层 $L_{i}$ 由一个前向计算函数 $f_{i}$ 和相应的参数集 $w_{i}$ 组成。GPipe还允许用户指定一个可选的计算成本估计函数 $c_{i}$ 。给定分区数K，L层的序列可以被分割成K个复合层，或称为cell。令 $p_{k}$ 包含层i和j之间的连续层。与 $p_{k}$ 对应的参数集等于 $w_{i} 、w_{i+1}、 \dots、w_{j}$ 的并集，它的前向函数为 $F_{k}=f_{j} \circ \ldots \circ f_{i+1} \circ f_{i}$ 。相应的反向传播函数 $B_{k}$ 可以通过自动符号微分从 $F_{k}$ 中计算得到。成本估计器 $C_{k}$ 被设置为 $\Sigma_{l=i}^{j} c_{l}$ 。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GPipe接口非常简单和直观，用户只需要指定以下内容：(i) 模型分区的数量K，(ii) 微批次(micro-batch)的数量M，以及 (iii) 定义模型的L层序列和定义。请参考补充材料中的示例。<br>

## 2.2 算法
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户一旦根据模型参数 $w_{i}$ 、前向计算函数 $f_{i}$ 和成本估计函数 $c_{i}$ 定义了网络中的层序列，GPipe将网络分割为K个cell，并将第k个cell放置在第k个加速器上。通信原语(Communication primitives)会自动插入到分区边界，以允许相邻分区之间的数据传输(data transfer)。分区算法通过最小化所有cell的估计成本的方差(minimizes the variance)，以便通过在所有分区之间**同步计算时间**来最大化流水线的效率。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在前向传播过程中，GPipe首先将大小为N的每个小批次分割成 **M个** 相等的微批次，然后通过K个加速器进行流水线处理。在反向传播过程中，根据前向传播过程中使用的相同模型参数，**计算每个微批次的梯度**。在每个小批次结束时，来自所有M个微批次的梯度**被累积并应用于更新所有加速器上的模型参数**。这一系列操作的顺序如图2c所示。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果网络中使用了批归一化（batch normalization）[20]，则在训练过程中，输入的足够统计量会在每个微批次和必要时在副本上进行计算[21]。我们还会跟踪**整个小批次(mini-batch)上**的足够统计量的移动平均值，以在评估过程(非训练)中使用。<br>









