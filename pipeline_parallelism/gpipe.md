# GPipe: 使用微批次流水线(Micro-Batch Pipeline)轻松扩展并行性

# 0 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;扩展深度神经网络容量被认为是提高多个不同机器学习任务模型质量的有效方法。在许多情况下，将模型容量增加到单个加速器的内存限制之外，需要开发特殊的算法或基础设施。这些解决方案通常是特定于体系结构的，无法转移到其他任务中。为了解决高效且与任务无关的模型并行性需求，我们引入了GPipe，一个流水线并行性库，允许扩展任何可以表示为层序列的网络。通过在不同的加速器上进行层子序列的流水线处理，GPipe能够高效地扩展各种不同网络的规模。此外，GPipe利用一种新颖的批次分割流水线算法，在模型跨多个加速器进行分割时实现几乎线性加速。我们通过在两个具有不同网络架构的不同任务上训练大规模神经网络来展示GPipe的优势：（一）图像分类：我们训练了一个含有5.57亿参数的AmoebaNet模型，在ImageNet-2012上达到84.4%的top-1准确率；（二）多语言神经机器翻译：我们训练了一个单一的60亿参数、128层的Transformer模型，涵盖100多种语言的语料库，实现了比所有双语模型更好的质量。
