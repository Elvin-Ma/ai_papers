# PipeDream: Fast and Efficient Pipeline Parallel DNN Training
- [paper 论文](https://arxiv.org/pdf/1806.03377)

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PipeDream是一种用于GPU的深度神经网络（DNN）训练系统，通过在多台机器上进行流水线执行来并行计算。其流水线并行计算模型**避免了**数据并行训练中面临的由于大型模型和/或有限的网络带宽引起的**高通信与计算比例而导致的减速问题**。**相对于数据并行训练，PipeDream相对于大型DNNs减少了高达95%的通信，并允许完全重叠的通信和计算**。PipeDream通过系统地**将DNN层分配给所有可用的GPU**，以平衡工作并最小化通信，为向后传递的正确性版本化模型参数，并以循环方式安排不同输入的正向和反向传递，以优化“达到目标准确性所需的时间”。在两个不同的集群上对五个不同的DNN进行的实验表明，与数据并行训练相比，**PipeDream在时间达到准确性方面最多快5倍**。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;过去五年中，深度神经网络（DNN）的使用迅速增加，研究人员和实践者在广泛的应用领域中成功地应用这些模型，包括图像和视频分类、语音识别和语言翻译 [16, 17, 21, 22, 44]。随着DNN的广泛发展和使用，模型的规模也在增加，以增加其效果——现在的模型通常有数十到数百个层，总共约有1000万到2000万个参数。这样的增长不仅加剧了已经耗时且资源密集的DNN训练过程的压力，还导致了常用的并行化方法失效。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最常见的方法是数据并行化，即将DNN模型复制到多个工作机器上，每个工作机器处理**训练数据**的一个子集。在各个工作机器上计算的权重更新(grads)被聚合起来(reduce)，以获得反映所有输入更新的最终权重更新。每次聚合传输的数据量与模型的大小成正比(传输的是weight的grad)。虽然**数据并行训练在**某些具有**高计算与通信比率（计算量/通信量）的流行模型上效果很好**，但两个重要趋势威胁了其效果。首先，不断增长的模型规模增加了每次聚合通信的量。事实上，**有些广泛使用的模型已经足够大，以至于通信开销已经超过了计算时间，限制了规模扩展并主导了总体训练时间**（例如，对于VGG-16，高达训练时间的85%）[36]。**其次，GPU计算能力的快速增长进一步将训练的瓶颈转移到了模型间的通信上**。我们的结果在三代NVIDIA GPU（Kepler、Pascal和Volta）以及五种不同的DNN模型上定量地展示了这些效果（图1）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另一种分布式训练的方法是**模型并行化**，传统上用于那些在训练过程中无法完全放入工作机器的内存或缓存的大型模型 [10, 25, 7]。模型并行训练涉及**将模型在工作机器之间进行分割**，使得**每个工作机器仅评估和更新模型的一部分参数**。然而，尽管模型并行化使得大型模型的训练成为可能，传统的模型并行化可能导致计算资源的严重浪费，因为它要么一次只能使用一个工作机器（如果每个层分配给一个工作机器），要么无法重叠计算和通信（如果每个层被分割）。此外，即使对于经验丰富的机器学习实践者来说，确定**如何最好地将DNN模型在工作机器之间分割也是一项具有挑战性的任务** [30]，通常会导致额外的低效。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文介绍了PipeDream，一种专门用于DNN的新型分布式训练系统。与模型并行化类似，它将DNN进行分割，并将各个层的**子集**分配给每个工作机器。但是，与传统的模型并行化不同，PipeDream采用了**积极的小批量处理流水线技术**，不同的工作机器在任何时间点处理不同的输入。这是通过将多个输入注入到具有第一个DNN层的工作机器中来实现的，从而保持流水线充满，并确保所有工作机器上的并发处理。它还对选定的层子集使用数据并行化以在工作机器之间平衡计算负载。我们**将这种流水线、模型并行化和数据并行化的组合称为流水线并行训练**。<br>





