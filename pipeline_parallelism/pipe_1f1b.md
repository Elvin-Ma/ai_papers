# PipeDream: Fast and Efficient Pipeline Parallel DNN Training
- [paper 论文](https://arxiv.org/pdf/1806.03377)

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PipeDream是一种用于GPU的深度神经网络（DNN）训练系统，通过在多台机器上进行流水线执行来并行计算。其流水线并行计算模型**避免了**数据并行训练中面临的由于大型模型和/或有限的网络带宽引起的**高通信与计算比例而导致的减速问题**。**相对于数据并行训练，PipeDream相对于大型DNNs减少了高达95%的通信，并允许完全重叠的通信和计算**。PipeDream通过系统地**将DNN层分配给所有可用的GPU**，以平衡工作并最小化通信，为向后传递的正确性版本化模型参数，并以循环方式安排不同输入的正向和反向传递，以优化“达到目标准确性所需的时间”。在两个不同的集群上对五个不同的DNN进行的实验表明，与数据并行训练相比，**PipeDream在时间达到准确性方面最多快5倍**。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;过去五年中，深度神经网络（DNN）的使用迅速增加，研究人员和实践者在广泛的应用领域中成功地应用这些模型，包括图像和视频分类、语音识别和语言翻译 [16, 17, 21, 22, 44]。随着DNN的广泛发展和使用，模型的规模也在增加，以增加其效果——现在的模型通常有数十到数百个层，总共约有1000万到2000万个参数。这样的增长不仅加剧了已经耗时且资源密集的DNN训练过程的压力，还导致了常用的并行化方法失效。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最常见的方法是数据并行化，即将DNN模型复制到多个工作机器上，每个工作机器处理**训练数据**的一个子集。在各个工作机器上计算的权重更新(grads)被聚合起来(reduce)，以获得反映所有输入更新的最终权重更新。每次聚合传输的数据量与模型的大小成正比(传输的是weight的grad)。虽然**数据并行训练在**某些具有**高计算与通信比率（计算量/通信量）的流行模型上效果很好**，但两个重要趋势威胁了其效果。首先，不断增长的模型规模增加了每次聚合通信的量。事实上，**有些广泛使用的模型已经足够大，以至于通信开销已经超过了计算时间，限制了规模扩展并主导了总体训练时间**（例如，对于VGG-16，高达训练时间的85%）[36]。**其次，GPU计算能力的快速增长进一步将训练的瓶颈转移到了模型间的通信上**。我们的结果在三代NVIDIA GPU（Kepler、Pascal和Volta）以及五种不同的DNN模型上定量地展示了这些效果（图1）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另一种分布式训练的方法是**模型并行化**，传统上用于那些在训练过程中无法完全放入工作机器的内存或缓存的大型模型 [10, 25, 7]。模型并行训练涉及**将模型在工作机器之间进行分割**，使得**每个工作机器仅评估和更新模型的一部分参数**。然而，尽管模型并行化使得大型模型的训练成为可能，传统的模型并行化可能导致计算资源的严重浪费，因为它要么一次只能使用一个工作机器（如果每个层分配给一个工作机器），要么无法重叠计算和通信（如果每个层被分割）。此外，即使对于经验丰富的机器学习实践者来说，确定**如何最好地将DNN模型在工作机器之间分割也是一项具有挑战性的任务** [30]，通常会导致额外的低效。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文介绍了PipeDream，一种专门用于DNN的新型分布式训练系统。与模型并行化类似，它将DNN进行分割，并将各个层的**子集**分配给每个工作机器。但是，与传统的模型并行化不同，PipeDream采用了**激进的(aggressively)小批量处理流水线技术**，不同的woker在任何时间点处理不同的输入。这是通过将多个输入注入到具有第一个DNN层的工作机器中来实现的，从而保持流水线充满，并确保所有工作机器上的并发处理。它还对选定的层子集使用数据并行化以在工作机器之间平衡计算负载。我们**将这种流水线、模型并行化和数据并行化的组合称为流水线并行训练**。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pipeline-parallel并行训练有潜力在数据并行性受限时提供高效的深度神经网络（DNN）训练性能。特别是，在不同工作节点上分配的相邻层之间的前向传递中，工作节点之间的**相互通信可以限制在激活值和梯度之间(weight 不用通信)**。我们观察到，与数据并行训练相比，这种通信量**可以减少高达95%**。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PipeDream是第一个以通用和自动化的方式结合了管道并行、模型并行和数据并行的系统。它通过解决多个挑战来实现管道并行的潜力。首先，就像处理器中的流水线处理一样，实现高效率需要将DNN正确地分割成“stage(阶段)”（layer-subsequences），**每个stage在不同的工作节点上执行(ps:每个worker只负责固定的stage)**；这取决于模型架构和硬件部署。错误的分割，即stage的工作量严重不平衡，可能导致工作节点花费大量时间处于闲置状态。PipeDream**根据短期性能分析运行自动确定如何对DNN的层进行分割**，使用一种平衡不同阶段计算负载并最小化通信的算法。其次，由于DNN不总是能均匀地分配给所有可用工作节点，**PipeDream可以在某些stage使用数据并行性，即将multiple workers分配给给定stage**，在并行处理不同的小批量数据。第三，与传统的单向流水线不同，DNN训练是双向的，即前向传递后面跟着反向传递，通过相同层以相反的顺序进行。PipeDream在每个worker上交替(interleaves)进行forward和backward小批量处理(**forward 和 backward 交错起来**)，同时确保在backward中将小批量数据路由到相同的工作节点。这有助于保持所有工作节点的繁忙状态，避免管道停顿，同时防止过多的进行中小批量数据，并确保模型收敛。第四，需要仔细管理权重版本，以获得训练结束时的高质量模型。我们发现，允许给定小批量的backward使用比对应的前向传递中使用的参数更为实时的参数可能会带来显著问题。**PipeDream为每个进行中(in-flight)的小批量数据维护参数值版本**，以解决这个问题。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用PipeDream进行的实验验证了其在我们评估的五个模型上的有效性，这些模型涵盖了两个重要的DNN类别——CNN和RNN（seq-to-seq）。在ILSVRC12数据集上训练Inception-v3 [19]、VGG16 [36]、Resnet-50 [16]和AlexNet [26]时，相比于数据并行BSP，PipeDream的训练加速效果分别为1.45倍、5.12倍、1.21倍和6.76倍。在MSVD [3]数据集上训练S2VT [43]模型时，相比于数据并行BSP，PipeDream的训练加速效果为3倍。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总结起来，这篇论文有四个主要贡献。首先，它引入了一种专门针对DNN训练的并行化方法，通过将模型并行、积极的管道并行和数据并行相结合，解决了通信瓶颈的问题。其次，它确定了实现这一想法性能潜力的关键挑战，并详细介绍了每个挑战的解决方案。第三，它描述了一个高效实现管道并行DNN训练的系统（PipeDream）。第四，它通过实验证明了PipeDream在通信开销限制数据并行训练的情况下，包括数据并行比单机训练更慢的情况下，实现了并行DNN训练的可能性。<br>






