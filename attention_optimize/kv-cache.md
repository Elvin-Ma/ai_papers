# KV Cache

# 1 æ¦‚è¿°
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;åœ¨ç”Ÿæˆå¼Transformerä¸­ï¼Œç¼“å­˜(Caching) Key(K)å’Œ Value(V)çŠ¶æ€çš„æŠ€æœ¯å·²ç»å­˜åœ¨ä¸€æ®µæ—¶é—´äº†ã€‚è¿™ç§æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜æ¨ç†é€Ÿåº¦ï¼Œåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒKeyå’ŒValueçŠ¶æ€ç”¨äºè®¡ç®—å¸¦ç¼©æ”¾çš„ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶(scaled dot-product attention)ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚<br>

![figure1](images/kv-cache-figure0.jpg)

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KV Cacheå‘ç”Ÿåœ¨å¤šä¸ªtokensç”Ÿæˆæ­¥éª¤ä¸­ï¼Œåªåœ¨Decoderä¸­è¿›è¡Œï¼ˆå³åœ¨ä»…è§£ç å™¨çš„æ¨¡å‹å¦‚GPTä¸­ï¼Œæˆ–è€…åœ¨ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹å¦‚T5ä¸­çš„è§£ç å™¨éƒ¨åˆ†ï¼‰ã€‚åƒBERTè¿™æ ·çš„æ¨¡å‹ä¸æ˜¯ç”Ÿæˆæ¨¡å‹ï¼Œå› æ­¤æ²¡æœ‰KV Cacheã€‚<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è§£ç å™¨ä»¥è‡ªå›å½’(auto-regressive)çš„æ–¹å¼å·¥ä½œï¼Œå°±åƒä¸‹å›¾GPT-2æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹æ‰€ç¤ºçš„é‚£æ ·ã€‚<br>

![figure1](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*sexO6adGhaKr7aH0.gif)

*(figrue 1: åœ¨Encoderçš„è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥ï¼Œæ¨¡å‹ä¼šé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œç„¶ååœ¨ä¸‹ä¸€æ­¥ä¸­ä½¿ç”¨ç»„åˆçš„è¾“å…¥è¿›è¡Œä¸‹ä¸€ä¸ªé¢„æµ‹ã€‚)* <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è¿™ç§è‡ªå›å½’è¡Œä¸ºä¼šé‡å¤(repeats)ä¸€äº›æ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ”¾å¤§(zoom in) Encoder ä¸­è®¡ç®—çš„å¸¦æ©ç çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›(masked scaled dot-product attention)æ¥æ›´å¥½åœ°ç†è§£è¿™ä¸€ç‚¹ã€‚<br>

![figure2](images/kv-cache-gif1.gif)
*(è§£ç å™¨ä¸­ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„é€æ­¥å¯è§†åŒ–ã€‚emb_sizeè¡¨ç¤ºembedding size.)* <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ç”±äºè§£ç å™¨æ˜¯å› æœçš„ï¼ˆå³ä»¤ç‰Œçš„æ³¨æ„åŠ›ä»…ä¾èµ–äºå…¶å‰é¢çš„ä»¤ç‰Œï¼‰ï¼Œåœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬é‡æ–°è®¡ç®—äº†ç›¸åŒçš„å…ˆå‰ä»¤ç‰Œçš„æ³¨æ„åŠ›ï¼Œè€Œå®é™…ä¸Šæˆ‘ä»¬åªæƒ³è®¡ç®—æ–°ä»¤ç‰Œçš„æ³¨æ„åŠ›ã€‚<br>

# 2 KV Cache
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è¿™å°±æ˜¯KVç¼“å­˜å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚é€šè¿‡ç¼“å­˜å…ˆå‰çš„é”®(Key)å’Œå€¼(Value)ï¼Œæˆ‘ä»¬å¯ä»¥åªä¸“æ³¨äºè®¡ç®—æ–°tokençš„æ³¨æ„åŠ›ã€‚<br>
![figure2](images/kv-cache-gif2.gif)

*(ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›çš„æ¯”è¾ƒï¼Œå¸¦æœ‰å’Œä¸å¸¦æœ‰KVç¼“å­˜ã€‚emb_sizeè¡¨ç¤ºåµŒå…¥å¤§å°ã€‚)* <br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è¿™ç§ä¼˜åŒ–ä¸ºä»€ä¹ˆé‡è¦å‘¢ï¼Ÿå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä½¿ç”¨KVç¼“å­˜å¾—åˆ°çš„çŸ©é˜µè¦å°å¾—å¤šï¼Œè¿™å¯¼è‡´çŸ©é˜µä¹˜æ³•æ›´å¿«ã€‚å”¯ä¸€çš„ç¼ºç‚¹æ˜¯å®ƒéœ€è¦æ›´å¤šçš„GPU VRAMï¼ˆæˆ–è€…å¦‚æœæ²¡æœ‰ä½¿ç”¨GPUï¼Œåˆ™éœ€è¦æ›´å¤šçš„CPU RAMï¼‰æ¥ç¼“å­˜é”®(Key)å’Œå€¼(Value)çš„çŠ¶æ€ã€‚<br>

# 3 KV Cache é™æ€å±•ç¤º
## 3.1 æ²¡æœ‰KV Cache çš„æƒ…å†µ
- ä¹‹å‰tokens <br>
![figure3](images/kv-cache-figure2.jpg)

- æ–°å¢token <br>
![figure3](images/kv-cache-figure3.jpg)

## 3.2 æœ‰KV Cache çš„æƒ…å†µ
- ä¹‹å‰tokens <br>
![figure4](images/kv-cache-figure4.jpg)

- æ–°å¢token <br>
![figure5](images/kv-cache-figure5.jpg)

# 4 åŠ é€Ÿæ•ˆæœå±•ç¤º
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è®©æˆ‘ä»¬ä½¿ç”¨[transformersåº“ğŸ¤—](https://github.com/huggingface/transformers)æ¥æ¯”è¾ƒä½¿ç”¨å’Œä¸ä½¿ç”¨KVç¼“å­˜æ—¶GPT-2çš„ç”Ÿæˆé€Ÿåº¦.<br>

- ä»£ç å¦‚ä¸‹ï¼š
```
import numpy as np
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2").to(device)

for use_cache in (True, False):
  times = []
  for _ in range(10):  # measuring 10 generations
    start = time.time()
    model.generate(**tokenizer("What is KV caching?", return_tensors="pt").to(device), use_cache=use_cache, max_new_tokens=1000)
    times.append(time.time() - start)
  print(f"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds")
```

- ç»“æœå¦‚ä¸‹ï¼š
åœ¨Google Colabç¬”è®°æœ¬ä¸Šï¼Œä½¿ç”¨Tesla T4 GPUï¼Œä»¥ä¸‹æ˜¯ç”Ÿæˆ1000ä¸ªæ–°tokençš„å¹³å‡æ—¶é—´å’Œæ ‡å‡†å·®æŠ¥å‘Šï¼š<br>
```python
with KV caching: 11.885 +- 0.272 seconds
without KV caching: 56.197 +- 1.855 seconds
```

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ç»“æœæ˜¾ç¤ºï¼Œæ¨ç†é€Ÿåº¦çš„å·®å¼‚å·¨å¤§ï¼Œè€ŒGPU VRAMçš„ä½¿ç”¨é‡å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚å› æ­¤ï¼Œè¯·ç¡®ä¿åœ¨æ‚¨çš„Transformeræ¨¡å‹ä¸­ä½¿ç”¨KVç¼“å­˜ï¼<br>

# 5 å‚è€ƒé“¾æ¥
- [å‚è€ƒé“¾æ¥1](https://jalammar.github.io/illustrated-gpt2/)
- [å‚è€ƒé“¾æ¥2](https://kipp.ly/transformer-inference-arithmetic/#kv-cache)
- [å‚è€ƒé“¾æ¥3](https://juejin.cn/post/7294638699418042378?from=search-suggest)

