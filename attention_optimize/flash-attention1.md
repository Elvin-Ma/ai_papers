# 

# 摘要
变形金刚在长序列上速度慢且内存不足，因为自我关注的时间和内存复杂性在序列长度上是二次的。近似注意力方法试图通过权衡模型质量以降低计算复杂度来解决这个问题，但通常无法实现挂钟加速。我们认为，一个缺失的原则是让注意力算法IO感知——考虑GPU内存级别之间的读写。我们提出了FlashAttention，这是一种IO感知的精确注意力算法，它使用平铺来减少GPU高带宽内存（HBM）和GPU片上SRAM之间的内存读/写次数。我们分析了FlashAttention的IO复杂性，表明它需要比标准关注更少的HBM访问，并且对于一系列SRAM大小来说是最佳的。我们还将FlashAttention扩展到阻止稀疏注意力，从而产生比任何现有的近似注意力方法更快的近似注意力算法。FlashAttention训练变形金刚的速度比现有基线更快：与MLPerf 1.1训练速度记录相比，BERT大型（序列长度512）的端到端时钟速度提高了15%，GPT-2（序列长度1K）的速度提高了3倍，远程竞技场（序列长度1K-4K）的速度提升了2.4倍。FlashAttention和块稀疏FlashAttentention使Transformers中的上下文更长，产生了更高质量的模型（GPT-2的困惑度为0.7，长文档分类的提升度为6.4）和全新的能力：第一个在Path-X挑战（序列长度16K，准确率为61.4%）和Path-256（序列长度64K，准确度为63.1%）上实现优于偶然性能的变形金刚。
