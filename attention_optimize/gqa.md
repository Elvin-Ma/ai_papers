# GQA: 从多头检查点中训练通用的多查询Transformer模型

摘要：
多查询注意力（MQA）仅使用单个键值头，极大地加快了解码器推理速度。然而，MQA可能会导致质量下降，而且训练一个单独的模型只是为了更快的推理可能并不理想。我们提出了一种方法，可以使用原始预训练计算量的5％来对现有的多头语言模型检查点进行自我训练，将其转换为具有MQA的模型。同时，我们引入了分组查询注意力（GQA），它是多查询注意力的一种推广形式，使用中间数量（大于一个，小于查询头的数量）的键值头。我们展示了经过自我训练的GQA在质量上接近多头注意力，并且与MQA具有相当的速度。
