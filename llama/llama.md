# LLaMA：开放高效的基础语言模型

# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们介绍了LLaMA，一系列参数从**7B到65B**的基础语言模型。我们在数**万亿**个tokens上训练我们的模型，并展示了只使用公开可用的数据集进行训练的可能性，而不依赖专有和不可访问的数据集。特别是，LLaMA-13B在大多数基准测试中优于GPT-3（175B），而LLaMA-65B与最佳模型Chinchilla-70B和PaLM-540B相媲美。我们将所有模型发布给研究社区[社区链接](https://github.com/facebookresearch/llama)。<br>

# 1 引言
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;于大规模语料(corpora)库训练的大型语言模型（LLM）展示了它们从文本指令或少量示例中执行新任务的能力（Brown等，2020）。这些少样本特性首次出现在将模型扩展到足够大的规模时（Kaplan等，2020），从而衍生出一系列工作，专注于进一步**扩展**这些模型（Chowdhery等，2022；Rae等，2021）。这些努力基于一个假设，即更多的参数会带来更好的性能。**然而**，Hoffmann等（2022）的最新研究表明，在给定的计算预算下，最佳性能并不是由最大的模型实现的，而是由在**更多数据**上训练的较小模型实现的。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hoffmann等人（2022）提出的扩展定律的目标是确定如何在**特定的训练计算预算下**最佳地扩展数据集和模型大小。然而，这个目标忽视了推理计算预算，在大规模使用语言模型时变得至关重要。在这个背景下，考虑到目标性能水平，首选的模型不是训练速度最快的模型，而是**推理速度最快**的模型，尽管训练一个大模型达到一定水平的成本可能更低，但**在实际使用时推理速度的快慢才是关键**。<br>




